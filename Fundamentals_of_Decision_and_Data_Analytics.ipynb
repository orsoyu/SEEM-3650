{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNSd8hEO7KOz",
        "outputId": "101a3a4f-54ea-4823-f2e4-d6713cbfbf8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 686, done.\u001b[K\n",
            "remote: Total 686 (delta 0), reused 0 (delta 0), pack-reused 686 (from 1)\u001b[K\n",
            "Receiving objects: 100% (686/686), 954.03 KiB | 1.38 MiB/s, done.\n",
            "Resolving deltas: 100% (387/387), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RJh2rs5xBPQl",
        "outputId": "2dc9135a-af9f-4779-ac87-75ef4c2725f6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/nanoGPT/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouNifiN2BbEx",
        "outputId": "33972eb7-e911-47e9-ac69-31c1d6a9be94"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanoGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 2: Train the Shakespeare Character-level Model\n",
        "\n",
        "!python data/shakespeare_char/prepare.py\n",
        "\n",
        "!python train.py config/train_shakespeare_char.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubzsygjL8N0F",
        "outputId": "0131c7f2-cdad-469e-c95c-718e856799e4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n",
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.65M\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 4.2874, val loss 4.2823\n",
            "iter 0: loss 4.2654, time 15449.60ms, mfu -100.00%\n",
            "iter 10: loss 3.1462, time 49.95ms, mfu 7.46%\n",
            "iter 20: loss 2.7313, time 51.36ms, mfu 7.44%\n",
            "iter 30: loss 2.6180, time 52.17ms, mfu 7.41%\n",
            "iter 40: loss 2.5755, time 53.75ms, mfu 7.36%\n",
            "iter 50: loss 2.5251, time 51.20ms, mfu 7.35%\n",
            "iter 60: loss 2.5142, time 54.33ms, mfu 7.30%\n",
            "iter 70: loss 2.4945, time 52.52ms, mfu 7.28%\n",
            "iter 80: loss 2.4938, time 54.83ms, mfu 7.23%\n",
            "iter 90: loss 2.4674, time 51.01ms, mfu 7.24%\n",
            "iter 100: loss 2.4595, time 54.45ms, mfu 7.20%\n",
            "iter 110: loss 2.4609, time 51.67ms, mfu 7.20%\n",
            "iter 120: loss 2.4290, time 54.74ms, mfu 7.16%\n",
            "iter 130: loss 2.4122, time 51.88ms, mfu 7.17%\n",
            "iter 140: loss 2.4067, time 54.50ms, mfu 7.13%\n",
            "iter 150: loss 2.4062, time 52.67ms, mfu 7.13%\n",
            "iter 160: loss 2.3806, time 54.39ms, mfu 7.10%\n",
            "iter 170: loss 2.3482, time 52.47ms, mfu 7.10%\n",
            "iter 180: loss 2.3180, time 52.96ms, mfu 7.09%\n",
            "iter 190: loss 2.2443, time 53.81ms, mfu 7.08%\n",
            "iter 200: loss 2.2102, time 53.08ms, mfu 7.07%\n",
            "iter 210: loss 2.1385, time 54.08ms, mfu 7.05%\n",
            "iter 220: loss 2.1433, time 53.84ms, mfu 7.04%\n",
            "iter 230: loss 2.0694, time 53.79ms, mfu 7.03%\n",
            "iter 240: loss 2.0783, time 52.79ms, mfu 7.03%\n",
            "step 250: train loss 1.9670, val loss 2.0674\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 2.0323, time 7159.85ms, mfu 6.33%\n",
            "iter 260: loss 1.9791, time 51.54ms, mfu 6.42%\n",
            "iter 270: loss 1.9789, time 56.95ms, mfu 6.43%\n",
            "iter 280: loss 1.9783, time 52.57ms, mfu 6.50%\n",
            "iter 290: loss 1.9124, time 57.34ms, mfu 6.50%\n",
            "iter 300: loss 1.9023, time 52.89ms, mfu 6.55%\n",
            "iter 310: loss 1.8651, time 57.92ms, mfu 6.54%\n",
            "iter 320: loss 1.8472, time 53.03ms, mfu 6.59%\n",
            "iter 330: loss 1.8250, time 55.80ms, mfu 6.60%\n",
            "iter 340: loss 1.7819, time 54.38ms, mfu 6.62%\n",
            "iter 350: loss 1.8231, time 55.95ms, mfu 6.63%\n",
            "iter 360: loss 1.7700, time 54.23ms, mfu 6.65%\n",
            "iter 370: loss 1.7366, time 55.07ms, mfu 6.66%\n",
            "iter 380: loss 1.7252, time 54.83ms, mfu 6.68%\n",
            "iter 390: loss 1.7250, time 54.93ms, mfu 6.69%\n",
            "iter 400: loss 1.7768, time 54.97ms, mfu 6.70%\n",
            "iter 410: loss 1.7047, time 53.96ms, mfu 6.72%\n",
            "iter 420: loss 1.7127, time 54.56ms, mfu 6.73%\n",
            "iter 430: loss 1.6821, time 54.66ms, mfu 6.74%\n",
            "iter 440: loss 1.6469, time 54.65ms, mfu 6.75%\n",
            "iter 450: loss 1.6546, time 54.83ms, mfu 6.75%\n",
            "iter 460: loss 1.5961, time 54.33ms, mfu 6.76%\n",
            "iter 470: loss 1.6472, time 54.28ms, mfu 6.77%\n",
            "iter 480: loss 1.6121, time 54.63ms, mfu 6.78%\n",
            "iter 490: loss 1.5954, time 54.23ms, mfu 6.79%\n",
            "step 500: train loss 1.5178, val loss 1.7196\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 1.5955, time 7148.81ms, mfu 6.11%\n",
            "iter 510: loss 1.6047, time 50.81ms, mfu 6.24%\n",
            "iter 520: loss 1.5900, time 55.65ms, mfu 6.28%\n",
            "iter 530: loss 1.5526, time 52.28ms, mfu 6.37%\n",
            "iter 540: loss 1.6122, time 54.86ms, mfu 6.41%\n",
            "iter 550: loss 1.5595, time 51.79ms, mfu 6.49%\n",
            "iter 560: loss 1.5592, time 54.31ms, mfu 6.52%\n",
            "iter 570: loss 1.5614, time 52.17ms, mfu 6.59%\n",
            "iter 580: loss 1.5258, time 54.68ms, mfu 6.61%\n",
            "iter 590: loss 1.4876, time 52.52ms, mfu 6.66%\n",
            "iter 600: loss 1.5066, time 53.76ms, mfu 6.68%\n",
            "iter 610: loss 1.5474, time 52.11ms, mfu 6.73%\n",
            "iter 620: loss 1.5315, time 54.57ms, mfu 6.74%\n",
            "iter 630: loss 1.5062, time 52.29ms, mfu 6.78%\n",
            "iter 640: loss 1.4642, time 53.36ms, mfu 6.80%\n",
            "iter 650: loss 1.4970, time 53.45ms, mfu 6.82%\n",
            "iter 660: loss 1.5045, time 53.67ms, mfu 6.83%\n",
            "iter 670: loss 1.4361, time 54.08ms, mfu 6.84%\n",
            "iter 680: loss 1.5042, time 53.45ms, mfu 6.85%\n",
            "iter 690: loss 1.4610, time 52.44ms, mfu 6.87%\n",
            "iter 700: loss 1.4808, time 52.56ms, mfu 6.90%\n",
            "iter 710: loss 1.4511, time 53.01ms, mfu 6.91%\n",
            "iter 720: loss 1.4371, time 53.05ms, mfu 6.92%\n",
            "iter 730: loss 1.4276, time 53.04ms, mfu 6.93%\n",
            "iter 740: loss 1.4224, time 52.96ms, mfu 6.94%\n",
            "step 750: train loss 1.3594, val loss 1.5843\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.4238, time 7118.19ms, mfu 6.25%\n",
            "iter 760: loss 1.4420, time 50.65ms, mfu 6.36%\n",
            "iter 770: loss 1.4228, time 54.75ms, mfu 6.41%\n",
            "iter 780: loss 1.4152, time 51.18ms, mfu 6.49%\n",
            "iter 790: loss 1.4167, time 54.56ms, mfu 6.53%\n",
            "iter 800: loss 1.4310, time 51.61ms, mfu 6.60%\n",
            "iter 810: loss 1.4043, time 53.72ms, mfu 6.63%\n",
            "iter 820: loss 1.4030, time 51.80ms, mfu 6.69%\n",
            "iter 830: loss 1.3910, time 54.53ms, mfu 6.70%\n",
            "iter 840: loss 1.3990, time 52.20ms, mfu 6.75%\n",
            "iter 850: loss 1.3846, time 53.65ms, mfu 6.77%\n",
            "iter 860: loss 1.3957, time 51.72ms, mfu 6.81%\n",
            "iter 870: loss 1.3965, time 52.79ms, mfu 6.83%\n",
            "iter 880: loss 1.3628, time 52.79ms, mfu 6.86%\n",
            "iter 890: loss 1.3887, time 53.80ms, mfu 6.86%\n",
            "iter 900: loss 1.3644, time 51.61ms, mfu 6.90%\n",
            "iter 910: loss 1.3201, time 53.08ms, mfu 6.91%\n",
            "iter 920: loss 1.3584, time 52.20ms, mfu 6.93%\n",
            "iter 930: loss 1.3600, time 52.46ms, mfu 6.95%\n",
            "iter 940: loss 1.3454, time 53.26ms, mfu 6.96%\n",
            "iter 950: loss 1.3524, time 52.26ms, mfu 6.97%\n",
            "iter 960: loss 1.3624, time 52.96ms, mfu 6.98%\n",
            "iter 970: loss 1.3582, time 52.10ms, mfu 7.00%\n",
            "iter 980: loss 1.3516, time 52.93ms, mfu 7.00%\n",
            "iter 990: loss 1.3383, time 52.15ms, mfu 7.02%\n",
            "step 1000: train loss 1.2757, val loss 1.5235\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.3382, time 7096.52ms, mfu 6.32%\n",
            "iter 1010: loss 1.3389, time 49.92ms, mfu 6.43%\n",
            "iter 1020: loss 1.3173, time 54.66ms, mfu 6.47%\n",
            "iter 1030: loss 1.3369, time 50.82ms, mfu 6.56%\n",
            "iter 1040: loss 1.3559, time 54.26ms, mfu 6.59%\n",
            "iter 1050: loss 1.2892, time 51.50ms, mfu 6.65%\n",
            "iter 1060: loss 1.3336, time 54.03ms, mfu 6.68%\n",
            "iter 1070: loss 1.3314, time 51.58ms, mfu 6.73%\n",
            "iter 1080: loss 1.3328, time 54.14ms, mfu 6.75%\n",
            "iter 1090: loss 1.3471, time 51.28ms, mfu 6.80%\n",
            "iter 1100: loss 1.3193, time 54.75ms, mfu 6.80%\n",
            "iter 1110: loss 1.2966, time 51.67ms, mfu 6.84%\n",
            "iter 1120: loss 1.2964, time 53.49ms, mfu 6.85%\n",
            "iter 1130: loss 1.2891, time 51.38ms, mfu 6.89%\n",
            "iter 1140: loss 1.2931, time 53.86ms, mfu 6.90%\n",
            "iter 1150: loss 1.3051, time 52.06ms, mfu 6.92%\n",
            "iter 1160: loss 1.3207, time 53.69ms, mfu 6.92%\n",
            "iter 1170: loss 1.2965, time 52.66ms, mfu 6.94%\n",
            "iter 1180: loss 1.3205, time 53.01ms, mfu 6.95%\n",
            "iter 1190: loss 1.2639, time 53.08ms, mfu 6.96%\n",
            "iter 1200: loss 1.2895, time 52.20ms, mfu 6.97%\n",
            "iter 1210: loss 1.2627, time 54.03ms, mfu 6.97%\n",
            "iter 1220: loss 1.3082, time 51.89ms, mfu 6.99%\n",
            "iter 1230: loss 1.2959, time 52.87ms, mfu 6.99%\n",
            "iter 1240: loss 1.2976, time 51.95ms, mfu 7.01%\n",
            "step 1250: train loss 1.2072, val loss 1.4906\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.2729, time 7136.90ms, mfu 6.32%\n",
            "iter 1260: loss 1.2792, time 50.61ms, mfu 6.42%\n",
            "iter 1270: loss 1.2666, time 54.77ms, mfu 6.46%\n",
            "iter 1280: loss 1.2594, time 51.30ms, mfu 6.54%\n",
            "iter 1290: loss 1.2787, time 54.74ms, mfu 6.57%\n",
            "iter 1300: loss 1.2981, time 52.18ms, mfu 6.62%\n",
            "iter 1310: loss 1.2298, time 54.60ms, mfu 6.64%\n",
            "iter 1320: loss 1.3052, time 52.44ms, mfu 6.69%\n",
            "iter 1330: loss 1.2664, time 53.94ms, mfu 6.71%\n",
            "iter 1340: loss 1.2923, time 52.23ms, mfu 6.75%\n",
            "iter 1350: loss 1.2615, time 54.80ms, mfu 6.76%\n",
            "iter 1360: loss 1.2719, time 52.07ms, mfu 6.80%\n",
            "iter 1370: loss 1.2583, time 54.33ms, mfu 6.80%\n",
            "iter 1380: loss 1.2591, time 52.37ms, mfu 6.84%\n",
            "iter 1390: loss 1.2474, time 54.05ms, mfu 6.84%\n",
            "iter 1400: loss 1.2612, time 53.84ms, mfu 6.85%\n",
            "iter 1410: loss 1.2476, time 52.27ms, mfu 6.88%\n",
            "iter 1420: loss 1.2745, time 53.70ms, mfu 6.88%\n",
            "iter 1430: loss 1.2404, time 53.37ms, mfu 6.89%\n",
            "iter 1440: loss 1.2499, time 53.14ms, mfu 6.91%\n",
            "iter 1450: loss 1.2326, time 52.57ms, mfu 6.92%\n",
            "iter 1460: loss 1.2330, time 54.10ms, mfu 6.92%\n",
            "iter 1470: loss 1.2242, time 52.28ms, mfu 6.94%\n",
            "iter 1480: loss 1.2149, time 53.90ms, mfu 6.94%\n",
            "iter 1490: loss 1.2370, time 53.42ms, mfu 6.94%\n",
            "step 1500: train loss 1.1506, val loss 1.4747\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.1885, time 7150.74ms, mfu 6.25%\n",
            "iter 1510: loss 1.2305, time 51.06ms, mfu 6.36%\n",
            "iter 1520: loss 1.2271, time 54.85ms, mfu 6.40%\n",
            "iter 1530: loss 1.2588, time 52.46ms, mfu 6.47%\n",
            "iter 1540: loss 1.1917, time 53.45ms, mfu 6.52%\n",
            "iter 1550: loss 1.2261, time 51.96ms, mfu 6.59%\n",
            "iter 1560: loss 1.2069, time 54.34ms, mfu 6.61%\n",
            "iter 1570: loss 1.2328, time 52.15ms, mfu 6.67%\n",
            "iter 1580: loss 1.2067, time 54.09ms, mfu 6.69%\n",
            "iter 1590: loss 1.1882, time 51.97ms, mfu 6.74%\n",
            "iter 1600: loss 1.1928, time 54.69ms, mfu 6.74%\n",
            "iter 1610: loss 1.2266, time 52.37ms, mfu 6.78%\n",
            "iter 1620: loss 1.1800, time 53.51ms, mfu 6.80%\n",
            "iter 1630: loss 1.2066, time 53.45ms, mfu 6.82%\n",
            "iter 1640: loss 1.1934, time 53.40ms, mfu 6.83%\n",
            "iter 1650: loss 1.1733, time 53.97ms, mfu 6.84%\n",
            "iter 1660: loss 1.2166, time 53.40ms, mfu 6.85%\n",
            "iter 1670: loss 1.2025, time 53.82ms, mfu 6.86%\n",
            "iter 1680: loss 1.1965, time 53.05ms, mfu 6.88%\n",
            "iter 1690: loss 1.1948, time 53.70ms, mfu 6.88%\n",
            "iter 1700: loss 1.1806, time 52.99ms, mfu 6.90%\n",
            "iter 1710: loss 1.1755, time 53.90ms, mfu 6.90%\n",
            "iter 1720: loss 1.1797, time 53.72ms, mfu 6.90%\n",
            "iter 1730: loss 1.1987, time 53.17ms, mfu 6.91%\n",
            "iter 1740: loss 1.1685, time 53.72ms, mfu 6.92%\n",
            "step 1750: train loss 1.1054, val loss 1.4679\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1750: loss 1.1941, time 7132.15ms, mfu 6.23%\n",
            "iter 1760: loss 1.1861, time 50.40ms, mfu 6.35%\n",
            "iter 1770: loss 1.1941, time 54.87ms, mfu 6.39%\n",
            "iter 1780: loss 1.1973, time 51.88ms, mfu 6.47%\n",
            "iter 1790: loss 1.1990, time 54.64ms, mfu 6.50%\n",
            "iter 1800: loss 1.1773, time 51.52ms, mfu 6.58%\n",
            "iter 1810: loss 1.1565, time 54.56ms, mfu 6.60%\n",
            "iter 1820: loss 1.1697, time 51.37ms, mfu 6.67%\n",
            "iter 1830: loss 1.1630, time 54.35ms, mfu 6.69%\n",
            "iter 1840: loss 1.1602, time 52.43ms, mfu 6.73%\n",
            "iter 1850: loss 1.1575, time 54.74ms, mfu 6.74%\n",
            "iter 1860: loss 1.1852, time 51.95ms, mfu 6.78%\n",
            "iter 1870: loss 1.1429, time 54.04ms, mfu 6.79%\n",
            "iter 1880: loss 1.1775, time 52.94ms, mfu 6.82%\n",
            "iter 1890: loss 1.1769, time 54.36ms, mfu 6.82%\n",
            "iter 1900: loss 1.1296, time 52.62ms, mfu 6.85%\n",
            "iter 1910: loss 1.1666, time 53.35ms, mfu 6.86%\n",
            "iter 1920: loss 1.1686, time 53.01ms, mfu 6.88%\n",
            "iter 1930: loss 1.1426, time 52.43ms, mfu 6.90%\n",
            "iter 1940: loss 1.1275, time 53.60ms, mfu 6.91%\n",
            "iter 1950: loss 1.1328, time 52.26ms, mfu 6.93%\n",
            "iter 1960: loss 1.1532, time 53.84ms, mfu 6.93%\n",
            "iter 1970: loss 1.1476, time 54.14ms, mfu 6.92%\n",
            "iter 1980: loss 1.1491, time 53.10ms, mfu 6.93%\n",
            "iter 1990: loss 1.1530, time 53.53ms, mfu 6.93%\n",
            "step 2000: train loss 1.0540, val loss 1.4749\n",
            "iter 2000: loss 1.1278, time 6847.66ms, mfu 6.25%\n",
            "iter 2010: loss 1.1197, time 53.44ms, mfu 6.32%\n",
            "iter 2020: loss 1.1139, time 51.36ms, mfu 6.41%\n",
            "iter 2030: loss 1.1509, time 53.14ms, mfu 6.47%\n",
            "iter 2040: loss 1.1381, time 51.55ms, mfu 6.55%\n",
            "iter 2050: loss 1.1074, time 53.04ms, mfu 6.60%\n",
            "iter 2060: loss 1.1037, time 52.00ms, mfu 6.65%\n",
            "iter 2070: loss 1.1290, time 53.88ms, mfu 6.68%\n",
            "iter 2080: loss 1.1222, time 52.74ms, mfu 6.72%\n",
            "iter 2090: loss 1.1300, time 53.26ms, mfu 6.75%\n",
            "iter 2100: loss 1.1307, time 52.95ms, mfu 6.77%\n",
            "iter 2110: loss 1.1295, time 53.03ms, mfu 6.80%\n",
            "iter 2120: loss 1.1229, time 52.15ms, mfu 6.83%\n",
            "iter 2130: loss 1.1299, time 53.64ms, mfu 6.85%\n",
            "iter 2140: loss 1.1347, time 53.68ms, mfu 6.86%\n",
            "iter 2150: loss 1.1216, time 53.40ms, mfu 6.87%\n",
            "iter 2160: loss 1.1446, time 53.71ms, mfu 6.87%\n",
            "iter 2170: loss 1.1310, time 52.68ms, mfu 6.89%\n",
            "iter 2180: loss 1.1022, time 53.49ms, mfu 6.90%\n",
            "iter 2190: loss 1.1056, time 52.75ms, mfu 6.92%\n",
            "iter 2200: loss 1.1216, time 53.74ms, mfu 6.92%\n",
            "iter 2210: loss 1.1148, time 52.67ms, mfu 6.94%\n",
            "iter 2220: loss 1.1187, time 53.79ms, mfu 6.93%\n",
            "iter 2230: loss 1.1221, time 52.47ms, mfu 6.95%\n",
            "iter 2240: loss 1.1218, time 53.92ms, mfu 6.95%\n",
            "step 2250: train loss 1.0070, val loss 1.4780\n",
            "iter 2250: loss 1.1062, time 6849.15ms, mfu 6.26%\n",
            "iter 2260: loss 1.1066, time 53.07ms, mfu 6.33%\n",
            "iter 2270: loss 1.1246, time 53.35ms, mfu 6.40%\n",
            "iter 2280: loss 1.0895, time 53.14ms, mfu 6.46%\n",
            "iter 2290: loss 1.1417, time 51.99ms, mfu 6.53%\n",
            "iter 2300: loss 1.1135, time 53.24ms, mfu 6.58%\n",
            "iter 2310: loss 1.0888, time 53.03ms, mfu 6.62%\n",
            "iter 2320: loss 1.0983, time 52.96ms, mfu 6.66%\n",
            "iter 2330: loss 1.0990, time 52.31ms, mfu 6.71%\n",
            "iter 2340: loss 1.1128, time 52.66ms, mfu 6.75%\n",
            "iter 2350: loss 1.0979, time 53.24ms, mfu 6.77%\n",
            "iter 2360: loss 1.1158, time 52.40ms, mfu 6.81%\n",
            "iter 2370: loss 1.0872, time 52.85ms, mfu 6.83%\n",
            "iter 2380: loss 1.0820, time 52.81ms, mfu 6.85%\n",
            "iter 2390: loss 1.0808, time 52.82ms, mfu 6.87%\n",
            "iter 2400: loss 1.0828, time 53.49ms, mfu 6.88%\n",
            "iter 2410: loss 1.0767, time 52.72ms, mfu 6.90%\n",
            "iter 2420: loss 1.0752, time 53.65ms, mfu 6.91%\n",
            "iter 2430: loss 1.0605, time 52.87ms, mfu 6.92%\n",
            "iter 2440: loss 1.0565, time 52.60ms, mfu 6.94%\n",
            "iter 2450: loss 1.0732, time 52.64ms, mfu 6.95%\n",
            "iter 2460: loss 1.0812, time 53.64ms, mfu 6.95%\n",
            "iter 2470: loss 1.0897, time 52.69ms, mfu 6.96%\n",
            "iter 2480: loss 1.0825, time 53.09ms, mfu 6.97%\n",
            "iter 2490: loss 1.0555, time 53.40ms, mfu 6.97%\n",
            "step 2500: train loss 0.9576, val loss 1.4917\n",
            "iter 2500: loss 1.0788, time 6847.62ms, mfu 6.28%\n",
            "iter 2510: loss 1.0746, time 52.91ms, mfu 6.35%\n",
            "iter 2520: loss 1.0429, time 51.93ms, mfu 6.44%\n",
            "iter 2530: loss 1.0473, time 52.90ms, mfu 6.50%\n",
            "iter 2540: loss 1.0512, time 51.54ms, mfu 6.57%\n",
            "iter 2550: loss 1.0729, time 52.67ms, mfu 6.62%\n",
            "iter 2560: loss 1.0543, time 53.13ms, mfu 6.66%\n",
            "iter 2570: loss 1.0659, time 53.40ms, mfu 6.69%\n",
            "iter 2580: loss 1.0742, time 53.38ms, mfu 6.72%\n",
            "iter 2590: loss 1.0646, time 53.41ms, mfu 6.75%\n",
            "iter 2600: loss 1.0695, time 54.13ms, mfu 6.76%\n",
            "iter 2610: loss 1.0479, time 52.69ms, mfu 6.79%\n",
            "iter 2620: loss 1.0372, time 52.87ms, mfu 6.82%\n",
            "iter 2630: loss 1.0251, time 52.35ms, mfu 6.85%\n",
            "iter 2640: loss 1.0369, time 53.89ms, mfu 6.85%\n",
            "iter 2650: loss 1.0638, time 52.21ms, mfu 6.88%\n",
            "iter 2660: loss 1.0348, time 54.12ms, mfu 6.88%\n",
            "iter 2670: loss 1.0106, time 52.42ms, mfu 6.90%\n",
            "iter 2680: loss 1.0532, time 53.16ms, mfu 6.92%\n",
            "iter 2690: loss 1.0528, time 51.92ms, mfu 6.94%\n",
            "iter 2700: loss 1.0189, time 53.23ms, mfu 6.95%\n",
            "iter 2710: loss 1.0445, time 52.26ms, mfu 6.97%\n",
            "iter 2720: loss 1.0378, time 52.82ms, mfu 6.97%\n",
            "iter 2730: loss 1.0528, time 52.84ms, mfu 6.98%\n",
            "iter 2740: loss 1.0171, time 53.09ms, mfu 6.99%\n",
            "step 2750: train loss 0.9117, val loss 1.5060\n",
            "iter 2750: loss 1.0255, time 6863.95ms, mfu 6.29%\n",
            "iter 2760: loss 1.0245, time 53.70ms, mfu 6.36%\n",
            "iter 2770: loss 1.0238, time 53.61ms, mfu 6.42%\n",
            "iter 2780: loss 1.0177, time 52.90ms, mfu 6.48%\n",
            "iter 2790: loss 1.0368, time 52.05ms, mfu 6.55%\n",
            "iter 2800: loss 1.0166, time 53.66ms, mfu 6.59%\n",
            "iter 2810: loss 1.0449, time 52.81ms, mfu 6.63%\n",
            "iter 2820: loss 1.0199, time 53.13ms, mfu 6.67%\n",
            "iter 2830: loss 1.0322, time 52.78ms, mfu 6.71%\n",
            "iter 2840: loss 0.9954, time 52.79ms, mfu 6.75%\n",
            "iter 2850: loss 1.0200, time 53.55ms, mfu 6.77%\n",
            "iter 2860: loss 1.0198, time 53.08ms, mfu 6.79%\n",
            "iter 2870: loss 1.0014, time 53.13ms, mfu 6.81%\n",
            "iter 2880: loss 1.0345, time 52.65ms, mfu 6.84%\n",
            "iter 2890: loss 1.0116, time 53.37ms, mfu 6.85%\n",
            "iter 2900: loss 0.9831, time 53.87ms, mfu 6.86%\n",
            "iter 2910: loss 1.0382, time 53.18ms, mfu 6.88%\n",
            "iter 2920: loss 1.0107, time 53.78ms, mfu 6.88%\n",
            "iter 2930: loss 0.9963, time 53.38ms, mfu 6.89%\n",
            "iter 2940: loss 0.9899, time 52.41ms, mfu 6.91%\n",
            "iter 2950: loss 1.0198, time 53.75ms, mfu 6.91%\n",
            "iter 2960: loss 0.9978, time 53.06ms, mfu 6.93%\n",
            "iter 2970: loss 0.9939, time 52.90ms, mfu 6.94%\n",
            "iter 2980: loss 0.9992, time 52.88ms, mfu 6.95%\n",
            "iter 2990: loss 0.9813, time 53.23ms, mfu 6.95%\n",
            "step 3000: train loss 0.8634, val loss 1.5224\n",
            "iter 3000: loss 0.9796, time 6843.53ms, mfu 6.26%\n",
            "iter 3010: loss 0.9884, time 54.47ms, mfu 6.32%\n",
            "iter 3020: loss 1.0038, time 52.81ms, mfu 6.39%\n",
            "iter 3030: loss 0.9981, time 52.94ms, mfu 6.46%\n",
            "iter 3040: loss 1.0201, time 52.89ms, mfu 6.52%\n",
            "iter 3050: loss 0.9789, time 53.09ms, mfu 6.57%\n",
            "iter 3060: loss 0.9953, time 53.43ms, mfu 6.61%\n",
            "iter 3070: loss 1.0144, time 54.05ms, mfu 6.64%\n",
            "iter 3080: loss 0.9898, time 53.24ms, mfu 6.67%\n",
            "iter 3090: loss 0.9802, time 53.31ms, mfu 6.70%\n",
            "iter 3100: loss 0.9912, time 52.58ms, mfu 6.74%\n",
            "iter 3110: loss 0.9666, time 53.24ms, mfu 6.77%\n",
            "iter 3120: loss 0.9994, time 52.94ms, mfu 6.80%\n",
            "iter 3130: loss 0.9762, time 52.95ms, mfu 6.82%\n",
            "iter 3140: loss 0.9761, time 53.98ms, mfu 6.83%\n",
            "iter 3150: loss 0.9998, time 52.64ms, mfu 6.85%\n",
            "iter 3160: loss 1.0056, time 53.19ms, mfu 6.87%\n",
            "iter 3170: loss 0.9555, time 53.23ms, mfu 6.88%\n",
            "iter 3180: loss 0.9724, time 53.27ms, mfu 6.89%\n",
            "iter 3190: loss 0.9854, time 53.22ms, mfu 6.90%\n",
            "iter 3200: loss 0.9556, time 53.73ms, mfu 6.91%\n",
            "iter 3210: loss 0.9659, time 53.29ms, mfu 6.92%\n",
            "iter 3220: loss 0.9545, time 53.22ms, mfu 6.92%\n",
            "iter 3230: loss 0.9559, time 52.56ms, mfu 6.94%\n",
            "iter 3240: loss 0.9490, time 53.31ms, mfu 6.95%\n",
            "step 3250: train loss 0.8222, val loss 1.5564\n",
            "iter 3250: loss 0.9635, time 6859.79ms, mfu 6.26%\n",
            "iter 3260: loss 0.9599, time 54.19ms, mfu 6.32%\n",
            "iter 3270: loss 0.9682, time 52.49ms, mfu 6.40%\n",
            "iter 3280: loss 0.9527, time 53.38ms, mfu 6.45%\n",
            "iter 3290: loss 0.9405, time 52.07ms, mfu 6.52%\n",
            "iter 3300: loss 0.9405, time 53.72ms, mfu 6.57%\n",
            "iter 3310: loss 0.9536, time 53.71ms, mfu 6.60%\n",
            "iter 3320: loss 0.9544, time 53.04ms, mfu 6.65%\n",
            "iter 3330: loss 0.9546, time 52.92ms, mfu 6.69%\n",
            "iter 3340: loss 0.9559, time 52.87ms, mfu 6.72%\n",
            "iter 3350: loss 0.9489, time 53.56ms, mfu 6.75%\n",
            "iter 3360: loss 0.9209, time 53.34ms, mfu 6.77%\n",
            "iter 3370: loss 0.9478, time 53.27ms, mfu 6.79%\n",
            "iter 3380: loss 0.9459, time 52.92ms, mfu 6.82%\n",
            "iter 3390: loss 0.9528, time 53.43ms, mfu 6.83%\n",
            "iter 3400: loss 0.9535, time 52.03ms, mfu 6.87%\n",
            "iter 3410: loss 0.9422, time 54.41ms, mfu 6.86%\n",
            "iter 3420: loss 0.9411, time 52.59ms, mfu 6.89%\n",
            "iter 3430: loss 0.9393, time 53.78ms, mfu 6.89%\n",
            "iter 3440: loss 0.9783, time 52.62ms, mfu 6.91%\n",
            "iter 3450: loss 0.9451, time 53.70ms, mfu 6.91%\n",
            "iter 3460: loss 0.9456, time 53.60ms, mfu 6.92%\n",
            "iter 3470: loss 0.9290, time 53.02ms, mfu 6.93%\n",
            "iter 3480: loss 0.9415, time 53.54ms, mfu 6.93%\n",
            "iter 3490: loss 0.9131, time 53.60ms, mfu 6.93%\n",
            "step 3500: train loss 0.7786, val loss 1.5672\n",
            "iter 3500: loss 0.8982, time 6871.97ms, mfu 6.24%\n",
            "iter 3510: loss 0.9055, time 53.98ms, mfu 6.31%\n",
            "iter 3520: loss 0.9255, time 53.10ms, mfu 6.38%\n",
            "iter 3530: loss 0.9571, time 53.25ms, mfu 6.44%\n",
            "iter 3540: loss 0.9245, time 53.05ms, mfu 6.50%\n",
            "iter 3550: loss 0.9239, time 53.41ms, mfu 6.55%\n",
            "iter 3560: loss 0.9563, time 52.52ms, mfu 6.60%\n",
            "iter 3570: loss 0.9424, time 52.65ms, mfu 6.65%\n",
            "iter 3580: loss 0.9253, time 53.25ms, mfu 6.69%\n",
            "iter 3590: loss 0.9143, time 53.65ms, mfu 6.71%\n",
            "iter 3600: loss 0.9237, time 52.71ms, mfu 6.75%\n",
            "iter 3610: loss 0.9013, time 53.49ms, mfu 6.77%\n",
            "iter 3620: loss 0.9162, time 52.87ms, mfu 6.80%\n",
            "iter 3630: loss 0.9255, time 52.95ms, mfu 6.82%\n",
            "iter 3640: loss 0.9117, time 53.81ms, mfu 6.83%\n",
            "iter 3650: loss 0.9079, time 52.03ms, mfu 6.86%\n",
            "iter 3660: loss 0.9277, time 53.94ms, mfu 6.87%\n",
            "iter 3670: loss 0.9341, time 51.98ms, mfu 6.90%\n",
            "iter 3680: loss 0.9094, time 52.58ms, mfu 6.92%\n",
            "iter 3690: loss 0.9307, time 52.83ms, mfu 6.93%\n",
            "iter 3700: loss 0.8677, time 52.62ms, mfu 6.95%\n",
            "iter 3710: loss 0.8836, time 52.84ms, mfu 6.96%\n",
            "iter 3720: loss 0.9031, time 53.09ms, mfu 6.96%\n",
            "iter 3730: loss 0.8947, time 53.04ms, mfu 6.97%\n",
            "iter 3740: loss 0.8995, time 53.72ms, mfu 6.97%\n",
            "step 3750: train loss 0.7417, val loss 1.5965\n",
            "iter 3750: loss 0.8969, time 6856.89ms, mfu 6.27%\n",
            "iter 3760: loss 0.9370, time 53.75ms, mfu 6.34%\n",
            "iter 3770: loss 0.9330, time 54.27ms, mfu 6.39%\n",
            "iter 3780: loss 0.9162, time 54.02ms, mfu 6.44%\n",
            "iter 3790: loss 0.8980, time 52.65ms, mfu 6.51%\n",
            "iter 3800: loss 0.9108, time 52.89ms, mfu 6.56%\n",
            "iter 3810: loss 0.9296, time 52.92ms, mfu 6.61%\n",
            "iter 3820: loss 0.8842, time 54.06ms, mfu 6.64%\n",
            "iter 3830: loss 0.9018, time 52.22ms, mfu 6.69%\n",
            "iter 3840: loss 0.8898, time 53.15ms, mfu 6.72%\n",
            "iter 3850: loss 0.8858, time 52.20ms, mfu 6.76%\n",
            "iter 3860: loss 0.8729, time 52.92ms, mfu 6.79%\n",
            "iter 3870: loss 0.8890, time 54.20ms, mfu 6.80%\n",
            "iter 3880: loss 0.8901, time 52.78ms, mfu 6.82%\n",
            "iter 3890: loss 0.8881, time 53.56ms, mfu 6.84%\n",
            "iter 3900: loss 0.8889, time 53.64ms, mfu 6.85%\n",
            "iter 3910: loss 0.8915, time 53.67ms, mfu 6.86%\n",
            "iter 3920: loss 0.8742, time 53.45ms, mfu 6.87%\n",
            "iter 3930: loss 0.8881, time 52.58ms, mfu 6.89%\n",
            "iter 3940: loss 0.8727, time 53.47ms, mfu 6.90%\n",
            "iter 3950: loss 0.8804, time 52.90ms, mfu 6.91%\n",
            "iter 3960: loss 0.8948, time 53.30ms, mfu 6.92%\n",
            "iter 3970: loss 0.8860, time 53.76ms, mfu 6.92%\n",
            "iter 3980: loss 0.9041, time 53.03ms, mfu 6.93%\n",
            "iter 3990: loss 0.8811, time 53.47ms, mfu 6.94%\n",
            "step 4000: train loss 0.7056, val loss 1.6218\n",
            "iter 4000: loss 0.8542, time 6850.50ms, mfu 6.25%\n",
            "iter 4010: loss 0.8734, time 54.38ms, mfu 6.31%\n",
            "iter 4020: loss 0.8896, time 50.44ms, mfu 6.42%\n",
            "iter 4030: loss 0.8788, time 54.31ms, mfu 6.46%\n",
            "iter 4040: loss 0.8764, time 52.06ms, mfu 6.53%\n",
            "iter 4050: loss 0.8722, time 54.16ms, mfu 6.57%\n",
            "iter 4060: loss 0.8616, time 52.49ms, mfu 6.62%\n",
            "iter 4070: loss 0.8577, time 52.72ms, mfu 6.66%\n",
            "iter 4080: loss 0.8866, time 53.94ms, mfu 6.69%\n",
            "iter 4090: loss 0.8462, time 52.71ms, mfu 6.73%\n",
            "iter 4100: loss 0.8974, time 53.71ms, mfu 6.75%\n",
            "iter 4110: loss 0.8745, time 51.97ms, mfu 6.79%\n",
            "iter 4120: loss 0.8762, time 54.30ms, mfu 6.80%\n",
            "iter 4130: loss 0.8649, time 51.84ms, mfu 6.84%\n",
            "iter 4140: loss 0.8812, time 54.15ms, mfu 6.84%\n",
            "iter 4150: loss 0.8688, time 53.76ms, mfu 6.85%\n",
            "iter 4160: loss 0.8594, time 53.17ms, mfu 6.87%\n",
            "iter 4170: loss 0.8627, time 52.92ms, mfu 6.88%\n",
            "iter 4180: loss 0.8717, time 52.39ms, mfu 6.91%\n",
            "iter 4190: loss 0.8653, time 53.11ms, mfu 6.92%\n",
            "iter 4200: loss 0.8561, time 52.97ms, mfu 6.93%\n",
            "iter 4210: loss 0.8679, time 52.95ms, mfu 6.94%\n",
            "iter 4220: loss 0.8564, time 53.01ms, mfu 6.95%\n",
            "iter 4230: loss 0.8804, time 52.95ms, mfu 6.96%\n",
            "iter 4240: loss 0.8730, time 53.37ms, mfu 6.96%\n",
            "step 4250: train loss 0.6769, val loss 1.6423\n",
            "iter 4250: loss 0.8658, time 6873.01ms, mfu 6.27%\n",
            "iter 4260: loss 0.8599, time 52.60ms, mfu 6.35%\n",
            "iter 4270: loss 0.8643, time 51.71ms, mfu 6.44%\n",
            "iter 4280: loss 0.8563, time 52.66ms, mfu 6.50%\n",
            "iter 4290: loss 0.8329, time 53.19ms, mfu 6.55%\n",
            "iter 4300: loss 0.8251, time 53.32ms, mfu 6.59%\n",
            "iter 4310: loss 0.8550, time 52.11ms, mfu 6.65%\n",
            "iter 4320: loss 0.8317, time 53.91ms, mfu 6.68%\n",
            "iter 4330: loss 0.8593, time 53.05ms, mfu 6.71%\n",
            "iter 4340: loss 0.8315, time 53.20ms, mfu 6.74%\n",
            "iter 4350: loss 0.8268, time 53.20ms, mfu 6.77%\n",
            "iter 4360: loss 0.8450, time 53.54ms, mfu 6.79%\n",
            "iter 4370: loss 0.8457, time 53.49ms, mfu 6.80%\n",
            "iter 4380: loss 0.8443, time 53.17ms, mfu 6.82%\n",
            "iter 4390: loss 0.8660, time 53.04ms, mfu 6.84%\n",
            "iter 4400: loss 0.8377, time 52.78ms, mfu 6.87%\n",
            "iter 4410: loss 0.8537, time 53.43ms, mfu 6.88%\n",
            "iter 4420: loss 0.8589, time 53.12ms, mfu 6.89%\n",
            "iter 4430: loss 0.8409, time 53.31ms, mfu 6.90%\n",
            "iter 4440: loss 0.8456, time 52.98ms, mfu 6.91%\n",
            "iter 4450: loss 0.8576, time 52.97ms, mfu 6.93%\n",
            "iter 4460: loss 0.8341, time 53.33ms, mfu 6.93%\n",
            "iter 4470: loss 0.8412, time 54.20ms, mfu 6.93%\n",
            "iter 4480: loss 0.8371, time 52.48ms, mfu 6.94%\n",
            "iter 4490: loss 0.8375, time 53.23ms, mfu 6.95%\n",
            "step 4500: train loss 0.6514, val loss 1.6654\n",
            "iter 4500: loss 0.8525, time 6848.90ms, mfu 6.26%\n",
            "iter 4510: loss 0.8461, time 53.33ms, mfu 6.33%\n",
            "iter 4520: loss 0.8346, time 51.42ms, mfu 6.42%\n",
            "iter 4530: loss 0.8479, time 53.93ms, mfu 6.47%\n",
            "iter 4540: loss 0.8443, time 51.82ms, mfu 6.54%\n",
            "iter 4550: loss 0.8745, time 53.66ms, mfu 6.58%\n",
            "iter 4560: loss 0.8468, time 51.98ms, mfu 6.64%\n",
            "iter 4570: loss 0.8370, time 53.26ms, mfu 6.68%\n",
            "iter 4580: loss 0.8478, time 53.32ms, mfu 6.71%\n",
            "iter 4590: loss 0.8580, time 53.02ms, mfu 6.74%\n",
            "iter 4600: loss 0.8193, time 53.89ms, mfu 6.76%\n",
            "iter 4610: loss 0.8680, time 52.49ms, mfu 6.79%\n",
            "iter 4620: loss 0.8327, time 54.06ms, mfu 6.80%\n",
            "iter 4630: loss 0.8195, time 53.72ms, mfu 6.82%\n",
            "iter 4640: loss 0.8499, time 53.13ms, mfu 6.84%\n",
            "iter 4650: loss 0.8572, time 52.68ms, mfu 6.86%\n",
            "iter 4660: loss 0.8482, time 53.23ms, mfu 6.87%\n",
            "iter 4670: loss 0.8334, time 52.63ms, mfu 6.89%\n",
            "iter 4680: loss 0.8519, time 53.32ms, mfu 6.90%\n",
            "iter 4690: loss 0.8440, time 52.02ms, mfu 6.93%\n",
            "iter 4700: loss 0.8323, time 54.14ms, mfu 6.92%\n",
            "iter 4710: loss 0.7783, time 53.39ms, mfu 6.93%\n",
            "iter 4720: loss 0.8273, time 53.85ms, mfu 6.93%\n",
            "iter 4730: loss 0.8159, time 52.55ms, mfu 6.95%\n",
            "iter 4740: loss 0.8252, time 53.50ms, mfu 6.95%\n",
            "step 4750: train loss 0.6341, val loss 1.6861\n",
            "iter 4750: loss 0.8055, time 6859.24ms, mfu 6.26%\n",
            "iter 4760: loss 0.8187, time 54.01ms, mfu 6.32%\n",
            "iter 4770: loss 0.7938, time 51.87ms, mfu 6.41%\n",
            "iter 4780: loss 0.8100, time 53.90ms, mfu 6.46%\n",
            "iter 4790: loss 0.8340, time 51.96ms, mfu 6.53%\n",
            "iter 4800: loss 0.8191, time 53.49ms, mfu 6.57%\n",
            "iter 4810: loss 0.8347, time 53.34ms, mfu 6.61%\n",
            "iter 4820: loss 0.8156, time 53.13ms, mfu 6.65%\n",
            "iter 4830: loss 0.8234, time 53.75ms, mfu 6.68%\n",
            "iter 4840: loss 0.8266, time 53.48ms, mfu 6.71%\n",
            "iter 4850: loss 0.8117, time 53.40ms, mfu 6.74%\n",
            "iter 4860: loss 0.8256, time 53.42ms, mfu 6.76%\n",
            "iter 4870: loss 0.8061, time 53.06ms, mfu 6.79%\n",
            "iter 4880: loss 0.8247, time 53.45ms, mfu 6.81%\n",
            "iter 4890: loss 0.8133, time 52.63ms, mfu 6.83%\n",
            "iter 4900: loss 0.8072, time 52.57ms, mfu 6.86%\n",
            "iter 4910: loss 0.8258, time 53.77ms, mfu 6.87%\n",
            "iter 4920: loss 0.8170, time 53.27ms, mfu 6.88%\n",
            "iter 4930: loss 0.8060, time 54.04ms, mfu 6.88%\n",
            "iter 4940: loss 0.8007, time 52.46ms, mfu 6.90%\n",
            "iter 4950: loss 0.8287, time 53.71ms, mfu 6.91%\n",
            "iter 4960: loss 0.8299, time 52.95ms, mfu 6.92%\n",
            "iter 4970: loss 0.7817, time 53.19ms, mfu 6.93%\n",
            "iter 4980: loss 0.7957, time 54.42ms, mfu 6.92%\n",
            "iter 4990: loss 0.8152, time 52.99ms, mfu 6.93%\n",
            "step 5000: train loss 0.6188, val loss 1.7010\n",
            "iter 5000: loss 0.8221, time 6862.17ms, mfu 6.24%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/sample.py\", line 8, in <module>\n",
            "    import tiktoken\n",
            "ModuleNotFoundError: No module named 'tiktoken'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oEQ0J6-D7li",
        "outputId": "46c056c0-ad32-44f4-cad7-64ff69b3d9b4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate samples from the trained model\n",
        "!python sample.py --out_dir=out-shakespeare-char --device=cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auMegEHTEBI2",
        "outputId": "5bf5ca16-9059-4531-bce5-c3ddf8e2c24e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-shakespeare-char\n",
            "Overriding: device = cuda\n",
            "number of parameters: 10.65M\n",
            "Loading meta from data/shakespeare_char/meta.pkl...\n",
            "\n",
            "\n",
            "KING RICHARD II:\n",
            "Sweet Lord Hastings, was the army that world?\n",
            "\n",
            "Messenger:\n",
            "Nurse, I will determine, away!\n",
            "\n",
            "KING RICHARD III:\n",
            "\n",
            "YORK:\n",
            "My friend Scroop, my lord.\n",
            "\n",
            "RICHARD:\n",
            "I hear them in the soul.\n",
            "\n",
            "WARWICK:\n",
            "Ay, and well in the likeness of his soul:\n",
            "Marshal, the world's not of this death:\n",
            "I would tell thou the hour Clifford, and therefore now;\n",
            "But poor of hours are admitted to speak in his shroud.\n",
            "Talk of when here? thou shalt have no foot in his sake,\n",
            "And he makes the king and his his charm for hi\n",
            "---------------\n",
            "\n",
            "Men your knees,\n",
            "I would not be made a changed-foolman.\n",
            "\n",
            "Second Servant:\n",
            "Why, how now, uncled, whom comforted should so,\n",
            "I may as he loved him shall be better than my soul\n",
            "To our plaining of the needless creditate.\n",
            "\n",
            "Second Servant:\n",
            "Here's a pointed so man of the beauty as his business soul.\n",
            "\n",
            "Clown:\n",
            "For here is here this and head is broke.\n",
            "\n",
            "Shepherd:\n",
            "Here's no more: so I am in it.\n",
            "\n",
            "LEONTES:\n",
            "I am too much; and swear that I have learn'd with thee, and\n",
            "thou art death continuated: if thou stook'st one\n",
            "---------------\n",
            "\n",
            "Men that I both. This is the number which cannot,\n",
            "The nobility of one actions, and\n",
            "I never wanting of for worth.\n",
            "\n",
            "MENENIUS:\n",
            "O, this is the day\n",
            "Will be grave's baleIng.\n",
            "\n",
            "CORIOLANUS:\n",
            "A lord,\n",
            "Look o' the man's double intended,\n",
            "And will it walkway out of it; the stroke's land,\n",
            "That with the prince's life and just defend once;\n",
            "And therefore, still it is contraryl to the world:\n",
            "My lord and beguns, and voluments to the law;\n",
            "The present I heir of a tongue's eyes and therein,\n",
            "And yet then take the aftern\n",
            "---------------\n",
            "\n",
            "\n",
            "First Murderer:\n",
            "What pleasant, is something to know?\n",
            "\n",
            "Second Murderer:\n",
            "O, here's a piteous private of your loving shrewd.\n",
            "\n",
            "Second Murderer:\n",
            "Here's a woman are to say I, good Marcius,\n",
            "To prive over-house, how the hard of None entreat wife\n",
            "Of his many chances, thou wilt not give out the heart.\n",
            "What with his part and hard a service in the world spoil?\n",
            "\n",
            "CLAUDIO:\n",
            "If you will want for so would proud him your names,\n",
            "But by my lord and he should not like you shown.\n",
            "\n",
            "LUCIO:\n",
            "Yet, my lord.\n",
            "\n",
            "ISABELLA:\n",
            "Marr\n",
            "---------------\n",
            "\n",
            "Be every beloved, her lords your lossing in the rabble\n",
            "In followed and utmost haunted for the north.\n",
            "Come, come, come hither, fool Capel's death,\n",
            "Before to the second matter down near in the woman's man.\n",
            "\n",
            "LORD ROSS:\n",
            "Why, lords, noble Catesby man, how now she can admire us\n",
            "From the sweets, though not do your promises\n",
            "Of all the queen's death, like a beggar:\n",
            "And a most such disposition\n",
            "As I resign to our brains; and you may little hear\n",
            "From me with the body, how I will play\n",
            "My sin will further for\n",
            "---------------\n",
            "\n",
            "\n",
            "MENENIUS:\n",
            "Ha!\n",
            "\n",
            "CORIOLANUS:\n",
            "How! what said?\n",
            "\n",
            "MENENIUS:\n",
            "Nay, I am your corse\n",
            "And that nothing not stand of death? Lay the time\n",
            "He shall be craved to death.\n",
            "\n",
            "CORIOLANUS:\n",
            "Why, the dog in this ingentleness? This is the poor people,\n",
            "More your patricians, than must entreat it.\n",
            "\n",
            "CORIOLANUS:\n",
            "This is there.\n",
            "Come, sir, no more; yet this is a gentleman in the pride no\n",
            "cheap; I beseech your one worship with this best rest, your\n",
            "man to speak into your knees: I will nothing he wrong,\n",
            "he is so his abused; it i\n",
            "---------------\n",
            "\n",
            "She would be my left dishonour my father,\n",
            "And I have not to myself in the love:\n",
            "Now I am cause thee, I am grave too much.\n",
            "\n",
            "Lord Mayor:\n",
            "How do you all, my lord, I'll have our knees,\n",
            "And I would not have a happiness. How came with me?\n",
            "\n",
            "GLOUCESTER:\n",
            "Here's he?\n",
            "\n",
            "GLOUCESTER:\n",
            "I will rather be the world and like your lord.\n",
            "\n",
            "LADY ANNE:\n",
            "Here comes the law to the troublous morning lord.\n",
            "\n",
            "GLOUCESTER:\n",
            "Parcel of Herculet, or that teach manner fearful spirit:\n",
            "I thought that who to infects her brother than star\n",
            "---------------\n",
            "\n",
            "I'll play thee on thee:\n",
            "O, then yet have I cag a sort of thee,\n",
            "To make of the heavens to the world. Therefore, the gods\n",
            "I may not play thee and long to the air\n",
            "Of honour: but to make thee of thy bosom hatred\n",
            "And all the banishment of unbranchless hours of ears\n",
            "And stand tears to thee, bid crave a soldier wips,\n",
            "Which nature to be the varyer to speak with thy hate,\n",
            "And thou digst althought the world ded the country's purpose.\n",
            "\n",
            "DUKE OF YORK:\n",
            "Good good nurse, go to; and I will appear thee with thy l\n",
            "---------------\n",
            "\n",
            "\n",
            "LEONTES:\n",
            "The realm be from the gracious servant of the king,\n",
            "And by this being of fourteen years,\n",
            "So flattering holds all our flight sound is the grown,\n",
            "To have beguiled, that he should have quickly had but\n",
            "And many things false withrough with the shore-crackation\n",
            "Even that basily not man the lie but danger,\n",
            "To call me to die, as if they grieve the uptorney,\n",
            "And will in the story of the senators, there too much on\n",
            "To relent this her is a poor antiple.\n",
            "\n",
            "Just:\n",
            "I think, madam, sir, I hop--\n",
            "\n",
            "PETER:\n",
            "---------------\n",
            "\n",
            "How che now, I saw, and yet through your souls of your woes,\n",
            "He shall consider into yourself\n",
            "Bear that to the sweet soil's best your tongue\n",
            "Is thus made my bones. For the prince he would\n",
            "Entreat you wish your worships and yours.\n",
            "\n",
            "Provost:\n",
            "Play your servant; I do beseech your ladyship iny this crown.\n",
            "\n",
            "MENENIUS:\n",
            "Your honest best indeed, you says his foe.\n",
            "\n",
            "CORIOLANUS:\n",
            "Your more, the consented continuation: he says not has it\n",
            "but the noble gods i' the shepherd's power. Pro's the villain\n",
            "a very man a\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 3: Model Architecture Exploration\n",
        "\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import io\n",
        "\n",
        "# create figures directory\n",
        "!mkdir -p figures\n",
        "\n",
        "# modify the configuration file\n",
        "def modify_config(n_layer, n_head):\n",
        "    with open('config/train_shakespeare_char.py', 'r') as f:\n",
        "        config = f.read()\n",
        "\n",
        "    # modify key parameters\n",
        "    config = re.sub(r'n_layer = \\d+', f'n_layer = {n_layer}', config)\n",
        "    config = re.sub(r'n_head = \\d+', f'n_head = {n_head}', config)\n",
        "    # make sure we're using a reasonable number of iterations\n",
        "    config = re.sub(r'max_iters = \\d+', 'max_iters = 6000', config)\n",
        "    config = re.sub(r'eval_interval = \\d+', 'eval_interval = 1000', config)\n",
        "\n",
        "    # create a new configuration file\n",
        "    new_config_path = f'config/train_shakespeare_char_{n_layer}layer_{n_head}head.py'\n",
        "    with open(new_config_path, 'w') as f:\n",
        "        f.write(config)\n",
        "\n",
        "    return new_config_path\n",
        "\n",
        "# extract loss values from training log\n",
        "def extract_loss_from_log(log_text, target_iter=5000):\n",
        "    # Find the eval step closest to the target iteration\n",
        "    iter_pattern = r\"step (\\d+): train loss ([\\d.]+), val loss ([\\d.]+)\"\n",
        "    matches = re.findall(iter_pattern, log_text)\n",
        "\n",
        "    if not matches:\n",
        "        return None, None\n",
        "\n",
        "    # Find the evaluation step closest to target_iter\n",
        "    closest_match = min(matches, key=lambda x: abs(int(x[0]) - target_iter))\n",
        "    iter_num, train_loss, val_loss = closest_match\n",
        "\n",
        "    return float(train_loss), float(val_loss)\n",
        "\n",
        "# fixed number of heads = 4\n",
        "n_heads = 4\n",
        "n_layers_list = [2, 3, 5, 7]\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for n_layer in n_layers_list:\n",
        "    print(f\"\\nTraining model with {n_layer} layers and {n_heads} heads\")\n",
        "\n",
        "    config_path = modify_config(n_layer, n_heads)\n",
        "\n",
        "    command = f\"python train.py {config_path}\"\n",
        "    print(f\"Running command: {command}\")\n",
        "\n",
        "    # run the command and capture output\n",
        "    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    log_output = \"\"\n",
        "\n",
        "    while True:\n",
        "        output = process.stdout.readline()\n",
        "        if output == '' and process.poll() is not None:\n",
        "            break\n",
        "        if output:\n",
        "            print(output.strip())\n",
        "            log_output += output\n",
        "\n",
        "    process.communicate()\n",
        "\n",
        "    # extract the loss values from the log\n",
        "    train_loss, val_loss = extract_loss_from_log(log_output, target_iter=5000)\n",
        "\n",
        "    if train_loss is not None and val_loss is not None:\n",
        "        print(f\"Extracted losses: train={train_loss}, val={val_loss}\")\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "\n",
        "# filter out None values for plotting\n",
        "valid_points = [(layer, loss) for layer, loss in zip(n_layers_list, val_losses) if loss is not None]\n",
        "if valid_points:\n",
        "    valid_layers, valid_val_losses = zip(*valid_points)\n",
        "\n",
        "    # Plot the validation loss versus number of layers\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(valid_layers, valid_val_losses, marker='o', linestyle='-', color='blue')\n",
        "    plt.xlabel('Number of Layers')\n",
        "    plt.ylabel('Validation Loss at Iteration 5000')\n",
        "    plt.title('Impact of Model Depth on Validation Loss')\n",
        "    plt.grid(True)\n",
        "    plt.savefig('figures/layers_vs_loss.png')\n",
        "    plt.show()\n",
        "\n",
        "    # determine the best configuration\n",
        "    best_idx = np.argmin(valid_val_losses)\n",
        "    best_n_layer = valid_layers[best_idx]\n",
        "    best_val_loss = valid_val_losses[best_idx]\n",
        "    print(f\"\\nBest configuration: {best_n_layer} layers with {n_heads} heads, validation loss: {best_val_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_rbUkXDpF2Cq",
        "outputId": "f2c9d267-8c2d-4685-9193-d4f435d5356e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model with 2 layers and 4 heads\n",
            "Running command: python train.py config/train_shakespeare_char_2layer_4head.py\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "W0503 17:01:13.103000 10725 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "Overriding config with config/train_shakespeare_char_2layer_4head.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 1000 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 2\n",
            "n_head = 4\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 6000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 3.57M\n",
            "num decayed parameter tensors: 10, with 3,662,208 parameters\n",
            "num non-decayed parameter tensors: 5, with 1,920 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 4.2456, val loss 4.2425\n",
            "iter 0: loss 4.2539, time 12313.27ms, mfu -100.00%\n",
            "iter 10: loss 3.3113, time 7.12ms, mfu 17.51%\n",
            "iter 20: loss 2.9175, time 8.37ms, mfu 17.25%\n",
            "iter 30: loss 2.6728, time 7.05ms, mfu 17.29%\n",
            "iter 40: loss 2.5735, time 7.21ms, mfu 17.29%\n",
            "iter 50: loss 2.5358, time 6.89ms, mfu 17.38%\n",
            "iter 60: loss 2.4993, time 6.88ms, mfu 17.45%\n",
            "iter 70: loss 2.4863, time 6.98ms, mfu 17.49%\n",
            "iter 80: loss 2.4999, time 7.04ms, mfu 17.52%\n",
            "iter 90: loss 2.4844, time 7.10ms, mfu 17.52%\n",
            "iter 100: loss 2.4715, time 6.98ms, mfu 17.56%\n",
            "iter 110: loss 2.4613, time 6.92ms, mfu 17.60%\n",
            "iter 120: loss 2.4660, time 7.10ms, mfu 17.60%\n",
            "iter 130: loss 2.4270, time 7.04ms, mfu 17.61%\n",
            "iter 140: loss 2.4273, time 6.82ms, mfu 17.68%\n",
            "iter 150: loss 2.4215, time 7.01ms, mfu 17.69%\n",
            "iter 160: loss 2.3932, time 6.81ms, mfu 17.75%\n",
            "iter 170: loss 2.3573, time 6.89ms, mfu 17.79%\n",
            "iter 180: loss 2.3079, time 6.82ms, mfu 17.84%\n",
            "iter 190: loss 2.2787, time 7.20ms, mfu 17.79%\n",
            "iter 200: loss 2.2139, time 6.84ms, mfu 17.83%\n",
            "iter 210: loss 2.1649, time 6.94ms, mfu 17.85%\n",
            "iter 220: loss 2.1310, time 6.78ms, mfu 17.90%\n",
            "iter 230: loss 2.0784, time 6.99ms, mfu 17.90%\n",
            "iter 240: loss 2.0530, time 6.90ms, mfu 17.91%\n",
            "iter 250: loss 2.0265, time 7.02ms, mfu 17.90%\n",
            "iter 260: loss 1.9993, time 7.46ms, mfu 17.78%\n",
            "iter 270: loss 1.9784, time 6.82ms, mfu 17.83%\n",
            "iter 280: loss 1.9548, time 6.79ms, mfu 17.88%\n",
            "iter 290: loss 1.9477, time 6.83ms, mfu 17.92%\n",
            "iter 300: loss 1.9168, time 7.05ms, mfu 17.90%\n",
            "iter 310: loss 1.8943, time 7.04ms, mfu 17.88%\n",
            "iter 320: loss 1.8671, time 7.15ms, mfu 17.84%\n",
            "iter 330: loss 1.8457, time 7.08ms, mfu 17.82%\n",
            "iter 340: loss 1.8405, time 7.02ms, mfu 17.81%\n",
            "iter 350: loss 1.8174, time 6.89ms, mfu 17.84%\n",
            "iter 360: loss 1.8305, time 7.07ms, mfu 17.82%\n",
            "iter 370: loss 1.8050, time 6.95ms, mfu 17.83%\n",
            "iter 380: loss 1.7775, time 7.03ms, mfu 17.83%\n",
            "iter 390: loss 1.7762, time 6.96ms, mfu 17.84%\n",
            "iter 400: loss 1.7728, time 6.81ms, mfu 17.88%\n",
            "iter 410: loss 1.7621, time 6.95ms, mfu 17.89%\n",
            "iter 420: loss 1.7377, time 6.84ms, mfu 17.93%\n",
            "iter 430: loss 1.7415, time 6.95ms, mfu 17.93%\n",
            "iter 440: loss 1.7217, time 6.97ms, mfu 17.93%\n",
            "iter 450: loss 1.7280, time 6.89ms, mfu 17.95%\n",
            "iter 460: loss 1.7287, time 7.10ms, mfu 17.91%\n",
            "iter 470: loss 1.7306, time 7.21ms, mfu 17.85%\n",
            "iter 480: loss 1.7242, time 6.96ms, mfu 17.86%\n",
            "iter 490: loss 1.7012, time 7.06ms, mfu 17.84%\n",
            "iter 500: loss 1.6949, time 7.21ms, mfu 17.78%\n",
            "iter 510: loss 1.7180, time 6.93ms, mfu 17.80%\n",
            "iter 520: loss 1.7410, time 6.99ms, mfu 17.81%\n",
            "iter 530: loss 1.6872, time 6.89ms, mfu 17.84%\n",
            "iter 540: loss 1.6916, time 6.94ms, mfu 17.85%\n",
            "iter 550: loss 1.6580, time 7.29ms, mfu 17.78%\n",
            "iter 560: loss 1.6646, time 7.08ms, mfu 17.76%\n",
            "iter 570: loss 1.6597, time 6.94ms, mfu 17.78%\n",
            "iter 580: loss 1.6715, time 6.98ms, mfu 17.79%\n",
            "iter 590: loss 1.6527, time 6.80ms, mfu 17.85%\n",
            "iter 600: loss 1.6368, time 7.15ms, mfu 17.81%\n",
            "iter 610: loss 1.6565, time 6.95ms, mfu 17.82%\n",
            "iter 620: loss 1.6338, time 6.93ms, mfu 17.84%\n",
            "iter 630: loss 1.6029, time 7.14ms, mfu 17.80%\n",
            "iter 640: loss 1.6227, time 7.37ms, mfu 17.71%\n",
            "iter 650: loss 1.6357, time 7.10ms, mfu 17.70%\n",
            "iter 660: loss 1.6564, time 7.02ms, mfu 17.71%\n",
            "iter 670: loss 1.6341, time 7.54ms, mfu 17.59%\n",
            "iter 680: loss 1.6421, time 8.24ms, mfu 17.35%\n",
            "iter 690: loss 1.6366, time 7.72ms, mfu 17.23%\n",
            "iter 700: loss 1.6151, time 6.99ms, mfu 17.29%\n",
            "iter 710: loss 1.6128, time 7.10ms, mfu 17.32%\n",
            "iter 720: loss 1.5682, time 7.26ms, mfu 17.30%\n",
            "iter 730: loss 1.5909, time 7.09ms, mfu 17.33%\n",
            "iter 740: loss 1.5857, time 6.86ms, mfu 17.42%\n",
            "iter 750: loss 1.5904, time 6.89ms, mfu 17.49%\n",
            "iter 760: loss 1.5977, time 6.82ms, mfu 17.57%\n",
            "iter 770: loss 1.5489, time 6.82ms, mfu 17.64%\n",
            "iter 780: loss 1.5708, time 7.16ms, mfu 17.62%\n",
            "iter 790: loss 1.5899, time 6.99ms, mfu 17.64%\n",
            "iter 800: loss 1.6096, time 7.03ms, mfu 17.65%\n",
            "iter 810: loss 1.5782, time 6.77ms, mfu 17.73%\n",
            "iter 820: loss 1.5485, time 7.17ms, mfu 17.69%\n",
            "iter 830: loss 1.5546, time 6.98ms, mfu 17.71%\n",
            "iter 840: loss 1.5724, time 7.40ms, mfu 17.63%\n",
            "iter 850: loss 1.5466, time 6.93ms, mfu 17.66%\n",
            "iter 860: loss 1.5486, time 6.87ms, mfu 17.71%\n",
            "iter 870: loss 1.5358, time 7.02ms, mfu 17.72%\n",
            "iter 880: loss 1.5649, time 6.87ms, mfu 17.76%\n",
            "iter 890: loss 1.5445, time 6.92ms, mfu 17.79%\n",
            "iter 900: loss 1.5552, time 6.76ms, mfu 17.86%\n",
            "iter 910: loss 1.5650, time 7.03ms, mfu 17.84%\n",
            "iter 920: loss 1.5451, time 7.13ms, mfu 17.81%\n",
            "iter 930: loss 1.5544, time 6.88ms, mfu 17.84%\n",
            "iter 940: loss 1.5433, time 6.80ms, mfu 17.89%\n",
            "iter 950: loss 1.5238, time 6.80ms, mfu 17.94%\n",
            "iter 960: loss 1.5289, time 7.31ms, mfu 17.85%\n",
            "iter 970: loss 1.5077, time 6.94ms, mfu 17.86%\n",
            "iter 980: loss 1.5322, time 7.17ms, mfu 17.81%\n",
            "iter 990: loss 1.5212, time 6.96ms, mfu 17.83%\n",
            "step 1000: train loss 1.3977, val loss 1.6253\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.5288, time 2970.72ms, mfu 16.05%\n",
            "iter 1010: loss 1.5466, time 6.77ms, mfu 16.28%\n",
            "iter 1020: loss 1.5026, time 7.08ms, mfu 16.42%\n",
            "iter 1030: loss 1.4992, time 6.94ms, mfu 16.57%\n",
            "iter 1040: loss 1.5283, time 7.01ms, mfu 16.70%\n",
            "iter 1050: loss 1.5057, time 7.12ms, mfu 16.78%\n",
            "iter 1060: loss 1.4899, time 6.89ms, mfu 16.91%\n",
            "iter 1070: loss 1.5228, time 6.92ms, mfu 17.02%\n",
            "iter 1080: loss 1.5398, time 7.04ms, mfu 17.09%\n",
            "iter 1090: loss 1.5141, time 7.32ms, mfu 17.09%\n",
            "iter 1100: loss 1.5256, time 7.15ms, mfu 17.12%\n",
            "iter 1110: loss 1.4945, time 7.20ms, mfu 17.14%\n",
            "iter 1120: loss 1.4902, time 7.00ms, mfu 17.21%\n",
            "iter 1130: loss 1.5115, time 6.99ms, mfu 17.27%\n",
            "iter 1140: loss 1.4933, time 7.59ms, mfu 17.19%\n",
            "iter 1150: loss 1.4706, time 7.01ms, mfu 17.25%\n",
            "iter 1160: loss 1.4721, time 7.04ms, mfu 17.30%\n",
            "iter 1170: loss 1.4995, time 7.08ms, mfu 17.33%\n",
            "iter 1180: loss 1.4926, time 7.03ms, mfu 17.37%\n",
            "iter 1190: loss 1.4917, time 6.96ms, mfu 17.43%\n",
            "iter 1200: loss 1.4884, time 7.41ms, mfu 17.37%\n",
            "iter 1210: loss 1.5087, time 7.10ms, mfu 17.39%\n",
            "iter 1220: loss 1.5225, time 7.41ms, mfu 17.33%\n",
            "iter 1230: loss 1.5236, time 7.40ms, mfu 17.28%\n",
            "iter 1240: loss 1.4806, time 7.02ms, mfu 17.33%\n",
            "iter 1250: loss 1.4409, time 6.88ms, mfu 17.41%\n",
            "iter 1260: loss 1.4311, time 7.22ms, mfu 17.40%\n",
            "iter 1270: loss 1.4792, time 6.87ms, mfu 17.47%\n",
            "iter 1280: loss 1.4701, time 6.97ms, mfu 17.52%\n",
            "iter 1290: loss 1.4836, time 7.09ms, mfu 17.52%\n",
            "iter 1300: loss 1.4627, time 6.85ms, mfu 17.59%\n",
            "iter 1310: loss 1.4617, time 7.17ms, mfu 17.57%\n",
            "iter 1320: loss 1.5003, time 7.13ms, mfu 17.57%\n",
            "iter 1330: loss 1.4647, time 6.89ms, mfu 17.62%\n",
            "iter 1340: loss 1.4708, time 6.96ms, mfu 17.65%\n",
            "iter 1350: loss 1.4880, time 6.98ms, mfu 17.67%\n",
            "iter 1360: loss 1.4813, time 7.13ms, mfu 17.66%\n",
            "iter 1370: loss 1.4520, time 6.89ms, mfu 17.70%\n",
            "iter 1380: loss 1.4708, time 7.17ms, mfu 17.67%\n",
            "iter 1390: loss 1.4402, time 7.02ms, mfu 17.68%\n",
            "iter 1400: loss 1.4570, time 6.90ms, mfu 17.72%\n",
            "iter 1410: loss 1.4510, time 6.95ms, mfu 17.74%\n",
            "iter 1420: loss 1.4779, time 7.14ms, mfu 17.71%\n",
            "iter 1430: loss 1.4585, time 6.86ms, mfu 17.76%\n",
            "iter 1440: loss 1.4577, time 7.16ms, mfu 17.73%\n",
            "iter 1450: loss 1.4610, time 7.01ms, mfu 17.73%\n",
            "iter 1460: loss 1.4051, time 6.94ms, mfu 17.76%\n",
            "iter 1470: loss 1.4783, time 7.01ms, mfu 17.76%\n",
            "iter 1480: loss 1.4467, time 6.85ms, mfu 17.80%\n",
            "iter 1490: loss 1.4376, time 6.75ms, mfu 17.87%\n",
            "iter 1500: loss 1.4487, time 6.96ms, mfu 17.88%\n",
            "iter 1510: loss 1.4749, time 6.92ms, mfu 17.89%\n",
            "iter 1520: loss 1.4427, time 6.95ms, mfu 17.90%\n",
            "iter 1530: loss 1.4234, time 7.13ms, mfu 17.86%\n",
            "iter 1540: loss 1.4420, time 6.96ms, mfu 17.87%\n",
            "iter 1550: loss 1.4310, time 7.04ms, mfu 17.85%\n",
            "iter 1560: loss 1.4478, time 7.22ms, mfu 17.79%\n",
            "iter 1570: loss 1.4263, time 7.07ms, mfu 17.78%\n",
            "iter 1580: loss 1.4415, time 7.35ms, mfu 17.70%\n",
            "iter 1590: loss 1.4571, time 6.94ms, mfu 17.73%\n",
            "iter 1600: loss 1.4369, time 6.87ms, mfu 17.77%\n",
            "iter 1610: loss 1.4268, time 7.01ms, mfu 17.77%\n",
            "iter 1620: loss 1.4428, time 7.01ms, mfu 17.77%\n",
            "iter 1630: loss 1.4217, time 7.27ms, mfu 17.71%\n",
            "iter 1640: loss 1.4215, time 6.99ms, mfu 17.72%\n",
            "iter 1650: loss 1.4353, time 7.00ms, mfu 17.73%\n",
            "iter 1660: loss 1.4202, time 7.15ms, mfu 17.71%\n",
            "iter 1670: loss 1.4308, time 7.05ms, mfu 17.70%\n",
            "iter 1680: loss 1.4423, time 7.06ms, mfu 17.70%\n",
            "iter 1690: loss 1.4343, time 6.91ms, mfu 17.74%\n",
            "iter 1700: loss 1.4133, time 7.07ms, mfu 17.73%\n",
            "iter 1710: loss 1.4276, time 7.00ms, mfu 17.74%\n",
            "iter 1720: loss 1.4404, time 6.89ms, mfu 17.77%\n",
            "iter 1730: loss 1.4177, time 6.96ms, mfu 17.79%\n",
            "iter 1740: loss 1.3902, time 7.09ms, mfu 17.77%\n",
            "iter 1750: loss 1.4294, time 6.96ms, mfu 17.78%\n",
            "iter 1760: loss 1.4116, time 7.19ms, mfu 17.74%\n",
            "iter 1770: loss 1.4249, time 7.25ms, mfu 17.69%\n",
            "iter 1780: loss 1.4154, time 7.13ms, mfu 17.67%\n",
            "iter 1790: loss 1.4021, time 7.07ms, mfu 17.67%\n",
            "iter 1800: loss 1.4170, time 7.04ms, mfu 17.67%\n",
            "iter 1810: loss 1.4098, time 7.03ms, mfu 17.68%\n",
            "iter 1820: loss 1.4267, time 7.15ms, mfu 17.66%\n",
            "iter 1830: loss 1.4188, time 7.01ms, mfu 17.67%\n",
            "iter 1840: loss 1.3815, time 7.12ms, mfu 17.65%\n",
            "iter 1850: loss 1.4275, time 6.93ms, mfu 17.69%\n",
            "iter 1860: loss 1.3846, time 7.31ms, mfu 17.63%\n",
            "iter 1870: loss 1.4091, time 7.02ms, mfu 17.64%\n",
            "iter 1880: loss 1.4120, time 6.90ms, mfu 17.68%\n",
            "iter 1890: loss 1.4000, time 7.79ms, mfu 17.52%\n",
            "iter 1900: loss 1.3976, time 7.65ms, mfu 17.39%\n",
            "iter 1910: loss 1.4058, time 7.12ms, mfu 17.41%\n",
            "iter 1920: loss 1.3787, time 6.93ms, mfu 17.47%\n",
            "iter 1930: loss 1.4010, time 6.89ms, mfu 17.53%\n",
            "iter 1940: loss 1.4037, time 7.31ms, mfu 17.48%\n",
            "iter 1950: loss 1.4225, time 7.03ms, mfu 17.51%\n",
            "iter 1960: loss 1.3550, time 6.85ms, mfu 17.58%\n",
            "iter 1970: loss 1.3968, time 6.87ms, mfu 17.64%\n",
            "iter 1980: loss 1.4483, time 7.25ms, mfu 17.59%\n",
            "iter 1990: loss 1.3934, time 6.92ms, mfu 17.64%\n",
            "step 2000: train loss 1.2777, val loss 1.5490\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2000: loss 1.3865, time 2984.72ms, mfu 15.88%\n",
            "iter 2010: loss 1.4131, time 7.37ms, mfu 15.98%\n",
            "iter 2020: loss 1.4059, time 7.11ms, mfu 16.14%\n",
            "iter 2030: loss 1.3997, time 7.01ms, mfu 16.31%\n",
            "iter 2040: loss 1.3674, time 7.04ms, mfu 16.45%\n",
            "iter 2050: loss 1.4049, time 7.06ms, mfu 16.57%\n",
            "iter 2060: loss 1.4175, time 6.99ms, mfu 16.70%\n",
            "iter 2070: loss 1.3883, time 6.85ms, mfu 16.85%\n",
            "iter 2080: loss 1.3705, time 6.96ms, mfu 16.95%\n",
            "iter 2090: loss 1.3804, time 7.06ms, mfu 17.03%\n",
            "iter 2100: loss 1.4244, time 6.97ms, mfu 17.11%\n",
            "iter 2110: loss 1.3927, time 6.90ms, mfu 17.21%\n",
            "iter 2120: loss 1.3836, time 7.17ms, mfu 17.23%\n",
            "iter 2130: loss 1.4137, time 6.92ms, mfu 17.31%\n",
            "iter 2140: loss 1.3974, time 6.83ms, mfu 17.40%\n",
            "iter 2150: loss 1.4085, time 7.08ms, mfu 17.43%\n",
            "iter 2160: loss 1.3735, time 6.84ms, mfu 17.51%\n",
            "iter 2170: loss 1.4151, time 6.98ms, mfu 17.54%\n",
            "iter 2180: loss 1.3992, time 6.77ms, mfu 17.63%\n",
            "iter 2190: loss 1.3670, time 7.00ms, mfu 17.65%\n",
            "iter 2200: loss 1.3389, time 6.89ms, mfu 17.70%\n",
            "iter 2210: loss 1.3640, time 6.90ms, mfu 17.74%\n",
            "iter 2220: loss 1.3540, time 7.10ms, mfu 17.72%\n",
            "iter 2230: loss 1.3744, time 6.91ms, mfu 17.75%\n",
            "iter 2240: loss 1.3853, time 6.89ms, mfu 17.79%\n",
            "iter 2250: loss 1.3608, time 6.99ms, mfu 17.79%\n",
            "iter 2260: loss 1.4102, time 6.84ms, mfu 17.84%\n",
            "iter 2270: loss 1.3672, time 6.88ms, mfu 17.87%\n",
            "iter 2280: loss 1.3829, time 6.97ms, mfu 17.87%\n",
            "iter 2290: loss 1.3922, time 6.75ms, mfu 17.93%\n",
            "iter 2300: loss 1.3903, time 7.15ms, mfu 17.88%\n",
            "iter 2310: loss 1.3872, time 6.83ms, mfu 17.92%\n",
            "iter 2320: loss 1.4105, time 8.36ms, mfu 17.62%\n",
            "iter 2330: loss 1.3821, time 7.01ms, mfu 17.64%\n",
            "iter 2340: loss 1.3522, time 7.08ms, mfu 17.63%\n",
            "iter 2350: loss 1.3584, time 7.09ms, mfu 17.63%\n",
            "iter 2360: loss 1.3780, time 7.05ms, mfu 17.64%\n",
            "iter 2370: loss 1.3926, time 7.83ms, mfu 17.47%\n",
            "iter 2380: loss 1.3823, time 6.97ms, mfu 17.51%\n",
            "iter 2390: loss 1.3796, time 6.92ms, mfu 17.56%\n",
            "iter 2400: loss 1.3681, time 7.27ms, mfu 17.52%\n",
            "iter 2410: loss 1.3635, time 7.33ms, mfu 17.47%\n",
            "iter 2420: loss 1.3945, time 7.19ms, mfu 17.46%\n",
            "iter 2430: loss 1.3356, time 6.88ms, mfu 17.53%\n",
            "iter 2440: loss 1.3740, time 7.15ms, mfu 17.52%\n",
            "iter 2450: loss 1.3389, time 7.04ms, mfu 17.54%\n",
            "iter 2460: loss 1.3634, time 7.00ms, mfu 17.57%\n",
            "iter 2470: loss 1.3456, time 7.02ms, mfu 17.59%\n",
            "iter 2480: loss 1.3623, time 7.06ms, mfu 17.60%\n",
            "iter 2490: loss 1.3714, time 7.14ms, mfu 17.58%\n",
            "iter 2500: loss 1.3637, time 6.82ms, mfu 17.65%\n",
            "iter 2510: loss 1.3518, time 6.99ms, mfu 17.68%\n",
            "iter 2520: loss 1.4180, time 7.04ms, mfu 17.68%\n",
            "iter 2530: loss 1.3633, time 6.89ms, mfu 17.72%\n",
            "iter 2540: loss 1.3320, time 6.78ms, mfu 17.79%\n",
            "iter 2550: loss 1.3355, time 6.97ms, mfu 17.80%\n",
            "iter 2560: loss 1.3668, time 7.02ms, mfu 17.80%\n",
            "iter 2570: loss 1.3662, time 6.94ms, mfu 17.82%\n",
            "iter 2580: loss 1.3750, time 6.98ms, mfu 17.82%\n",
            "iter 2590: loss 1.3668, time 7.21ms, mfu 17.77%\n",
            "iter 2600: loss 1.3600, time 6.86ms, mfu 17.81%\n",
            "iter 2610: loss 1.3521, time 7.13ms, mfu 17.78%\n",
            "iter 2620: loss 1.3713, time 7.01ms, mfu 17.78%\n",
            "iter 2630: loss 1.3276, time 7.21ms, mfu 17.73%\n",
            "iter 2640: loss 1.3465, time 6.92ms, mfu 17.76%\n",
            "iter 2650: loss 1.3328, time 7.07ms, mfu 17.75%\n",
            "iter 2660: loss 1.3740, time 6.94ms, mfu 17.78%\n",
            "iter 2670: loss 1.3530, time 6.97ms, mfu 17.79%\n",
            "iter 2680: loss 1.3790, time 6.77ms, mfu 17.85%\n",
            "iter 2690: loss 1.3578, time 7.02ms, mfu 17.84%\n",
            "iter 2700: loss 1.3340, time 7.08ms, mfu 17.82%\n",
            "iter 2710: loss 1.3459, time 6.89ms, mfu 17.85%\n",
            "iter 2720: loss 1.3386, time 7.05ms, mfu 17.83%\n",
            "iter 2730: loss 1.3412, time 7.00ms, mfu 17.83%\n",
            "iter 2740: loss 1.3313, time 6.91ms, mfu 17.85%\n",
            "iter 2750: loss 1.3542, time 6.96ms, mfu 17.86%\n",
            "iter 2760: loss 1.3673, time 6.99ms, mfu 17.86%\n",
            "iter 2770: loss 1.3375, time 6.85ms, mfu 17.89%\n",
            "iter 2780: loss 1.3583, time 6.85ms, mfu 17.93%\n",
            "iter 2790: loss 1.3362, time 7.05ms, mfu 17.90%\n",
            "iter 2800: loss 1.3347, time 6.91ms, mfu 17.92%\n",
            "iter 2810: loss 1.3463, time 7.03ms, mfu 17.90%\n",
            "iter 2820: loss 1.3313, time 6.91ms, mfu 17.92%\n",
            "iter 2830: loss 1.3255, time 6.81ms, mfu 17.96%\n",
            "iter 2840: loss 1.3477, time 6.88ms, mfu 17.97%\n",
            "iter 2850: loss 1.3300, time 6.74ms, mfu 18.03%\n",
            "iter 2860: loss 1.3443, time 6.79ms, mfu 18.06%\n",
            "iter 2870: loss 1.3664, time 7.06ms, mfu 18.02%\n",
            "iter 2880: loss 1.3580, time 7.40ms, mfu 17.90%\n",
            "iter 2890: loss 1.3523, time 7.22ms, mfu 17.84%\n",
            "iter 2900: loss 1.3155, time 6.96ms, mfu 17.85%\n",
            "iter 2910: loss 1.3129, time 6.90ms, mfu 17.87%\n",
            "iter 2920: loss 1.3526, time 6.89ms, mfu 17.90%\n",
            "iter 2930: loss 1.3070, time 6.90ms, mfu 17.91%\n",
            "iter 2940: loss 1.3295, time 6.89ms, mfu 17.93%\n",
            "iter 2950: loss 1.3417, time 6.90ms, mfu 17.95%\n",
            "iter 2960: loss 1.3214, time 7.26ms, mfu 17.87%\n",
            "iter 2970: loss 1.3392, time 7.16ms, mfu 17.83%\n",
            "iter 2980: loss 1.3228, time 7.07ms, mfu 17.81%\n",
            "iter 2990: loss 1.3302, time 7.08ms, mfu 17.79%\n",
            "step 3000: train loss 1.2056, val loss 1.5156\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 3000: loss 1.3148, time 2999.96ms, mfu 16.01%\n",
            "iter 3010: loss 1.3234, time 7.25ms, mfu 16.13%\n",
            "iter 3020: loss 1.3537, time 7.15ms, mfu 16.26%\n",
            "iter 3030: loss 1.3049, time 6.79ms, mfu 16.47%\n",
            "iter 3040: loss 1.3502, time 7.10ms, mfu 16.58%\n",
            "iter 3050: loss 1.3208, time 6.85ms, mfu 16.75%\n",
            "iter 3060: loss 1.3178, time 7.16ms, mfu 16.81%\n",
            "iter 3070: loss 1.3519, time 6.92ms, mfu 16.94%\n",
            "iter 3080: loss 1.3406, time 7.12ms, mfu 16.99%\n",
            "iter 3090: loss 1.3383, time 7.00ms, mfu 17.08%\n",
            "iter 3100: loss 1.3389, time 7.66ms, mfu 17.00%\n",
            "iter 3110: loss 1.3310, time 6.89ms, mfu 17.11%\n",
            "iter 3120: loss 1.3208, time 7.04ms, mfu 17.17%\n",
            "iter 3130: loss 1.3122, time 6.91ms, mfu 17.26%\n",
            "iter 3140: loss 1.2990, time 7.05ms, mfu 17.30%\n",
            "iter 3150: loss 1.2965, time 6.95ms, mfu 17.36%\n",
            "iter 3160: loss 1.3345, time 7.02ms, mfu 17.40%\n",
            "iter 3170: loss 1.3507, time 7.11ms, mfu 17.42%\n",
            "iter 3180: loss 1.3162, time 7.01ms, mfu 17.46%\n",
            "iter 3190: loss 1.3433, time 6.95ms, mfu 17.50%\n",
            "iter 3200: loss 1.3245, time 6.96ms, mfu 17.55%\n",
            "iter 3210: loss 1.3351, time 6.93ms, mfu 17.59%\n",
            "iter 3220: loss 1.3068, time 7.19ms, mfu 17.57%\n",
            "iter 3230: loss 1.3175, time 6.91ms, mfu 17.62%\n",
            "iter 3240: loss 1.3429, time 6.91ms, mfu 17.66%\n",
            "iter 3250: loss 1.3267, time 7.07ms, mfu 17.66%\n",
            "iter 3260: loss 1.3242, time 6.82ms, mfu 17.72%\n",
            "iter 3270: loss 1.3201, time 6.97ms, mfu 17.74%\n",
            "iter 3280: loss 1.3396, time 6.83ms, mfu 17.79%\n",
            "iter 3290: loss 1.3490, time 7.27ms, mfu 17.73%\n",
            "iter 3300: loss 1.3122, time 7.71ms, mfu 17.57%\n",
            "iter 3310: loss 1.3220, time 6.88ms, mfu 17.63%\n",
            "iter 3320: loss 1.2894, time 6.97ms, mfu 17.65%\n",
            "iter 3330: loss 1.3389, time 6.94ms, mfu 17.69%\n",
            "iter 3340: loss 1.3083, time 6.91ms, mfu 17.72%\n",
            "iter 3350: loss 1.3114, time 6.95ms, mfu 17.75%\n",
            "iter 3360: loss 1.3112, time 6.83ms, mfu 17.80%\n",
            "iter 3370: loss 1.3093, time 7.09ms, mfu 17.78%\n",
            "iter 3380: loss 1.2888, time 7.17ms, mfu 17.74%\n",
            "iter 3390: loss 1.2985, time 6.94ms, mfu 17.76%\n",
            "iter 3400: loss 1.3262, time 7.02ms, mfu 17.76%\n",
            "iter 3410: loss 1.3122, time 7.04ms, mfu 17.76%\n",
            "iter 3420: loss 1.3196, time 7.17ms, mfu 17.72%\n",
            "iter 3430: loss 1.2887, time 7.41ms, mfu 17.63%\n",
            "iter 3440: loss 1.2768, time 6.97ms, mfu 17.66%\n",
            "iter 3450: loss 1.3312, time 6.94ms, mfu 17.69%\n",
            "iter 3460: loss 1.3365, time 7.05ms, mfu 17.69%\n",
            "iter 3470: loss 1.3399, time 7.13ms, mfu 17.67%\n",
            "iter 3480: loss 1.2998, time 7.26ms, mfu 17.62%\n",
            "iter 3490: loss 1.3294, time 7.17ms, mfu 17.60%\n",
            "iter 3500: loss 1.2772, time 7.33ms, mfu 17.54%\n",
            "iter 3510: loss 1.3088, time 7.05ms, mfu 17.56%\n",
            "iter 3520: loss 1.3386, time 7.10ms, mfu 17.56%\n",
            "iter 3530: loss 1.3175, time 7.08ms, mfu 17.56%\n",
            "iter 3540: loss 1.3018, time 7.39ms, mfu 17.50%\n",
            "iter 3550: loss 1.3096, time 7.19ms, mfu 17.48%\n",
            "iter 3560: loss 1.3261, time 7.22ms, mfu 17.46%\n",
            "iter 3570: loss 1.3171, time 7.22ms, mfu 17.44%\n",
            "iter 3580: loss 1.3235, time 7.21ms, mfu 17.43%\n",
            "iter 3590: loss 1.3089, time 7.11ms, mfu 17.44%\n",
            "iter 3600: loss 1.2783, time 7.21ms, mfu 17.43%\n",
            "iter 3610: loss 1.3244, time 7.12ms, mfu 17.44%\n",
            "iter 3620: loss 1.2848, time 6.93ms, mfu 17.49%\n",
            "iter 3630: loss 1.3069, time 7.05ms, mfu 17.51%\n",
            "iter 3640: loss 1.2882, time 7.17ms, mfu 17.50%\n",
            "iter 3650: loss 1.2986, time 7.06ms, mfu 17.52%\n",
            "iter 3660: loss 1.2937, time 6.96ms, mfu 17.56%\n",
            "iter 3670: loss 1.3000, time 7.10ms, mfu 17.56%\n",
            "iter 3680: loss 1.3015, time 7.21ms, mfu 17.53%\n",
            "iter 3690: loss 1.2918, time 6.88ms, mfu 17.59%\n",
            "iter 3700: loss 1.2814, time 7.24ms, mfu 17.56%\n",
            "iter 3710: loss 1.3188, time 6.83ms, mfu 17.63%\n",
            "iter 3720: loss 1.2908, time 7.12ms, mfu 17.62%\n",
            "iter 3730: loss 1.2658, time 6.95ms, mfu 17.65%\n",
            "iter 3740: loss 1.3032, time 7.01ms, mfu 17.66%\n",
            "iter 3750: loss 1.3281, time 6.89ms, mfu 17.71%\n",
            "iter 3760: loss 1.3120, time 7.00ms, mfu 17.72%\n",
            "iter 3770: loss 1.2992, time 6.91ms, mfu 17.75%\n",
            "iter 3780: loss 1.3041, time 7.27ms, mfu 17.69%\n",
            "iter 3790: loss 1.2890, time 7.04ms, mfu 17.70%\n",
            "iter 3800: loss 1.2935, time 7.11ms, mfu 17.68%\n",
            "iter 3810: loss 1.2766, time 6.74ms, mfu 17.76%\n",
            "iter 3820: loss 1.3003, time 6.96ms, mfu 17.78%\n",
            "iter 3830: loss 1.3521, time 6.94ms, mfu 17.80%\n",
            "iter 3840: loss 1.2926, time 6.96ms, mfu 17.81%\n",
            "iter 3850: loss 1.2653, time 6.95ms, mfu 17.82%\n",
            "iter 3860: loss 1.3200, time 6.79ms, mfu 17.88%\n",
            "iter 3870: loss 1.2948, time 6.79ms, mfu 17.93%\n",
            "iter 3880: loss 1.2809, time 6.90ms, mfu 17.94%\n",
            "iter 3890: loss 1.3133, time 7.09ms, mfu 17.91%\n",
            "iter 3900: loss 1.3045, time 7.03ms, mfu 17.89%\n",
            "iter 3910: loss 1.2916, time 6.81ms, mfu 17.94%\n",
            "iter 3920: loss 1.2891, time 6.96ms, mfu 17.93%\n",
            "iter 3930: loss 1.3189, time 6.98ms, mfu 17.93%\n",
            "iter 3940: loss 1.3071, time 6.77ms, mfu 17.98%\n",
            "iter 3950: loss 1.3005, time 7.02ms, mfu 17.96%\n",
            "iter 3960: loss 1.3227, time 6.77ms, mfu 18.00%\n",
            "iter 3970: loss 1.2939, time 7.09ms, mfu 17.96%\n",
            "iter 3980: loss 1.2876, time 6.84ms, mfu 17.99%\n",
            "iter 3990: loss 1.3177, time 6.81ms, mfu 18.02%\n",
            "step 4000: train loss 1.1642, val loss 1.4989\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 4000: loss 1.2934, time 2997.52ms, mfu 16.22%\n",
            "iter 4010: loss 1.2729, time 7.12ms, mfu 16.36%\n",
            "iter 4020: loss 1.2981, time 7.01ms, mfu 16.50%\n",
            "iter 4030: loss 1.2890, time 6.96ms, mfu 16.64%\n",
            "iter 4040: loss 1.3190, time 7.11ms, mfu 16.73%\n",
            "iter 4050: loss 1.2568, time 6.99ms, mfu 16.84%\n",
            "iter 4060: loss 1.3054, time 7.48ms, mfu 16.83%\n",
            "iter 4070: loss 1.2878, time 7.48ms, mfu 16.81%\n",
            "iter 4080: loss 1.3044, time 7.40ms, mfu 16.82%\n",
            "iter 4090: loss 1.2991, time 6.91ms, mfu 16.94%\n",
            "iter 4100: loss 1.2764, time 7.01ms, mfu 17.02%\n",
            "iter 4110: loss 1.2904, time 6.88ms, mfu 17.13%\n",
            "iter 4120: loss 1.2903, time 6.87ms, mfu 17.24%\n",
            "iter 4130: loss 1.2864, time 6.90ms, mfu 17.32%\n",
            "iter 4140: loss 1.2895, time 6.96ms, mfu 17.38%\n",
            "iter 4150: loss 1.3089, time 7.17ms, mfu 17.38%\n",
            "iter 4160: loss 1.2645, time 6.94ms, mfu 17.44%\n",
            "iter 4170: loss 1.2983, time 6.81ms, mfu 17.53%\n",
            "iter 4180: loss 1.2892, time 7.20ms, mfu 17.51%\n",
            "iter 4190: loss 1.2665, time 6.98ms, mfu 17.55%\n",
            "iter 4200: loss 1.3140, time 6.95ms, mfu 17.59%\n",
            "iter 4210: loss 1.2683, time 6.87ms, mfu 17.64%\n",
            "iter 4220: loss 1.2466, time 7.08ms, mfu 17.64%\n",
            "iter 4230: loss 1.2782, time 7.08ms, mfu 17.64%\n",
            "iter 4240: loss 1.2644, time 6.96ms, mfu 17.67%\n",
            "iter 4250: loss 1.3053, time 6.96ms, mfu 17.69%\n",
            "iter 4260: loss 1.2888, time 7.04ms, mfu 17.69%\n",
            "iter 4270: loss 1.2894, time 7.07ms, mfu 17.69%\n",
            "iter 4280: loss 1.3086, time 6.96ms, mfu 17.71%\n",
            "iter 4290: loss 1.2964, time 7.02ms, mfu 17.72%\n",
            "iter 4300: loss 1.2786, time 7.18ms, mfu 17.68%\n",
            "iter 4310: loss 1.2706, time 7.25ms, mfu 17.64%\n",
            "iter 4320: loss 1.2685, time 6.86ms, mfu 17.69%\n",
            "iter 4330: loss 1.2605, time 6.83ms, mfu 17.75%\n",
            "iter 4340: loss 1.2860, time 6.85ms, mfu 17.79%\n",
            "iter 4350: loss 1.2885, time 7.13ms, mfu 17.76%\n",
            "iter 4360: loss 1.2971, time 6.97ms, mfu 17.78%\n",
            "iter 4370: loss 1.2828, time 7.13ms, mfu 17.75%\n",
            "iter 4380: loss 1.2837, time 6.84ms, mfu 17.80%\n",
            "iter 4390: loss 1.2562, time 6.99ms, mfu 17.80%\n",
            "iter 4400: loss 1.3010, time 7.03ms, mfu 17.80%\n",
            "iter 4410: loss 1.2795, time 7.03ms, mfu 17.79%\n",
            "iter 4420: loss 1.2995, time 6.90ms, mfu 17.82%\n",
            "iter 4430: loss 1.3091, time 6.81ms, mfu 17.87%\n",
            "iter 4440: loss 1.2745, time 6.89ms, mfu 17.89%\n",
            "iter 4450: loss 1.3037, time 7.14ms, mfu 17.85%\n",
            "iter 4460: loss 1.2547, time 7.11ms, mfu 17.82%\n",
            "iter 4470: loss 1.2771, time 6.86ms, mfu 17.86%\n",
            "iter 4480: loss 1.2754, time 6.93ms, mfu 17.87%\n",
            "iter 4490: loss 1.2952, time 6.87ms, mfu 17.90%\n",
            "iter 4500: loss 1.2659, time 6.74ms, mfu 17.96%\n",
            "iter 4510: loss 1.2506, time 6.77ms, mfu 18.01%\n",
            "iter 4520: loss 1.3025, time 6.95ms, mfu 18.00%\n",
            "iter 4530: loss 1.2891, time 6.88ms, mfu 18.01%\n",
            "iter 4540: loss 1.2893, time 6.97ms, mfu 18.00%\n",
            "iter 4550: loss 1.2778, time 7.25ms, mfu 17.92%\n",
            "iter 4560: loss 1.2582, time 7.00ms, mfu 17.91%\n",
            "iter 4570: loss 1.2570, time 7.08ms, mfu 17.88%\n",
            "iter 4580: loss 1.2558, time 7.14ms, mfu 17.84%\n",
            "iter 4590: loss 1.3088, time 6.94ms, mfu 17.85%\n",
            "iter 4600: loss 1.2738, time 6.84ms, mfu 17.89%\n",
            "iter 4610: loss 1.2686, time 6.98ms, mfu 17.89%\n",
            "iter 4620: loss 1.2600, time 7.04ms, mfu 17.87%\n",
            "iter 4630: loss 1.2972, time 6.81ms, mfu 17.92%\n",
            "iter 4640: loss 1.3018, time 7.80ms, mfu 17.72%\n",
            "iter 4650: loss 1.2927, time 7.27ms, mfu 17.67%\n",
            "iter 4660: loss 1.2871, time 7.23ms, mfu 17.63%\n",
            "iter 4670: loss 1.2776, time 7.26ms, mfu 17.58%\n",
            "iter 4680: loss 1.2718, time 7.05ms, mfu 17.59%\n",
            "iter 4690: loss 1.2733, time 7.32ms, mfu 17.54%\n",
            "iter 4700: loss 1.2594, time 7.34ms, mfu 17.48%\n",
            "iter 4710: loss 1.3042, time 7.12ms, mfu 17.49%\n",
            "iter 4720: loss 1.3032, time 7.34ms, mfu 17.44%\n",
            "iter 4730: loss 1.2604, time 7.34ms, mfu 17.39%\n",
            "iter 4740: loss 1.2782, time 6.98ms, mfu 17.44%\n",
            "iter 4750: loss 1.2709, time 6.84ms, mfu 17.52%\n",
            "iter 4760: loss 1.2642, time 7.17ms, mfu 17.51%\n",
            "iter 4770: loss 1.2492, time 7.29ms, mfu 17.47%\n",
            "iter 4780: loss 1.2943, time 7.11ms, mfu 17.47%\n",
            "iter 4790: loss 1.3051, time 6.97ms, mfu 17.51%\n",
            "iter 4800: loss 1.2616, time 7.03ms, mfu 17.54%\n",
            "iter 4810: loss 1.2738, time 6.97ms, mfu 17.57%\n",
            "iter 4820: loss 1.2962, time 6.96ms, mfu 17.61%\n",
            "iter 4830: loss 1.2689, time 7.14ms, mfu 17.60%\n",
            "iter 4840: loss 1.2418, time 7.03ms, mfu 17.61%\n",
            "iter 4850: loss 1.2615, time 7.05ms, mfu 17.62%\n",
            "iter 4860: loss 1.2777, time 6.96ms, mfu 17.65%\n",
            "iter 4870: loss 1.2664, time 7.00ms, mfu 17.67%\n",
            "iter 4880: loss 1.2374, time 7.01ms, mfu 17.68%\n",
            "iter 4890: loss 1.2812, time 6.92ms, mfu 17.71%\n",
            "iter 4900: loss 1.2583, time 7.26ms, mfu 17.66%\n",
            "iter 4910: loss 1.2504, time 7.39ms, mfu 17.58%\n",
            "iter 4920: loss 1.2586, time 7.08ms, mfu 17.59%\n",
            "iter 4930: loss 1.3025, time 7.09ms, mfu 17.59%\n",
            "iter 4940: loss 1.2911, time 6.99ms, mfu 17.61%\n",
            "iter 4950: loss 1.2718, time 6.93ms, mfu 17.65%\n",
            "iter 4960: loss 1.2252, time 7.27ms, mfu 17.60%\n",
            "iter 4970: loss 1.2759, time 6.87ms, mfu 17.66%\n",
            "iter 4980: loss 1.2939, time 7.03ms, mfu 17.67%\n",
            "iter 4990: loss 1.2505, time 7.00ms, mfu 17.68%\n",
            "step 5000: train loss 1.1382, val loss 1.4958\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 5000: loss 1.2646, time 3001.08ms, mfu 15.92%\n",
            "iter 5010: loss 1.2905, time 7.10ms, mfu 16.08%\n",
            "iter 5020: loss 1.2579, time 6.84ms, mfu 16.30%\n",
            "iter 5030: loss 1.2709, time 6.98ms, mfu 16.45%\n",
            "iter 5040: loss 1.2461, time 6.93ms, mfu 16.61%\n",
            "iter 5050: loss 1.3125, time 7.00ms, mfu 16.73%\n",
            "iter 5060: loss 1.2691, time 7.10ms, mfu 16.81%\n",
            "iter 5070: loss 1.2743, time 7.09ms, mfu 16.89%\n",
            "iter 5080: loss 1.2484, time 6.83ms, mfu 17.03%\n",
            "iter 5090: loss 1.2389, time 6.93ms, mfu 17.13%\n",
            "iter 5100: loss 1.2490, time 7.17ms, mfu 17.15%\n",
            "iter 5110: loss 1.2610, time 6.94ms, mfu 17.24%\n",
            "iter 5120: loss 1.2460, time 6.96ms, mfu 17.30%\n",
            "iter 5130: loss 1.2901, time 7.38ms, mfu 17.26%\n",
            "iter 5140: loss 1.2381, time 7.01ms, mfu 17.32%\n",
            "iter 5150: loss 1.2795, time 7.07ms, mfu 17.35%\n",
            "iter 5160: loss 1.2810, time 7.26ms, mfu 17.33%\n",
            "iter 5170: loss 1.2687, time 6.93ms, mfu 17.40%\n",
            "iter 5180: loss 1.2448, time 7.09ms, mfu 17.42%\n",
            "iter 5190: loss 1.2899, time 7.46ms, mfu 17.35%\n",
            "iter 5200: loss 1.2507, time 7.10ms, mfu 17.37%\n",
            "iter 5210: loss 1.2709, time 6.99ms, mfu 17.42%\n",
            "iter 5220: loss 1.2375, time 7.15ms, mfu 17.42%\n",
            "iter 5230: loss 1.2769, time 7.00ms, mfu 17.46%\n",
            "iter 5240: loss 1.2640, time 6.86ms, mfu 17.53%\n",
            "iter 5250: loss 1.2940, time 6.74ms, mfu 17.63%\n",
            "iter 5260: loss 1.2779, time 7.05ms, mfu 17.64%\n",
            "iter 5270: loss 1.2991, time 6.72ms, mfu 17.73%\n",
            "iter 5280: loss 1.2474, time 7.00ms, mfu 17.74%\n",
            "iter 5290: loss 1.2491, time 6.89ms, mfu 17.77%\n",
            "iter 5300: loss 1.2714, time 6.82ms, mfu 17.83%\n",
            "iter 5310: loss 1.2654, time 6.76ms, mfu 17.89%\n",
            "iter 5320: loss 1.2586, time 7.25ms, mfu 17.82%\n",
            "iter 5330: loss 1.2909, time 6.95ms, mfu 17.83%\n",
            "iter 5340: loss 1.2555, time 6.75ms, mfu 17.90%\n",
            "iter 5350: loss 1.2563, time 7.01ms, mfu 17.89%\n",
            "iter 5360: loss 1.2559, time 6.96ms, mfu 17.89%\n",
            "iter 5370: loss 1.2456, time 7.00ms, mfu 17.88%\n",
            "iter 5380: loss 1.2605, time 6.72ms, mfu 17.95%\n",
            "iter 5390: loss 1.2965, time 6.76ms, mfu 18.00%\n",
            "iter 5400: loss 1.2745, time 6.89ms, mfu 18.01%\n",
            "iter 5410: loss 1.2618, time 6.85ms, mfu 18.03%\n",
            "iter 5420: loss 1.2727, time 6.95ms, mfu 18.02%\n",
            "iter 5430: loss 1.2320, time 7.14ms, mfu 17.97%\n",
            "iter 5440: loss 1.2788, time 6.69ms, mfu 18.04%\n",
            "iter 5450: loss 1.2393, time 6.77ms, mfu 18.07%\n",
            "iter 5460: loss 1.2267, time 6.87ms, mfu 18.08%\n",
            "iter 5470: loss 1.2587, time 7.01ms, mfu 18.05%\n",
            "iter 5480: loss 1.2872, time 6.85ms, mfu 18.07%\n",
            "iter 5490: loss 1.2815, time 7.05ms, mfu 18.03%\n",
            "iter 5500: loss 1.2716, time 6.88ms, mfu 18.04%\n",
            "iter 5510: loss 1.2626, time 6.99ms, mfu 18.02%\n",
            "iter 5520: loss 1.2629, time 6.97ms, mfu 18.01%\n",
            "iter 5530: loss 1.2588, time 6.94ms, mfu 18.00%\n",
            "iter 5540: loss 1.2675, time 6.92ms, mfu 18.01%\n",
            "iter 5550: loss 1.2536, time 6.89ms, mfu 18.02%\n",
            "iter 5560: loss 1.2383, time 6.69ms, mfu 18.08%\n",
            "iter 5570: loss 1.3109, time 6.83ms, mfu 18.10%\n",
            "iter 5580: loss 1.2606, time 6.89ms, mfu 18.10%\n",
            "iter 5590: loss 1.2812, time 7.13ms, mfu 18.04%\n",
            "iter 5600: loss 1.3032, time 7.02ms, mfu 18.01%\n",
            "iter 5610: loss 1.2754, time 6.83ms, mfu 18.04%\n",
            "iter 5620: loss 1.2792, time 6.99ms, mfu 18.02%\n",
            "iter 5630: loss 1.2715, time 6.77ms, mfu 18.06%\n",
            "iter 5640: loss 1.2694, time 6.89ms, mfu 18.06%\n",
            "iter 5650: loss 1.2572, time 6.97ms, mfu 18.05%\n",
            "iter 5660: loss 1.2796, time 6.87ms, mfu 18.06%\n",
            "iter 5670: loss 1.2856, time 6.99ms, mfu 18.04%\n",
            "iter 5680: loss 1.2642, time 6.99ms, mfu 18.02%\n",
            "iter 5690: loss 1.2637, time 7.00ms, mfu 18.00%\n",
            "iter 5700: loss 1.2726, time 7.06ms, mfu 17.96%\n",
            "iter 5710: loss 1.2777, time 7.05ms, mfu 17.94%\n",
            "iter 5720: loss 1.2742, time 7.16ms, mfu 17.89%\n",
            "iter 5730: loss 1.2301, time 6.90ms, mfu 17.91%\n",
            "iter 5740: loss 1.2594, time 7.14ms, mfu 17.86%\n",
            "iter 5750: loss 1.2811, time 6.93ms, mfu 17.88%\n",
            "iter 5760: loss 1.2450, time 6.77ms, mfu 17.93%\n",
            "iter 5770: loss 1.3032, time 7.09ms, mfu 17.90%\n",
            "iter 5780: loss 1.2576, time 6.92ms, mfu 17.91%\n",
            "iter 5790: loss 1.2339, time 6.85ms, mfu 17.94%\n",
            "iter 5800: loss 1.2662, time 6.83ms, mfu 17.97%\n",
            "iter 5810: loss 1.2495, time 6.75ms, mfu 18.02%\n",
            "iter 5820: loss 1.2565, time 7.18ms, mfu 17.96%\n",
            "iter 5830: loss 1.2860, time 7.00ms, mfu 17.94%\n",
            "iter 5840: loss 1.2560, time 7.03ms, mfu 17.92%\n",
            "iter 5850: loss 1.2440, time 6.86ms, mfu 17.95%\n",
            "iter 5860: loss 1.2748, time 8.87ms, mfu 17.56%\n",
            "iter 5870: loss 1.2309, time 6.74ms, mfu 17.65%\n",
            "iter 5880: loss 1.2913, time 6.79ms, mfu 17.73%\n",
            "iter 5890: loss 1.2555, time 6.74ms, mfu 17.80%\n",
            "iter 5900: loss 1.2654, time 6.84ms, mfu 17.85%\n",
            "iter 5910: loss 1.2318, time 6.99ms, mfu 17.85%\n",
            "iter 5920: loss 1.2437, time 6.82ms, mfu 17.89%\n",
            "iter 5930: loss 1.2518, time 6.83ms, mfu 17.93%\n",
            "iter 5940: loss 1.2639, time 7.20ms, mfu 17.87%\n",
            "iter 5950: loss 1.2931, time 6.82ms, mfu 17.91%\n",
            "iter 5960: loss 1.2750, time 6.83ms, mfu 17.95%\n",
            "iter 5970: loss 1.2537, time 6.83ms, mfu 17.98%\n",
            "iter 5980: loss 1.2891, time 6.75ms, mfu 18.03%\n",
            "iter 5990: loss 1.2786, time 6.82ms, mfu 18.05%\n",
            "step 6000: train loss 1.1271, val loss 1.4934\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 6000: loss 1.2680, time 3004.74ms, mfu 16.25%\n",
            "Extracted losses: train=1.1382, val=1.4958\n",
            "\n",
            "Training model with 3 layers and 4 heads\n",
            "Running command: python train.py config/train_shakespeare_char_3layer_4head.py\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "W0503 17:03:34.238000 11439 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "Overriding config with config/train_shakespeare_char_3layer_4head.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 1000 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 3\n",
            "n_head = 4\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 6000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 5.34M\n",
            "num decayed parameter tensors: 14, with 5,431,680 parameters\n",
            "num non-decayed parameter tensors: 7, with 2,688 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 4.2420, val loss 4.2383\n",
            "iter 0: loss 4.2506, time 14811.72ms, mfu -100.00%\n",
            "iter 10: loss 3.2542, time 8.18ms, mfu 22.83%\n",
            "iter 20: loss 2.8211, time 14.65ms, mfu 21.82%\n",
            "iter 30: loss 2.6140, time 15.57ms, mfu 20.84%\n",
            "iter 40: loss 2.5612, time 19.15ms, mfu 19.73%\n",
            "iter 50: loss 2.5283, time 25.95ms, mfu 18.48%\n",
            "iter 60: loss 2.4928, time 16.32ms, mfu 17.77%\n",
            "iter 70: loss 2.4871, time 16.10ms, mfu 17.16%\n",
            "iter 80: loss 2.5063, time 16.22ms, mfu 16.59%\n",
            "iter 90: loss 2.4725, time 22.36ms, mfu 15.77%\n",
            "iter 100: loss 2.4614, time 18.20ms, mfu 15.22%\n",
            "iter 110: loss 2.4504, time 14.22ms, mfu 15.01%\n",
            "iter 120: loss 2.4479, time 23.36ms, mfu 14.31%\n",
            "iter 130: loss 2.4207, time 18.11ms, mfu 13.91%\n",
            "iter 140: loss 2.4262, time 16.17ms, mfu 13.67%\n",
            "iter 150: loss 2.4052, time 18.28ms, mfu 13.32%\n",
            "iter 160: loss 2.3611, time 24.55ms, mfu 12.75%\n",
            "iter 170: loss 2.3923, time 16.87ms, mfu 12.58%\n",
            "iter 180: loss 2.2874, time 16.77ms, mfu 12.44%\n",
            "iter 190: loss 2.2193, time 20.62ms, mfu 12.10%\n",
            "iter 200: loss 2.1799, time 21.12ms, mfu 11.77%\n",
            "iter 210: loss 2.1293, time 17.55ms, mfu 11.66%\n",
            "iter 220: loss 2.0870, time 18.36ms, mfu 11.51%\n",
            "iter 230: loss 2.0741, time 18.13ms, mfu 11.39%\n",
            "iter 240: loss 2.0350, time 22.40ms, mfu 11.08%\n",
            "iter 250: loss 2.0315, time 16.18ms, mfu 11.13%\n",
            "iter 260: loss 2.0041, time 18.33ms, mfu 11.04%\n",
            "iter 270: loss 1.9470, time 20.09ms, mfu 10.86%\n",
            "iter 280: loss 1.9701, time 15.90ms, mfu 10.95%\n",
            "iter 290: loss 1.9127, time 16.24ms, mfu 11.00%\n",
            "iter 300: loss 1.8899, time 19.60ms, mfu 10.86%\n",
            "iter 310: loss 1.8776, time 16.96ms, mfu 10.87%\n",
            "iter 320: loss 1.8654, time 16.52ms, mfu 10.91%\n",
            "iter 330: loss 1.8218, time 14.89ms, mfu 11.08%\n",
            "iter 340: loss 1.8616, time 15.40ms, mfu 11.18%\n",
            "iter 350: loss 1.8086, time 19.77ms, mfu 11.01%\n",
            "iter 360: loss 1.7798, time 19.42ms, mfu 10.87%\n",
            "iter 370: loss 1.7858, time 19.44ms, mfu 10.74%\n",
            "iter 380: loss 1.7673, time 19.68ms, mfu 10.62%\n",
            "iter 390: loss 1.7749, time 20.88ms, mfu 10.45%\n",
            "iter 400: loss 1.7816, time 17.31ms, mfu 10.48%\n",
            "iter 410: loss 1.7334, time 18.63ms, mfu 10.44%\n",
            "iter 420: loss 1.7198, time 17.41ms, mfu 10.47%\n",
            "iter 430: loss 1.7149, time 19.78ms, mfu 10.36%\n",
            "iter 440: loss 1.7290, time 16.98ms, mfu 10.43%\n",
            "iter 450: loss 1.7229, time 19.08ms, mfu 10.36%\n",
            "iter 460: loss 1.6818, time 18.45ms, mfu 10.34%\n",
            "iter 470: loss 1.6905, time 19.38ms, mfu 10.27%\n",
            "iter 480: loss 1.6992, time 18.02ms, mfu 10.28%\n",
            "iter 490: loss 1.7033, time 18.02ms, mfu 10.29%\n",
            "iter 500: loss 1.6871, time 20.28ms, mfu 10.18%\n",
            "iter 510: loss 1.6842, time 18.34ms, mfu 10.18%\n",
            "iter 520: loss 1.6464, time 15.98ms, mfu 10.33%\n",
            "iter 530: loss 1.6452, time 19.66ms, mfu 10.25%\n",
            "iter 540: loss 1.6634, time 19.63ms, mfu 10.17%\n",
            "iter 550: loss 1.6213, time 19.60ms, mfu 10.11%\n",
            "iter 560: loss 1.6269, time 20.45ms, mfu 10.01%\n",
            "iter 570: loss 1.6304, time 19.70ms, mfu 9.96%\n",
            "iter 580: loss 1.6310, time 20.91ms, mfu 9.85%\n",
            "iter 590: loss 1.6468, time 20.60ms, mfu 9.77%\n",
            "iter 600: loss 1.5962, time 19.92ms, mfu 9.73%\n",
            "iter 610: loss 1.5953, time 20.66ms, mfu 9.66%\n",
            "iter 620: loss 1.5857, time 18.45ms, mfu 9.71%\n",
            "iter 630: loss 1.5881, time 23.07ms, mfu 9.55%\n",
            "iter 640: loss 1.5803, time 20.78ms, mfu 9.49%\n",
            "iter 650: loss 1.5934, time 21.38ms, mfu 9.42%\n",
            "iter 660: loss 1.5879, time 20.33ms, mfu 9.39%\n",
            "iter 670: loss 1.5730, time 19.07ms, mfu 9.43%\n",
            "iter 680: loss 1.5230, time 17.61ms, mfu 9.55%\n",
            "iter 690: loss 1.5925, time 18.59ms, mfu 9.60%\n",
            "iter 700: loss 1.5964, time 21.58ms, mfu 9.50%\n",
            "iter 710: loss 1.5749, time 18.69ms, mfu 9.55%\n",
            "iter 720: loss 1.5543, time 18.97ms, mfu 9.58%\n",
            "iter 730: loss 1.5969, time 20.61ms, mfu 9.53%\n",
            "iter 740: loss 1.5269, time 20.89ms, mfu 9.47%\n",
            "iter 750: loss 1.5224, time 16.73ms, mfu 9.64%\n",
            "iter 760: loss 1.5458, time 20.11ms, mfu 9.60%\n",
            "iter 770: loss 1.5478, time 16.38ms, mfu 9.78%\n",
            "iter 780: loss 1.5273, time 18.86ms, mfu 9.79%\n",
            "iter 790: loss 1.5174, time 16.53ms, mfu 9.94%\n",
            "iter 800: loss 1.5621, time 23.06ms, mfu 9.76%\n",
            "iter 810: loss 1.5128, time 20.25ms, mfu 9.71%\n",
            "iter 820: loss 1.5394, time 20.16ms, mfu 9.66%\n",
            "iter 830: loss 1.5437, time 19.82ms, mfu 9.64%\n",
            "iter 840: loss 1.4939, time 22.83ms, mfu 9.49%\n",
            "iter 850: loss 1.5270, time 18.73ms, mfu 9.54%\n",
            "iter 860: loss 1.5117, time 19.22ms, mfu 9.56%\n",
            "iter 870: loss 1.5056, time 23.20ms, mfu 9.41%\n",
            "iter 880: loss 1.4881, time 20.49ms, mfu 9.38%\n",
            "iter 890: loss 1.5166, time 17.76ms, mfu 9.49%\n",
            "iter 900: loss 1.5188, time 17.40ms, mfu 9.61%\n",
            "iter 910: loss 1.5138, time 20.74ms, mfu 9.55%\n",
            "iter 920: loss 1.4816, time 19.26ms, mfu 9.57%\n",
            "iter 930: loss 1.4806, time 21.89ms, mfu 9.46%\n",
            "iter 940: loss 1.4824, time 19.59ms, mfu 9.47%\n",
            "iter 950: loss 1.4854, time 17.60ms, mfu 9.58%\n",
            "iter 960: loss 1.4826, time 20.84ms, mfu 9.52%\n",
            "iter 970: loss 1.4536, time 21.44ms, mfu 9.44%\n",
            "iter 980: loss 1.4984, time 18.85ms, mfu 9.49%\n",
            "iter 990: loss 1.4534, time 22.51ms, mfu 9.37%\n",
            "step 1000: train loss 1.3603, val loss 1.5940\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.4833, time 4088.47ms, mfu 8.44%\n",
            "iter 1010: loss 1.4387, time 11.31ms, mfu 9.24%\n",
            "iter 1020: loss 1.4608, time 16.81ms, mfu 9.43%\n",
            "iter 1030: loss 1.4397, time 15.00ms, mfu 9.73%\n",
            "iter 1040: loss 1.4747, time 19.42ms, mfu 9.72%\n",
            "iter 1050: loss 1.4787, time 20.49ms, mfu 9.66%\n",
            "iter 1060: loss 1.4846, time 15.09ms, mfu 9.93%\n",
            "iter 1070: loss 1.4402, time 15.42ms, mfu 10.15%\n",
            "iter 1080: loss 1.4447, time 26.00ms, mfu 9.85%\n",
            "iter 1090: loss 1.4338, time 24.35ms, mfu 9.63%\n",
            "iter 1100: loss 1.4408, time 20.33ms, mfu 9.59%\n",
            "iter 1110: loss 1.4302, time 19.68ms, mfu 9.58%\n",
            "iter 1120: loss 1.4492, time 25.93ms, mfu 9.34%\n",
            "iter 1130: loss 1.4498, time 16.21ms, mfu 9.56%\n",
            "iter 1140: loss 1.4267, time 17.41ms, mfu 9.67%\n",
            "iter 1150: loss 1.4562, time 21.29ms, mfu 9.58%\n",
            "iter 1160: loss 1.3963, time 22.68ms, mfu 9.45%\n",
            "iter 1170: loss 1.4299, time 21.56ms, mfu 9.37%\n",
            "iter 1180: loss 1.4047, time 19.14ms, mfu 9.41%\n",
            "iter 1190: loss 1.3944, time 23.80ms, mfu 9.25%\n",
            "iter 1200: loss 1.3829, time 18.14ms, mfu 9.36%\n",
            "iter 1210: loss 1.4206, time 22.15ms, mfu 9.26%\n",
            "iter 1220: loss 1.4498, time 20.46ms, mfu 9.25%\n",
            "iter 1230: loss 1.4224, time 22.37ms, mfu 9.16%\n",
            "iter 1240: loss 1.4353, time 21.73ms, mfu 9.10%\n",
            "iter 1250: loss 1.4209, time 19.42ms, mfu 9.15%\n",
            "iter 1260: loss 1.4149, time 21.52ms, mfu 9.11%\n",
            "iter 1270: loss 1.4341, time 21.51ms, mfu 9.06%\n",
            "iter 1280: loss 1.4143, time 16.39ms, mfu 9.30%\n",
            "iter 1290: loss 1.3991, time 19.88ms, mfu 9.31%\n",
            "iter 1300: loss 1.4126, time 22.62ms, mfu 9.20%\n",
            "iter 1310: loss 1.4138, time 21.44ms, mfu 9.15%\n",
            "iter 1320: loss 1.4243, time 21.20ms, mfu 9.12%\n",
            "iter 1330: loss 1.4158, time 21.25ms, mfu 9.08%\n",
            "iter 1340: loss 1.3855, time 21.92ms, mfu 9.03%\n",
            "iter 1350: loss 1.4118, time 19.06ms, mfu 9.10%\n",
            "iter 1360: loss 1.4069, time 21.50ms, mfu 9.06%\n",
            "iter 1370: loss 1.4016, time 18.99ms, mfu 9.14%\n",
            "iter 1380: loss 1.3879, time 21.95ms, mfu 9.08%\n",
            "iter 1390: loss 1.3644, time 19.48ms, mfu 9.13%\n",
            "iter 1400: loss 1.3491, time 20.08ms, mfu 9.14%\n",
            "iter 1410: loss 1.4102, time 21.26ms, mfu 9.11%\n",
            "iter 1420: loss 1.3749, time 19.17ms, mfu 9.17%\n",
            "iter 1430: loss 1.4068, time 16.10ms, mfu 9.41%\n",
            "iter 1440: loss 1.4254, time 21.06ms, mfu 9.36%\n",
            "iter 1450: loss 1.3858, time 23.26ms, mfu 9.23%\n",
            "iter 1460: loss 1.3873, time 19.43ms, mfu 9.26%\n",
            "iter 1470: loss 1.3823, time 18.26ms, mfu 9.36%\n",
            "iter 1480: loss 1.3608, time 18.48ms, mfu 9.43%\n",
            "iter 1490: loss 1.3540, time 20.49ms, mfu 9.40%\n",
            "iter 1500: loss 1.3702, time 20.48ms, mfu 9.37%\n",
            "iter 1510: loss 1.3865, time 20.47ms, mfu 9.35%\n",
            "iter 1520: loss 1.3640, time 21.92ms, mfu 9.27%\n",
            "iter 1530: loss 1.3701, time 18.56ms, mfu 9.34%\n",
            "iter 1540: loss 1.3935, time 21.39ms, mfu 9.28%\n",
            "iter 1550: loss 1.3757, time 20.99ms, mfu 9.24%\n",
            "iter 1560: loss 1.3774, time 21.61ms, mfu 9.18%\n",
            "iter 1570: loss 1.3779, time 18.95ms, mfu 9.25%\n",
            "iter 1580: loss 1.3733, time 20.15ms, mfu 9.25%\n",
            "iter 1590: loss 1.3556, time 19.92ms, mfu 9.26%\n",
            "iter 1600: loss 1.3800, time 22.08ms, mfu 9.18%\n",
            "iter 1610: loss 1.3800, time 19.51ms, mfu 9.22%\n",
            "iter 1620: loss 1.3524, time 21.38ms, mfu 9.17%\n",
            "iter 1630: loss 1.3589, time 20.83ms, mfu 9.15%\n",
            "iter 1640: loss 1.3822, time 19.51ms, mfu 9.19%\n",
            "iter 1650: loss 1.3672, time 19.10ms, mfu 9.25%\n",
            "iter 1660: loss 1.3652, time 22.20ms, mfu 9.17%\n",
            "iter 1670: loss 1.3362, time 19.20ms, mfu 9.22%\n",
            "iter 1680: loss 1.3546, time 16.26ms, mfu 9.45%\n",
            "iter 1690: loss 1.3479, time 19.89ms, mfu 9.44%\n",
            "iter 1700: loss 1.4098, time 21.13ms, mfu 9.38%\n",
            "iter 1710: loss 1.3179, time 20.49ms, mfu 9.36%\n",
            "iter 1720: loss 1.3436, time 19.91ms, mfu 9.36%\n",
            "iter 1730: loss 1.3428, time 19.70ms, mfu 9.37%\n",
            "iter 1740: loss 1.3307, time 19.38ms, mfu 9.40%\n",
            "iter 1750: loss 1.3018, time 21.50ms, mfu 9.33%\n",
            "iter 1760: loss 1.3487, time 18.66ms, mfu 9.39%\n",
            "iter 1770: loss 1.3799, time 11.79ms, mfu 10.04%\n",
            "iter 1780: loss 1.3533, time 18.07ms, mfu 10.07%\n",
            "iter 1790: loss 1.3279, time 17.05ms, mfu 10.16%\n",
            "iter 1800: loss 1.3209, time 19.44ms, mfu 10.10%\n",
            "iter 1810: loss 1.3428, time 20.57ms, mfu 10.00%\n",
            "iter 1820: loss 1.3397, time 21.75ms, mfu 9.86%\n",
            "iter 1830: loss 1.3527, time 20.21ms, mfu 9.79%\n",
            "iter 1840: loss 1.3400, time 18.53ms, mfu 9.82%\n",
            "iter 1850: loss 1.3287, time 19.67ms, mfu 9.79%\n",
            "iter 1860: loss 1.3317, time 16.43ms, mfu 9.95%\n",
            "iter 1870: loss 1.3046, time 19.10ms, mfu 9.93%\n",
            "iter 1880: loss 1.3221, time 19.92ms, mfu 9.87%\n",
            "iter 1890: loss 1.3465, time 21.23ms, mfu 9.77%\n",
            "iter 1900: loss 1.3305, time 16.46ms, mfu 9.92%\n",
            "iter 1910: loss 1.2934, time 14.39ms, mfu 10.23%\n",
            "iter 1920: loss 1.3296, time 20.11ms, mfu 10.14%\n",
            "iter 1930: loss 1.3137, time 21.08ms, mfu 10.01%\n",
            "iter 1940: loss 1.3202, time 21.02ms, mfu 9.89%\n",
            "iter 1950: loss 1.2913, time 18.02ms, mfu 9.94%\n",
            "iter 1960: loss 1.2946, time 19.40ms, mfu 9.91%\n",
            "iter 1970: loss 1.3307, time 20.08ms, mfu 9.85%\n",
            "iter 1980: loss 1.3204, time 20.14ms, mfu 9.79%\n",
            "iter 1990: loss 1.2839, time 19.15ms, mfu 9.79%\n",
            "step 2000: train loss 1.2049, val loss 1.4949\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2000: loss 1.3378, time 4076.16ms, mfu 8.81%\n",
            "iter 2010: loss 1.2911, time 11.33ms, mfu 9.58%\n",
            "iter 2020: loss 1.3095, time 15.72ms, mfu 9.81%\n",
            "iter 2030: loss 1.3248, time 15.73ms, mfu 10.01%\n",
            "iter 2040: loss 1.3009, time 23.20ms, mfu 9.82%\n",
            "iter 2050: loss 1.3409, time 24.57ms, mfu 9.60%\n",
            "iter 2060: loss 1.2970, time 20.04ms, mfu 9.57%\n",
            "iter 2070: loss 1.3047, time 15.88ms, mfu 9.79%\n",
            "iter 2080: loss 1.3189, time 23.80ms, mfu 9.59%\n",
            "iter 2090: loss 1.3235, time 20.35ms, mfu 9.55%\n",
            "iter 2100: loss 1.2759, time 17.32ms, mfu 9.67%\n",
            "iter 2110: loss 1.2859, time 16.75ms, mfu 9.82%\n",
            "iter 2120: loss 1.2813, time 23.76ms, mfu 9.62%\n",
            "iter 2130: loss 1.2864, time 21.29ms, mfu 9.54%\n",
            "iter 2140: loss 1.3064, time 18.62ms, mfu 9.59%\n",
            "iter 2150: loss 1.3021, time 16.98ms, mfu 9.73%\n",
            "iter 2160: loss 1.2973, time 23.72ms, mfu 9.54%\n",
            "iter 2170: loss 1.3028, time 19.59ms, mfu 9.54%\n",
            "iter 2180: loss 1.2840, time 14.14ms, mfu 9.91%\n",
            "iter 2190: loss 1.2768, time 18.14ms, mfu 9.95%\n",
            "iter 2200: loss 1.2545, time 18.56ms, mfu 9.96%\n",
            "iter 2210: loss 1.2981, time 18.78ms, mfu 9.96%\n",
            "iter 2220: loss 1.3487, time 21.24ms, mfu 9.84%\n",
            "iter 2230: loss 1.3291, time 22.89ms, mfu 9.67%\n",
            "iter 2240: loss 1.2763, time 17.08ms, mfu 9.80%\n",
            "iter 2250: loss 1.2888, time 16.85ms, mfu 9.93%\n",
            "iter 2260: loss 1.2642, time 17.46ms, mfu 10.00%\n",
            "iter 2270: loss 1.2629, time 19.78ms, mfu 9.95%\n",
            "iter 2280: loss 1.2800, time 18.28ms, mfu 9.97%\n",
            "iter 2290: loss 1.2824, time 18.65ms, mfu 9.98%\n",
            "iter 2300: loss 1.2656, time 17.43ms, mfu 10.05%\n",
            "iter 2310: loss 1.2876, time 18.37ms, mfu 10.06%\n",
            "iter 2320: loss 1.2947, time 18.71ms, mfu 10.05%\n",
            "iter 2330: loss 1.2752, time 18.74ms, mfu 10.04%\n",
            "iter 2340: loss 1.2869, time 19.78ms, mfu 9.98%\n",
            "iter 2350: loss 1.2859, time 20.70ms, mfu 9.89%\n",
            "iter 2360: loss 1.2982, time 18.73ms, mfu 9.90%\n",
            "iter 2370: loss 1.2595, time 19.89ms, mfu 9.84%\n",
            "iter 2380: loss 1.2974, time 17.22ms, mfu 9.94%\n",
            "iter 2390: loss 1.2616, time 14.29ms, mfu 10.26%\n",
            "iter 2400: loss 1.2745, time 18.64ms, mfu 10.23%\n",
            "iter 2410: loss 1.2789, time 18.82ms, mfu 10.20%\n",
            "iter 2420: loss 1.2513, time 18.75ms, mfu 10.18%\n",
            "iter 2430: loss 1.2940, time 18.08ms, mfu 10.19%\n",
            "iter 2440: loss 1.2612, time 18.19ms, mfu 10.20%\n",
            "iter 2450: loss 1.3170, time 20.86ms, mfu 10.07%\n",
            "iter 2460: loss 1.2872, time 17.94ms, mfu 10.11%\n",
            "iter 2470: loss 1.2892, time 16.75ms, mfu 10.21%\n",
            "iter 2480: loss 1.2892, time 19.51ms, mfu 10.15%\n",
            "iter 2490: loss 1.2687, time 19.48ms, mfu 10.09%\n",
            "iter 2500: loss 1.2861, time 17.54ms, mfu 10.15%\n",
            "iter 2510: loss 1.2754, time 20.12ms, mfu 10.06%\n",
            "iter 2520: loss 1.2460, time 20.53ms, mfu 9.96%\n",
            "iter 2530: loss 1.2679, time 15.96ms, mfu 10.14%\n",
            "iter 2540: loss 1.2876, time 15.40ms, mfu 10.34%\n",
            "iter 2550: loss 1.2583, time 18.67ms, mfu 10.30%\n",
            "iter 2560: loss 1.2574, time 18.82ms, mfu 10.26%\n",
            "iter 2570: loss 1.2511, time 18.55ms, mfu 10.24%\n",
            "iter 2580: loss 1.2772, time 18.89ms, mfu 10.21%\n",
            "iter 2590: loss 1.2618, time 14.88ms, mfu 10.44%\n",
            "iter 2600: loss 1.2795, time 18.71ms, mfu 10.40%\n",
            "iter 2610: loss 1.2588, time 17.27ms, mfu 10.44%\n",
            "iter 2620: loss 1.2722, time 15.99ms, mfu 10.56%\n",
            "iter 2630: loss 1.2328, time 17.82ms, mfu 10.55%\n",
            "iter 2640: loss 1.2862, time 16.43ms, mfu 10.63%\n",
            "iter 2650: loss 1.2443, time 16.68ms, mfu 10.69%\n",
            "iter 2660: loss 1.2399, time 17.67ms, mfu 10.68%\n",
            "iter 2670: loss 1.2537, time 19.03ms, mfu 10.59%\n",
            "iter 2680: loss 1.2648, time 19.90ms, mfu 10.47%\n",
            "iter 2690: loss 1.2453, time 16.93ms, mfu 10.53%\n",
            "iter 2700: loss 1.2771, time 20.24ms, mfu 10.40%\n",
            "iter 2710: loss 1.2313, time 17.52ms, mfu 10.42%\n",
            "iter 2720: loss 1.2474, time 20.48ms, mfu 10.29%\n",
            "iter 2730: loss 1.2743, time 17.40ms, mfu 10.34%\n",
            "iter 2740: loss 1.2621, time 20.05ms, mfu 10.23%\n",
            "iter 2750: loss 1.2813, time 19.30ms, mfu 10.18%\n",
            "iter 2760: loss 1.2667, time 17.76ms, mfu 10.21%\n",
            "iter 2770: loss 1.2744, time 18.64ms, mfu 10.19%\n",
            "iter 2780: loss 1.2635, time 18.06ms, mfu 10.21%\n",
            "iter 2790: loss 1.2607, time 17.81ms, mfu 10.23%\n",
            "iter 2800: loss 1.2320, time 19.42ms, mfu 10.17%\n",
            "iter 2810: loss 1.2339, time 19.57ms, mfu 10.11%\n",
            "iter 2820: loss 1.2402, time 20.21ms, mfu 10.02%\n",
            "iter 2830: loss 1.2536, time 20.89ms, mfu 9.91%\n",
            "iter 2840: loss 1.2358, time 17.01ms, mfu 10.02%\n",
            "iter 2850: loss 1.2447, time 21.36ms, mfu 9.89%\n",
            "iter 2860: loss 1.2532, time 19.08ms, mfu 9.88%\n",
            "iter 2870: loss 1.2154, time 17.64ms, mfu 9.95%\n",
            "iter 2880: loss 1.2633, time 19.87ms, mfu 9.90%\n",
            "iter 2890: loss 1.2419, time 18.42ms, mfu 9.92%\n",
            "iter 2900: loss 1.2203, time 17.30ms, mfu 10.01%\n",
            "iter 2910: loss 1.2390, time 19.54ms, mfu 9.96%\n",
            "iter 2920: loss 1.2209, time 19.54ms, mfu 9.92%\n",
            "iter 2930: loss 1.2295, time 17.51ms, mfu 10.00%\n",
            "iter 2940: loss 1.2198, time 18.67ms, mfu 10.00%\n",
            "iter 2950: loss 1.2373, time 16.87ms, mfu 10.10%\n",
            "iter 2960: loss 1.2211, time 18.25ms, mfu 10.12%\n",
            "iter 2970: loss 1.2172, time 18.80ms, mfu 10.10%\n",
            "iter 2980: loss 1.2480, time 19.28ms, mfu 10.06%\n",
            "iter 2990: loss 1.2290, time 17.42ms, mfu 10.12%\n",
            "step 3000: train loss 1.1135, val loss 1.4717\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 3000: loss 1.2227, time 4086.71ms, mfu 9.11%\n",
            "iter 3010: loss 1.2140, time 11.99ms, mfu 9.76%\n",
            "iter 3020: loss 1.2260, time 16.35ms, mfu 9.93%\n",
            "iter 3030: loss 1.2052, time 15.58ms, mfu 10.13%\n",
            "iter 3040: loss 1.2418, time 22.11ms, mfu 9.96%\n",
            "iter 3050: loss 1.2279, time 24.35ms, mfu 9.73%\n",
            "iter 3060: loss 1.2230, time 17.31ms, mfu 9.84%\n",
            "iter 3070: loss 1.2355, time 17.23ms, mfu 9.94%\n",
            "iter 3080: loss 1.2464, time 22.39ms, mfu 9.78%\n",
            "iter 3090: loss 1.2318, time 21.50ms, mfu 9.67%\n",
            "iter 3100: loss 1.2075, time 16.89ms, mfu 9.81%\n",
            "iter 3110: loss 1.2180, time 16.87ms, mfu 9.93%\n",
            "iter 3120: loss 1.1949, time 20.58ms, mfu 9.85%\n",
            "iter 3130: loss 1.2554, time 18.22ms, mfu 9.89%\n",
            "iter 3140: loss 1.2167, time 12.55ms, mfu 10.39%\n",
            "iter 3150: loss 1.2685, time 17.43ms, mfu 10.42%\n",
            "iter 3160: loss 1.1879, time 25.08ms, mfu 10.12%\n",
            "iter 3170: loss 1.1969, time 20.21ms, mfu 10.03%\n",
            "iter 3180: loss 1.2123, time 15.70ms, mfu 10.22%\n",
            "iter 3190: loss 1.2231, time 19.72ms, mfu 10.14%\n",
            "iter 3200: loss 1.2056, time 23.12ms, mfu 9.94%\n",
            "iter 3210: loss 1.2161, time 18.03ms, mfu 9.98%\n",
            "iter 3220: loss 1.2306, time 16.53ms, mfu 10.11%\n",
            "iter 3230: loss 1.2148, time 21.44ms, mfu 9.97%\n",
            "iter 3240: loss 1.2258, time 22.17ms, mfu 9.82%\n",
            "iter 3250: loss 1.2099, time 18.80ms, mfu 9.83%\n",
            "iter 3260: loss 1.2106, time 19.14ms, mfu 9.82%\n",
            "iter 3270: loss 1.2017, time 20.65ms, mfu 9.74%\n",
            "iter 3280: loss 1.2020, time 19.84ms, mfu 9.71%\n",
            "iter 3290: loss 1.2116, time 14.76ms, mfu 10.00%\n",
            "iter 3300: loss 1.1937, time 20.64ms, mfu 9.91%\n",
            "iter 3310: loss 1.2149, time 16.40ms, mfu 10.06%\n",
            "iter 3320: loss 1.2044, time 16.12ms, mfu 10.21%\n",
            "iter 3330: loss 1.1996, time 16.59ms, mfu 10.31%\n",
            "iter 3340: loss 1.2152, time 22.22ms, mfu 10.12%\n",
            "iter 3350: loss 1.1968, time 19.24ms, mfu 10.08%\n",
            "iter 3360: loss 1.2005, time 11.56ms, mfu 10.69%\n",
            "iter 3370: loss 1.2035, time 16.67ms, mfu 10.74%\n",
            "iter 3380: loss 1.1843, time 20.61ms, mfu 10.57%\n",
            "iter 3390: loss 1.2216, time 19.17ms, mfu 10.49%\n",
            "iter 3400: loss 1.2165, time 18.27ms, mfu 10.46%\n",
            "iter 3410: loss 1.1842, time 19.39ms, mfu 10.38%\n",
            "iter 3420: loss 1.2537, time 20.25ms, mfu 10.26%\n",
            "iter 3430: loss 1.1825, time 20.18ms, mfu 10.16%\n",
            "iter 3440: loss 1.2163, time 16.39ms, mfu 10.28%\n",
            "iter 3450: loss 1.2179, time 23.56ms, mfu 10.05%\n",
            "iter 3460: loss 1.2140, time 19.64ms, mfu 9.99%\n",
            "iter 3470: loss 1.1889, time 18.21ms, mfu 10.02%\n",
            "iter 3480: loss 1.2069, time 15.50ms, mfu 10.22%\n",
            "iter 3490: loss 1.1773, time 19.88ms, mfu 10.14%\n",
            "iter 3500: loss 1.1752, time 21.51ms, mfu 9.99%\n",
            "iter 3510: loss 1.1799, time 17.92ms, mfu 10.04%\n",
            "iter 3520: loss 1.1876, time 20.07ms, mfu 9.96%\n",
            "iter 3530: loss 1.2285, time 18.87ms, mfu 9.96%\n",
            "iter 3540: loss 1.2136, time 19.51ms, mfu 9.92%\n",
            "iter 3550: loss 1.1954, time 16.28ms, mfu 10.07%\n",
            "iter 3560: loss 1.1811, time 18.83ms, mfu 10.06%\n",
            "iter 3570: loss 1.1994, time 20.53ms, mfu 9.96%\n",
            "iter 3580: loss 1.1962, time 17.70ms, mfu 10.02%\n",
            "iter 3590: loss 1.1961, time 16.17ms, mfu 10.17%\n",
            "iter 3600: loss 1.1841, time 19.11ms, mfu 10.13%\n",
            "iter 3610: loss 1.2058, time 20.06ms, mfu 10.05%\n",
            "iter 3620: loss 1.1840, time 16.06ms, mfu 10.21%\n",
            "iter 3630: loss 1.2297, time 17.88ms, mfu 10.23%\n",
            "iter 3640: loss 1.1893, time 19.42ms, mfu 10.17%\n",
            "iter 3650: loss 1.1849, time 19.45ms, mfu 10.11%\n",
            "iter 3660: loss 1.1885, time 16.59ms, mfu 10.23%\n",
            "iter 3670: loss 1.1908, time 19.54ms, mfu 10.16%\n",
            "iter 3680: loss 1.1703, time 18.95ms, mfu 10.13%\n",
            "iter 3690: loss 1.1835, time 21.10ms, mfu 10.00%\n",
            "iter 3700: loss 1.1728, time 14.12ms, mfu 10.32%\n",
            "iter 3710: loss 1.1710, time 20.41ms, mfu 10.21%\n",
            "iter 3720: loss 1.1742, time 19.18ms, mfu 10.16%\n",
            "iter 3730: loss 1.1700, time 20.75ms, mfu 10.04%\n",
            "iter 3740: loss 1.1692, time 17.19ms, mfu 10.12%\n",
            "iter 3750: loss 1.1795, time 16.40ms, mfu 10.25%\n",
            "iter 3760: loss 1.1842, time 17.35ms, mfu 10.30%\n",
            "iter 3770: loss 1.2142, time 20.11ms, mfu 10.20%\n",
            "iter 3780: loss 1.1900, time 14.76ms, mfu 10.45%\n",
            "iter 3790: loss 1.2075, time 18.31ms, mfu 10.42%\n",
            "iter 3800: loss 1.1878, time 19.03ms, mfu 10.36%\n",
            "iter 3810: loss 1.1909, time 19.18ms, mfu 10.30%\n",
            "iter 3820: loss 1.1905, time 19.12ms, mfu 10.24%\n",
            "iter 3830: loss 1.2137, time 17.69ms, mfu 10.27%\n",
            "iter 3840: loss 1.1954, time 15.81ms, mfu 10.43%\n",
            "iter 3850: loss 1.1474, time 19.92ms, mfu 10.32%\n",
            "iter 3860: loss 1.1910, time 21.92ms, mfu 10.14%\n",
            "iter 3870: loss 1.1829, time 20.68ms, mfu 10.03%\n",
            "iter 3880: loss 1.2113, time 18.69ms, mfu 10.03%\n",
            "iter 3890: loss 1.1804, time 17.94ms, mfu 10.06%\n",
            "iter 3900: loss 1.1643, time 20.41ms, mfu 9.97%\n",
            "iter 3910: loss 1.2051, time 20.28ms, mfu 9.90%\n",
            "iter 3920: loss 1.1741, time 21.35ms, mfu 9.78%\n",
            "iter 3930: loss 1.1780, time 19.20ms, mfu 9.78%\n",
            "iter 3940: loss 1.1785, time 19.63ms, mfu 9.75%\n",
            "iter 3950: loss 1.1574, time 19.46ms, mfu 9.73%\n",
            "iter 3960: loss 1.1759, time 20.79ms, mfu 9.66%\n",
            "iter 3970: loss 1.1825, time 18.42ms, mfu 9.71%\n",
            "iter 3980: loss 1.1737, time 18.44ms, mfu 9.75%\n",
            "iter 3990: loss 1.1830, time 18.72ms, mfu 9.77%\n",
            "step 4000: train loss 1.0516, val loss 1.4795\n",
            "iter 4000: loss 1.1471, time 3949.24ms, mfu 8.80%\n",
            "iter 4010: loss 1.1647, time 19.32ms, mfu 8.88%\n",
            "iter 4020: loss 1.1587, time 20.75ms, mfu 8.90%\n",
            "iter 4030: loss 1.1827, time 21.59ms, mfu 8.87%\n",
            "iter 4040: loss 1.1833, time 14.80ms, mfu 9.25%\n",
            "iter 4050: loss 1.1462, time 16.15ms, mfu 9.48%\n",
            "iter 4060: loss 1.1630, time 21.56ms, mfu 9.40%\n",
            "iter 4070: loss 1.1563, time 21.43ms, mfu 9.33%\n",
            "iter 4080: loss 1.1730, time 14.83ms, mfu 9.65%\n",
            "iter 4090: loss 1.1388, time 17.46ms, mfu 9.76%\n",
            "iter 4100: loss 1.1650, time 21.26ms, mfu 9.66%\n",
            "iter 4110: loss 1.1688, time 18.49ms, mfu 9.70%\n",
            "iter 4120: loss 1.1683, time 18.44ms, mfu 9.75%\n",
            "iter 4130: loss 1.1742, time 18.00ms, mfu 9.81%\n",
            "iter 4140: loss 1.1496, time 21.09ms, mfu 9.71%\n",
            "iter 4150: loss 1.1504, time 19.34ms, mfu 9.71%\n",
            "iter 4160: loss 1.1737, time 19.02ms, mfu 9.72%\n",
            "iter 4170: loss 1.1512, time 17.15ms, mfu 9.83%\n",
            "iter 4180: loss 1.1561, time 20.43ms, mfu 9.77%\n",
            "iter 4190: loss 1.1556, time 16.13ms, mfu 9.95%\n",
            "iter 4200: loss 1.1697, time 19.31ms, mfu 9.92%\n",
            "iter 4210: loss 1.1588, time 20.74ms, mfu 9.83%\n",
            "iter 4220: loss 1.1596, time 18.01ms, mfu 9.88%\n",
            "iter 4230: loss 1.1810, time 15.16ms, mfu 10.12%\n",
            "iter 4240: loss 1.1588, time 20.65ms, mfu 10.02%\n",
            "iter 4250: loss 1.1512, time 17.45ms, mfu 10.09%\n",
            "iter 4260: loss 1.1779, time 19.12ms, mfu 10.05%\n",
            "iter 4270: loss 1.1667, time 15.59ms, mfu 10.25%\n",
            "iter 4280: loss 1.1459, time 18.74ms, mfu 10.22%\n",
            "iter 4290: loss 1.1769, time 19.11ms, mfu 10.17%\n",
            "iter 4300: loss 1.1488, time 20.31ms, mfu 10.07%\n",
            "iter 4310: loss 1.1637, time 20.04ms, mfu 10.00%\n",
            "iter 4320: loss 1.1806, time 20.39ms, mfu 9.91%\n",
            "iter 4330: loss 1.1534, time 20.44ms, mfu 9.84%\n",
            "iter 4340: loss 1.2024, time 20.47ms, mfu 9.77%\n",
            "iter 4350: loss 1.1625, time 17.81ms, mfu 9.84%\n",
            "iter 4360: loss 1.1702, time 18.72ms, mfu 9.85%\n",
            "iter 4370: loss 1.1588, time 18.17ms, mfu 9.89%\n",
            "iter 4380: loss 1.1787, time 18.19ms, mfu 9.93%\n",
            "iter 4390: loss 1.1427, time 20.12ms, mfu 9.86%\n",
            "iter 4400: loss 1.1743, time 20.86ms, mfu 9.77%\n",
            "iter 4410: loss 1.1417, time 18.91ms, mfu 9.78%\n",
            "iter 4420: loss 1.1590, time 19.14ms, mfu 9.78%\n",
            "iter 4430: loss 1.1490, time 18.39ms, mfu 9.82%\n",
            "iter 4440: loss 1.1832, time 22.52ms, mfu 9.66%\n",
            "iter 4450: loss 1.1615, time 18.54ms, mfu 9.71%\n",
            "iter 4460: loss 1.1589, time 20.64ms, mfu 9.64%\n",
            "iter 4470: loss 1.1667, time 18.25ms, mfu 9.70%\n",
            "iter 4480: loss 1.1632, time 20.72ms, mfu 9.63%\n",
            "iter 4490: loss 1.1278, time 17.15ms, mfu 9.76%\n",
            "iter 4500: loss 1.1795, time 17.40ms, mfu 9.85%\n",
            "iter 4510: loss 1.1469, time 20.24ms, mfu 9.79%\n",
            "iter 4520: loss 1.1587, time 16.94ms, mfu 9.91%\n",
            "iter 4530: loss 1.1650, time 17.55ms, mfu 9.99%\n",
            "iter 4540: loss 1.2015, time 17.68ms, mfu 10.04%\n",
            "iter 4550: loss 1.1619, time 21.11ms, mfu 9.92%\n",
            "iter 4560: loss 1.1683, time 16.37ms, mfu 10.07%\n",
            "iter 4570: loss 1.1518, time 19.84ms, mfu 10.01%\n",
            "iter 4580: loss 1.1613, time 19.46ms, mfu 9.96%\n",
            "iter 4590: loss 1.1521, time 21.42ms, mfu 9.84%\n",
            "iter 4600: loss 1.1632, time 18.43ms, mfu 9.87%\n",
            "iter 4610: loss 1.1637, time 20.44ms, mfu 9.80%\n",
            "iter 4620: loss 1.1541, time 20.51ms, mfu 9.73%\n",
            "iter 4630: loss 1.1447, time 21.05ms, mfu 9.64%\n",
            "iter 4640: loss 1.1568, time 19.27ms, mfu 9.65%\n",
            "iter 4650: loss 1.1615, time 16.73ms, mfu 9.80%\n",
            "iter 4660: loss 1.1122, time 20.51ms, mfu 9.73%\n",
            "iter 4670: loss 1.1426, time 21.39ms, mfu 9.63%\n",
            "iter 4680: loss 1.1434, time 19.21ms, mfu 9.64%\n",
            "iter 4690: loss 1.1481, time 20.75ms, mfu 9.57%\n",
            "iter 4700: loss 1.1548, time 22.04ms, mfu 9.46%\n",
            "iter 4710: loss 1.1196, time 18.41ms, mfu 9.53%\n",
            "iter 4720: loss 1.1341, time 17.67ms, mfu 9.63%\n",
            "iter 4730: loss 1.1327, time 17.03ms, mfu 9.77%\n",
            "iter 4740: loss 1.1489, time 18.73ms, mfu 9.79%\n",
            "iter 4750: loss 1.1428, time 20.17ms, mfu 9.73%\n",
            "iter 4760: loss 1.1422, time 17.66ms, mfu 9.82%\n",
            "iter 4770: loss 1.1570, time 18.52ms, mfu 9.84%\n",
            "iter 4780: loss 1.1352, time 17.38ms, mfu 9.93%\n",
            "iter 4790: loss 1.1461, time 17.78ms, mfu 9.99%\n",
            "iter 4800: loss 1.1766, time 20.30ms, mfu 9.91%\n",
            "iter 4810: loss 1.1363, time 18.66ms, mfu 9.92%\n",
            "iter 4820: loss 1.1316, time 17.73ms, mfu 9.98%\n",
            "iter 4830: loss 1.1363, time 19.32ms, mfu 9.95%\n",
            "iter 4840: loss 1.1407, time 20.56ms, mfu 9.86%\n",
            "iter 4850: loss 1.1240, time 17.32ms, mfu 9.96%\n",
            "iter 4860: loss 1.1282, time 16.37ms, mfu 10.10%\n",
            "iter 4870: loss 1.1687, time 18.41ms, mfu 10.10%\n",
            "iter 4880: loss 1.1378, time 21.02ms, mfu 9.98%\n",
            "iter 4890: loss 1.1458, time 17.25ms, mfu 10.07%\n",
            "iter 4900: loss 1.1291, time 18.49ms, mfu 10.07%\n",
            "iter 4910: loss 1.1599, time 13.95ms, mfu 10.40%\n",
            "iter 4920: loss 1.1832, time 18.57ms, mfu 10.37%\n",
            "iter 4930: loss 1.1309, time 17.68ms, mfu 10.39%\n",
            "iter 4940: loss 1.1117, time 15.96ms, mfu 10.52%\n",
            "iter 4950: loss 1.1297, time 16.42ms, mfu 10.60%\n",
            "iter 4960: loss 1.1232, time 16.01ms, mfu 10.71%\n",
            "iter 4970: loss 1.1128, time 16.79ms, mfu 10.75%\n",
            "iter 4980: loss 1.1336, time 19.95ms, mfu 10.61%\n",
            "iter 4990: loss 1.1201, time 21.18ms, mfu 10.43%\n",
            "step 5000: train loss 1.0134, val loss 1.4773\n",
            "iter 5000: loss 1.1268, time 3977.02ms, mfu 9.39%\n",
            "iter 5010: loss 1.1332, time 17.69ms, mfu 9.51%\n",
            "iter 5020: loss 1.1246, time 21.38ms, mfu 9.43%\n",
            "iter 5030: loss 1.1480, time 23.75ms, mfu 9.27%\n",
            "iter 5040: loss 1.1581, time 22.27ms, mfu 9.19%\n",
            "iter 5050: loss 1.1019, time 19.24ms, mfu 9.24%\n",
            "iter 5060: loss 1.1637, time 19.61ms, mfu 9.27%\n",
            "iter 5070: loss 1.1636, time 20.81ms, mfu 9.24%\n",
            "iter 5080: loss 1.1395, time 16.96ms, mfu 9.41%\n",
            "iter 5090: loss 1.1490, time 16.12ms, mfu 9.63%\n",
            "iter 5100: loss 1.1612, time 22.36ms, mfu 9.50%\n",
            "iter 5110: loss 1.1437, time 18.89ms, mfu 9.54%\n",
            "iter 5120: loss 1.1524, time 16.20ms, mfu 9.74%\n",
            "iter 5130: loss 1.1514, time 16.96ms, mfu 9.87%\n",
            "iter 5140: loss 1.1105, time 21.92ms, mfu 9.73%\n",
            "iter 5150: loss 1.1728, time 17.84ms, mfu 9.80%\n",
            "iter 5160: loss 1.1336, time 17.27ms, mfu 9.90%\n",
            "iter 5170: loss 1.1422, time 20.44ms, mfu 9.83%\n",
            "iter 5180: loss 1.1292, time 21.47ms, mfu 9.71%\n",
            "iter 5190: loss 1.1325, time 20.16ms, mfu 9.67%\n",
            "iter 5200: loss 1.1474, time 13.65ms, mfu 10.07%\n",
            "iter 5210: loss 1.1642, time 20.57ms, mfu 9.97%\n",
            "iter 5220: loss 1.1507, time 17.92ms, mfu 10.02%\n",
            "iter 5230: loss 1.1424, time 18.93ms, mfu 10.00%\n",
            "iter 5240: loss 1.1588, time 14.87ms, mfu 10.26%\n",
            "iter 5250: loss 1.1401, time 18.20ms, mfu 10.26%\n",
            "iter 5260: loss 1.1532, time 11.22ms, mfu 10.89%\n",
            "iter 5270: loss 1.1326, time 18.01ms, mfu 10.84%\n",
            "iter 5280: loss 1.1492, time 17.14ms, mfu 10.85%\n",
            "iter 5290: loss 1.1395, time 20.27ms, mfu 10.68%\n",
            "iter 5300: loss 1.2038, time 16.58ms, mfu 10.74%\n",
            "iter 5310: loss 1.1334, time 19.35ms, mfu 10.63%\n",
            "iter 5320: loss 1.1669, time 20.21ms, mfu 10.49%\n",
            "iter 5330: loss 1.1247, time 16.00ms, mfu 10.61%\n",
            "iter 5340: loss 1.1597, time 20.27ms, mfu 10.47%\n",
            "iter 5350: loss 1.1323, time 19.02ms, mfu 10.41%\n",
            "iter 5360: loss 1.1637, time 19.28ms, mfu 10.33%\n",
            "iter 5370: loss 1.1315, time 18.60ms, mfu 10.30%\n",
            "iter 5380: loss 1.1314, time 18.78ms, mfu 10.27%\n",
            "iter 5390: loss 1.1364, time 16.63ms, mfu 10.36%\n",
            "iter 5400: loss 1.1032, time 22.33ms, mfu 10.16%\n",
            "iter 5410: loss 1.1248, time 18.02ms, mfu 10.18%\n",
            "iter 5420: loss 1.1462, time 18.35ms, mfu 10.18%\n",
            "iter 5430: loss 1.1350, time 18.29ms, mfu 10.19%\n",
            "iter 5440: loss 1.1413, time 20.79ms, mfu 10.06%\n",
            "iter 5450: loss 1.1559, time 18.17ms, mfu 10.09%\n",
            "iter 5460: loss 1.1311, time 17.02ms, mfu 10.17%\n",
            "iter 5470: loss 1.1427, time 18.59ms, mfu 10.16%\n",
            "iter 5480: loss 1.1395, time 19.78ms, mfu 10.09%\n",
            "iter 5490: loss 1.1435, time 19.06ms, mfu 10.06%\n",
            "iter 5500: loss 1.1498, time 18.63ms, mfu 10.06%\n",
            "iter 5510: loss 1.1656, time 17.80ms, mfu 10.10%\n",
            "iter 5520: loss 1.1640, time 19.49ms, mfu 10.05%\n",
            "iter 5530: loss 1.1751, time 18.87ms, mfu 10.03%\n",
            "iter 5540: loss 1.1242, time 18.73ms, mfu 10.03%\n",
            "iter 5550: loss 1.1633, time 22.22ms, mfu 9.86%\n",
            "iter 5560: loss 1.1732, time 19.21ms, mfu 9.85%\n",
            "iter 5570: loss 1.1410, time 20.85ms, mfu 9.76%\n",
            "iter 5580: loss 1.1363, time 19.83ms, mfu 9.72%\n",
            "iter 5590: loss 1.1445, time 18.54ms, mfu 9.76%\n",
            "iter 5600: loss 1.1489, time 21.95ms, mfu 9.63%\n",
            "iter 5610: loss 1.1400, time 19.74ms, mfu 9.62%\n",
            "iter 5620: loss 1.1484, time 19.51ms, mfu 9.61%\n",
            "iter 5630: loss 1.1347, time 19.90ms, mfu 9.59%\n",
            "iter 5640: loss 1.1451, time 16.66ms, mfu 9.75%\n",
            "iter 5650: loss 1.1292, time 15.79ms, mfu 9.96%\n",
            "iter 5660: loss 1.1175, time 20.97ms, mfu 9.85%\n",
            "iter 5670: loss 1.1302, time 15.83ms, mfu 10.05%\n",
            "iter 5680: loss 1.1504, time 15.37ms, mfu 10.26%\n",
            "iter 5690: loss 1.1450, time 18.63ms, mfu 10.23%\n",
            "iter 5700: loss 1.1074, time 19.47ms, mfu 10.17%\n",
            "iter 5710: loss 1.1120, time 20.73ms, mfu 10.05%\n",
            "iter 5720: loss 1.1115, time 19.27ms, mfu 10.02%\n",
            "iter 5730: loss 1.1527, time 21.80ms, mfu 9.87%\n",
            "iter 5740: loss 1.1465, time 17.54ms, mfu 9.95%\n",
            "iter 5750: loss 1.1345, time 17.84ms, mfu 10.00%\n",
            "iter 5760: loss 1.1352, time 18.23ms, mfu 10.02%\n",
            "iter 5770: loss 1.1600, time 17.54ms, mfu 10.09%\n",
            "iter 5780: loss 1.1538, time 19.59ms, mfu 10.03%\n",
            "iter 5790: loss 1.1395, time 18.40ms, mfu 10.04%\n",
            "iter 5800: loss 1.1359, time 19.48ms, mfu 10.00%\n",
            "iter 5810: loss 1.1228, time 20.17ms, mfu 9.92%\n",
            "iter 5820: loss 1.1359, time 17.93ms, mfu 9.97%\n",
            "iter 5830: loss 1.1161, time 20.69ms, mfu 9.88%\n",
            "iter 5840: loss 1.1229, time 17.80ms, mfu 9.94%\n",
            "iter 5850: loss 1.1517, time 20.50ms, mfu 9.86%\n",
            "iter 5860: loss 1.1375, time 21.46ms, mfu 9.74%\n",
            "iter 5870: loss 1.1068, time 19.37ms, mfu 9.73%\n",
            "iter 5880: loss 1.1271, time 20.75ms, mfu 9.66%\n",
            "iter 5890: loss 1.1350, time 21.06ms, mfu 9.58%\n",
            "iter 5900: loss 1.1157, time 17.15ms, mfu 9.71%\n",
            "iter 5910: loss 1.1431, time 16.86ms, mfu 9.85%\n",
            "iter 5920: loss 1.1377, time 18.29ms, mfu 9.88%\n",
            "iter 5930: loss 1.1622, time 18.49ms, mfu 9.90%\n",
            "iter 5940: loss 1.1248, time 17.02ms, mfu 10.01%\n",
            "iter 5950: loss 1.1574, time 19.88ms, mfu 9.95%\n",
            "iter 5960: loss 1.1308, time 18.94ms, mfu 9.94%\n",
            "iter 5970: loss 1.1216, time 20.81ms, mfu 9.84%\n",
            "iter 5980: loss 1.1484, time 18.56ms, mfu 9.86%\n",
            "iter 5990: loss 1.1191, time 18.31ms, mfu 9.90%\n",
            "step 6000: train loss 0.9931, val loss 1.4822\n",
            "iter 6000: loss 1.1480, time 3982.83ms, mfu 8.91%\n",
            "Extracted losses: train=1.0134, val=1.4773\n",
            "\n",
            "Training model with 5 layers and 4 heads\n",
            "Running command: python train.py config/train_shakespeare_char_5layer_4head.py\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "W0503 17:06:59.921000 12419 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "Overriding config with config/train_shakespeare_char_5layer_4head.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 1000 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 5\n",
            "n_head = 4\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 6000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 8.88M\n",
            "num decayed parameter tensors: 22, with 8,970,624 parameters\n",
            "num non-decayed parameter tensors: 11, with 4,224 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 4.2125, val loss 4.2089\n",
            "iter 0: loss 4.2305, time 19682.43ms, mfu -100.00%\n",
            "iter 10: loss 3.1895, time 41.29ms, mfu 7.52%\n",
            "iter 20: loss 2.7595, time 42.15ms, mfu 7.51%\n",
            "iter 30: loss 2.6331, time 46.15ms, mfu 7.43%\n",
            "iter 40: loss 2.5571, time 41.23ms, mfu 7.44%\n",
            "iter 50: loss 2.5306, time 45.97ms, mfu 7.37%\n",
            "iter 60: loss 2.5031, time 42.77ms, mfu 7.36%\n",
            "iter 70: loss 2.4891, time 44.96ms, mfu 7.32%\n",
            "iter 80: loss 2.4964, time 44.16ms, mfu 7.29%\n",
            "iter 90: loss 2.4685, time 43.36ms, mfu 7.28%\n",
            "iter 100: loss 2.4920, time 44.78ms, mfu 7.24%\n",
            "iter 110: loss 2.4353, time 42.24ms, mfu 7.25%\n",
            "iter 120: loss 2.4293, time 44.73ms, mfu 7.22%\n",
            "iter 130: loss 2.4072, time 43.29ms, mfu 7.22%\n",
            "iter 140: loss 2.4234, time 44.65ms, mfu 7.19%\n",
            "iter 150: loss 2.3870, time 44.17ms, mfu 7.18%\n",
            "iter 160: loss 2.3824, time 45.28ms, mfu 7.14%\n",
            "iter 170: loss 2.3214, time 43.87ms, mfu 7.14%\n",
            "iter 180: loss 2.2518, time 43.73ms, mfu 7.13%\n",
            "iter 190: loss 2.2024, time 44.80ms, mfu 7.11%\n",
            "iter 200: loss 2.1674, time 43.05ms, mfu 7.12%\n",
            "iter 210: loss 2.1300, time 45.90ms, mfu 7.09%\n",
            "iter 220: loss 2.0668, time 43.60ms, mfu 7.09%\n",
            "iter 230: loss 2.0839, time 43.94ms, mfu 7.09%\n",
            "iter 240: loss 2.0141, time 44.80ms, mfu 7.07%\n",
            "iter 250: loss 1.9716, time 45.06ms, mfu 7.06%\n",
            "iter 260: loss 1.9934, time 44.88ms, mfu 7.04%\n",
            "iter 270: loss 1.9626, time 45.10ms, mfu 7.03%\n",
            "iter 280: loss 1.9310, time 44.83ms, mfu 7.02%\n",
            "iter 290: loss 1.9216, time 44.39ms, mfu 7.02%\n",
            "iter 300: loss 1.8864, time 45.52ms, mfu 7.00%\n",
            "iter 310: loss 1.8911, time 44.37ms, mfu 7.00%\n",
            "iter 320: loss 1.8476, time 45.61ms, mfu 6.98%\n",
            "iter 330: loss 1.8372, time 44.94ms, mfu 6.97%\n",
            "iter 340: loss 1.8505, time 45.28ms, mfu 6.96%\n",
            "iter 350: loss 1.7687, time 44.01ms, mfu 6.97%\n",
            "iter 360: loss 1.7952, time 44.78ms, mfu 6.97%\n",
            "iter 370: loss 1.7858, time 44.64ms, mfu 6.97%\n",
            "iter 380: loss 1.7468, time 45.03ms, mfu 6.96%\n",
            "iter 390: loss 1.7354, time 45.05ms, mfu 6.95%\n",
            "iter 400: loss 1.7522, time 45.38ms, mfu 6.94%\n",
            "iter 410: loss 1.7335, time 44.54ms, mfu 6.95%\n",
            "iter 420: loss 1.7200, time 45.95ms, mfu 6.93%\n",
            "iter 430: loss 1.7069, time 45.69ms, mfu 6.91%\n",
            "iter 440: loss 1.6976, time 44.54ms, mfu 6.92%\n",
            "iter 450: loss 1.6860, time 45.35ms, mfu 6.91%\n",
            "iter 460: loss 1.6784, time 45.37ms, mfu 6.91%\n",
            "iter 470: loss 1.6707, time 45.24ms, mfu 6.90%\n",
            "iter 480: loss 1.6583, time 45.43ms, mfu 6.90%\n",
            "iter 490: loss 1.6719, time 46.03ms, mfu 6.88%\n",
            "iter 500: loss 1.6612, time 44.59ms, mfu 6.89%\n",
            "iter 510: loss 1.6235, time 45.96ms, mfu 6.88%\n",
            "iter 520: loss 1.6649, time 46.00ms, mfu 6.86%\n",
            "iter 530: loss 1.6265, time 45.28ms, mfu 6.86%\n",
            "iter 540: loss 1.6506, time 44.70ms, mfu 6.87%\n",
            "iter 550: loss 1.6055, time 44.61ms, mfu 6.88%\n",
            "iter 560: loss 1.5961, time 45.37ms, mfu 6.88%\n",
            "iter 570: loss 1.6175, time 45.47ms, mfu 6.87%\n",
            "iter 580: loss 1.5680, time 45.95ms, mfu 6.86%\n",
            "iter 590: loss 1.6040, time 44.66ms, mfu 6.87%\n",
            "iter 600: loss 1.6102, time 46.02ms, mfu 6.86%\n",
            "iter 610: loss 1.5609, time 45.47ms, mfu 6.86%\n",
            "iter 620: loss 1.5797, time 45.69ms, mfu 6.85%\n",
            "iter 630: loss 1.5468, time 46.22ms, mfu 6.84%\n",
            "iter 640: loss 1.5259, time 44.69ms, mfu 6.85%\n",
            "iter 650: loss 1.5587, time 45.46ms, mfu 6.85%\n",
            "iter 660: loss 1.5409, time 46.27ms, mfu 6.83%\n",
            "iter 670: loss 1.5365, time 46.08ms, mfu 6.83%\n",
            "iter 680: loss 1.5229, time 44.43ms, mfu 6.84%\n",
            "iter 690: loss 1.5125, time 45.94ms, mfu 6.83%\n",
            "iter 700: loss 1.5235, time 45.72ms, mfu 6.83%\n",
            "iter 710: loss 1.4895, time 46.02ms, mfu 6.82%\n",
            "iter 720: loss 1.4950, time 45.54ms, mfu 6.82%\n",
            "iter 730: loss 1.4455, time 45.25ms, mfu 6.83%\n",
            "iter 740: loss 1.4997, time 45.77ms, mfu 6.82%\n",
            "iter 750: loss 1.4980, time 45.38ms, mfu 6.82%\n",
            "iter 760: loss 1.4910, time 44.35ms, mfu 6.84%\n",
            "iter 770: loss 1.4694, time 45.74ms, mfu 6.84%\n",
            "iter 780: loss 1.5010, time 45.70ms, mfu 6.83%\n",
            "iter 790: loss 1.4791, time 44.47ms, mfu 6.85%\n",
            "iter 800: loss 1.4503, time 45.32ms, mfu 6.85%\n",
            "iter 810: loss 1.4806, time 44.77ms, mfu 6.86%\n",
            "iter 820: loss 1.4460, time 44.70ms, mfu 6.87%\n",
            "iter 830: loss 1.4425, time 45.58ms, mfu 6.86%\n",
            "iter 840: loss 1.4253, time 44.60ms, mfu 6.87%\n",
            "iter 850: loss 1.4674, time 44.81ms, mfu 6.88%\n",
            "iter 860: loss 1.4339, time 44.83ms, mfu 6.88%\n",
            "iter 870: loss 1.4386, time 44.78ms, mfu 6.89%\n",
            "iter 880: loss 1.4182, time 44.72ms, mfu 6.89%\n",
            "iter 890: loss 1.4295, time 45.29ms, mfu 6.89%\n",
            "iter 900: loss 1.4040, time 45.76ms, mfu 6.88%\n",
            "iter 910: loss 1.4466, time 44.70ms, mfu 6.89%\n",
            "iter 920: loss 1.4171, time 45.91ms, mfu 6.88%\n",
            "iter 930: loss 1.4304, time 43.48ms, mfu 6.90%\n",
            "iter 940: loss 1.4126, time 44.75ms, mfu 6.91%\n",
            "iter 950: loss 1.3998, time 44.77ms, mfu 6.91%\n",
            "iter 960: loss 1.3939, time 44.81ms, mfu 6.91%\n",
            "iter 970: loss 1.4136, time 45.31ms, mfu 6.91%\n",
            "iter 980: loss 1.4041, time 43.66ms, mfu 6.93%\n",
            "iter 990: loss 1.4100, time 44.57ms, mfu 6.93%\n",
            "step 1000: train loss 1.3063, val loss 1.5442\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.4241, time 6135.36ms, mfu 6.24%\n",
            "iter 1010: loss 1.3747, time 42.31ms, mfu 6.35%\n",
            "iter 1020: loss 1.4032, time 42.13ms, mfu 6.46%\n",
            "iter 1030: loss 1.4140, time 46.61ms, mfu 6.48%\n",
            "iter 1040: loss 1.3762, time 42.69ms, mfu 6.56%\n",
            "iter 1050: loss 1.3713, time 46.03ms, mfu 6.58%\n",
            "iter 1060: loss 1.3653, time 42.11ms, mfu 6.66%\n",
            "iter 1070: loss 1.3802, time 46.33ms, mfu 6.66%\n",
            "iter 1080: loss 1.3989, time 44.76ms, mfu 6.69%\n",
            "iter 1090: loss 1.3513, time 43.60ms, mfu 6.73%\n",
            "iter 1100: loss 1.3476, time 45.35ms, mfu 6.74%\n",
            "iter 1110: loss 1.3716, time 43.21ms, mfu 6.79%\n",
            "iter 1120: loss 1.3723, time 45.32ms, mfu 6.80%\n",
            "iter 1130: loss 1.3869, time 43.25ms, mfu 6.83%\n",
            "iter 1140: loss 1.3603, time 45.09ms, mfu 6.84%\n",
            "iter 1150: loss 1.3330, time 43.69ms, mfu 6.87%\n",
            "iter 1160: loss 1.3448, time 45.06ms, mfu 6.87%\n",
            "iter 1170: loss 1.3649, time 44.24ms, mfu 6.88%\n",
            "iter 1180: loss 1.3864, time 44.80ms, mfu 6.89%\n",
            "iter 1190: loss 1.3403, time 44.06ms, mfu 6.91%\n",
            "iter 1200: loss 1.3334, time 43.47ms, mfu 6.93%\n",
            "iter 1210: loss 1.3371, time 44.56ms, mfu 6.93%\n",
            "iter 1220: loss 1.3689, time 43.60ms, mfu 6.95%\n",
            "iter 1230: loss 1.3642, time 44.79ms, mfu 6.95%\n",
            "iter 1240: loss 1.3313, time 44.34ms, mfu 6.96%\n",
            "iter 1250: loss 1.3467, time 44.70ms, mfu 6.96%\n",
            "iter 1260: loss 1.3479, time 44.13ms, mfu 6.96%\n",
            "iter 1270: loss 1.3474, time 43.58ms, mfu 6.98%\n",
            "iter 1280: loss 1.3358, time 43.88ms, mfu 6.99%\n",
            "iter 1290: loss 1.2997, time 43.38ms, mfu 7.01%\n",
            "iter 1300: loss 1.3218, time 44.76ms, mfu 7.00%\n",
            "iter 1310: loss 1.3196, time 43.97ms, mfu 7.01%\n",
            "iter 1320: loss 1.3019, time 43.79ms, mfu 7.02%\n",
            "iter 1330: loss 1.3290, time 43.83ms, mfu 7.02%\n",
            "iter 1340: loss 1.2867, time 44.46ms, mfu 7.02%\n",
            "iter 1350: loss 1.2814, time 43.65ms, mfu 7.03%\n",
            "iter 1360: loss 1.3173, time 44.25ms, mfu 7.03%\n",
            "iter 1370: loss 1.3066, time 44.78ms, mfu 7.02%\n",
            "iter 1380: loss 1.2925, time 43.85ms, mfu 7.03%\n",
            "iter 1390: loss 1.2824, time 43.96ms, mfu 7.03%\n",
            "iter 1400: loss 1.2995, time 44.12ms, mfu 7.03%\n",
            "iter 1410: loss 1.2968, time 44.55ms, mfu 7.03%\n",
            "iter 1420: loss 1.3344, time 44.38ms, mfu 7.02%\n",
            "iter 1430: loss 1.2860, time 44.01ms, mfu 7.03%\n",
            "iter 1440: loss 1.3265, time 44.22ms, mfu 7.03%\n",
            "iter 1450: loss 1.3051, time 43.60ms, mfu 7.04%\n",
            "iter 1460: loss 1.2856, time 44.11ms, mfu 7.04%\n",
            "iter 1470: loss 1.2787, time 43.93ms, mfu 7.04%\n",
            "iter 1480: loss 1.2998, time 43.90ms, mfu 7.04%\n",
            "iter 1490: loss 1.2963, time 43.44ms, mfu 7.05%\n",
            "iter 1500: loss 1.2885, time 43.22ms, mfu 7.07%\n",
            "iter 1510: loss 1.2845, time 44.04ms, mfu 7.07%\n",
            "iter 1520: loss 1.2724, time 43.71ms, mfu 7.07%\n",
            "iter 1530: loss 1.2870, time 42.78ms, mfu 7.09%\n",
            "iter 1540: loss 1.2692, time 42.92ms, mfu 7.10%\n",
            "iter 1550: loss 1.2901, time 44.04ms, mfu 7.10%\n",
            "iter 1560: loss 1.2782, time 43.65ms, mfu 7.10%\n",
            "iter 1570: loss 1.3019, time 44.50ms, mfu 7.09%\n",
            "iter 1580: loss 1.2657, time 44.75ms, mfu 7.07%\n",
            "iter 1590: loss 1.2938, time 43.93ms, mfu 7.07%\n",
            "iter 1600: loss 1.2540, time 44.25ms, mfu 7.07%\n",
            "iter 1610: loss 1.2950, time 44.70ms, mfu 7.06%\n",
            "iter 1620: loss 1.2459, time 43.96ms, mfu 7.06%\n",
            "iter 1630: loss 1.2612, time 44.34ms, mfu 7.05%\n",
            "iter 1640: loss 1.2369, time 44.55ms, mfu 7.04%\n",
            "iter 1650: loss 1.2515, time 44.34ms, mfu 7.04%\n",
            "iter 1660: loss 1.2763, time 44.33ms, mfu 7.04%\n",
            "iter 1670: loss 1.2620, time 44.43ms, mfu 7.03%\n",
            "iter 1680: loss 1.2604, time 44.02ms, mfu 7.04%\n",
            "iter 1690: loss 1.2594, time 43.91ms, mfu 7.04%\n",
            "iter 1700: loss 1.2648, time 44.29ms, mfu 7.04%\n",
            "iter 1710: loss 1.2307, time 44.03ms, mfu 7.04%\n",
            "iter 1720: loss 1.2324, time 43.87ms, mfu 7.04%\n",
            "iter 1730: loss 1.2235, time 44.03ms, mfu 7.04%\n",
            "iter 1740: loss 1.2507, time 44.65ms, mfu 7.04%\n",
            "iter 1750: loss 1.2375, time 44.53ms, mfu 7.03%\n",
            "iter 1760: loss 1.2420, time 43.69ms, mfu 7.04%\n",
            "iter 1770: loss 1.2188, time 44.49ms, mfu 7.03%\n",
            "iter 1780: loss 1.2236, time 44.01ms, mfu 7.03%\n",
            "iter 1790: loss 1.2541, time 44.36ms, mfu 7.03%\n",
            "iter 1800: loss 1.2292, time 44.00ms, mfu 7.03%\n",
            "iter 1810: loss 1.2323, time 43.99ms, mfu 7.04%\n",
            "iter 1820: loss 1.2546, time 44.29ms, mfu 7.03%\n",
            "iter 1830: loss 1.2179, time 44.13ms, mfu 7.04%\n",
            "iter 1840: loss 1.2302, time 44.06ms, mfu 7.04%\n",
            "iter 1850: loss 1.2412, time 43.89ms, mfu 7.04%\n",
            "iter 1860: loss 1.2411, time 43.99ms, mfu 7.04%\n",
            "iter 1870: loss 1.2351, time 44.09ms, mfu 7.04%\n",
            "iter 1880: loss 1.2270, time 44.80ms, mfu 7.03%\n",
            "iter 1890: loss 1.2290, time 44.32ms, mfu 7.03%\n",
            "iter 1900: loss 1.1999, time 44.02ms, mfu 7.03%\n",
            "iter 1910: loss 1.2013, time 44.09ms, mfu 7.03%\n",
            "iter 1920: loss 1.2159, time 43.92ms, mfu 7.04%\n",
            "iter 1930: loss 1.1960, time 44.20ms, mfu 7.04%\n",
            "iter 1940: loss 1.2051, time 43.42ms, mfu 7.05%\n",
            "iter 1950: loss 1.2198, time 43.88ms, mfu 7.05%\n",
            "iter 1960: loss 1.1930, time 43.51ms, mfu 7.06%\n",
            "iter 1970: loss 1.1899, time 43.87ms, mfu 7.06%\n",
            "iter 1980: loss 1.2479, time 43.88ms, mfu 7.06%\n",
            "iter 1990: loss 1.1918, time 43.93ms, mfu 7.06%\n",
            "step 2000: train loss 1.1128, val loss 1.4547\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2000: loss 1.2203, time 6132.68ms, mfu 6.36%\n",
            "iter 2010: loss 1.1985, time 42.21ms, mfu 6.46%\n",
            "iter 2020: loss 1.2394, time 42.70ms, mfu 6.54%\n",
            "iter 2030: loss 1.1869, time 46.07ms, mfu 6.56%\n",
            "iter 2040: loss 1.2292, time 42.25ms, mfu 6.64%\n",
            "iter 2050: loss 1.1984, time 46.32ms, mfu 6.65%\n",
            "iter 2060: loss 1.1848, time 44.04ms, mfu 6.69%\n",
            "iter 2070: loss 1.2053, time 45.99ms, mfu 6.70%\n",
            "iter 2080: loss 1.1649, time 42.60ms, mfu 6.76%\n",
            "iter 2090: loss 1.1996, time 44.25ms, mfu 6.78%\n",
            "iter 2100: loss 1.2061, time 45.03ms, mfu 6.79%\n",
            "iter 2110: loss 1.1556, time 43.35ms, mfu 6.83%\n",
            "iter 2120: loss 1.1706, time 45.27ms, mfu 6.83%\n",
            "iter 2130: loss 1.1947, time 43.90ms, mfu 6.86%\n",
            "iter 2140: loss 1.1672, time 45.58ms, mfu 6.85%\n",
            "iter 2150: loss 1.2022, time 42.86ms, mfu 6.89%\n",
            "iter 2160: loss 1.1743, time 44.64ms, mfu 6.90%\n",
            "iter 2170: loss 1.1863, time 44.78ms, mfu 6.90%\n",
            "iter 2180: loss 1.1989, time 44.81ms, mfu 6.91%\n",
            "iter 2190: loss 1.1812, time 43.96ms, mfu 6.92%\n",
            "iter 2200: loss 1.1699, time 43.77ms, mfu 6.94%\n",
            "iter 2210: loss 1.1933, time 45.11ms, mfu 6.93%\n",
            "iter 2220: loss 1.1900, time 44.41ms, mfu 6.94%\n",
            "iter 2230: loss 1.1971, time 44.38ms, mfu 6.95%\n",
            "iter 2240: loss 1.1939, time 44.21ms, mfu 6.95%\n",
            "iter 2250: loss 1.1720, time 45.70ms, mfu 6.94%\n",
            "iter 2260: loss 1.1530, time 44.51ms, mfu 6.94%\n",
            "iter 2270: loss 1.1636, time 43.53ms, mfu 6.96%\n",
            "iter 2280: loss 1.1617, time 44.61ms, mfu 6.96%\n",
            "iter 2290: loss 1.1702, time 44.82ms, mfu 6.96%\n",
            "iter 2300: loss 1.1575, time 44.72ms, mfu 6.96%\n",
            "iter 2310: loss 1.1688, time 43.69ms, mfu 6.97%\n",
            "iter 2320: loss 1.2032, time 44.20ms, mfu 6.98%\n",
            "iter 2330: loss 1.1434, time 45.26ms, mfu 6.97%\n",
            "iter 2340: loss 1.1702, time 44.58ms, mfu 6.97%\n",
            "iter 2350: loss 1.1616, time 45.07ms, mfu 6.96%\n",
            "iter 2360: loss 1.1666, time 45.15ms, mfu 6.95%\n",
            "iter 2370: loss 1.1559, time 43.63ms, mfu 6.97%\n",
            "iter 2380: loss 1.1552, time 45.24ms, mfu 6.96%\n",
            "iter 2390: loss 1.1366, time 44.90ms, mfu 6.95%\n",
            "iter 2400: loss 1.1147, time 43.34ms, mfu 6.98%\n",
            "iter 2410: loss 1.1641, time 45.12ms, mfu 6.97%\n",
            "iter 2420: loss 1.1612, time 45.21ms, mfu 6.96%\n",
            "iter 2430: loss 1.1742, time 44.95ms, mfu 6.95%\n",
            "iter 2440: loss 1.1611, time 43.72ms, mfu 6.97%\n",
            "iter 2450: loss 1.1515, time 45.67ms, mfu 6.95%\n",
            "iter 2460: loss 1.1192, time 44.72ms, mfu 6.95%\n",
            "iter 2470: loss 1.1524, time 44.36ms, mfu 6.96%\n",
            "iter 2480: loss 1.1732, time 45.01ms, mfu 6.95%\n",
            "iter 2490: loss 1.1536, time 44.66ms, mfu 6.95%\n",
            "iter 2500: loss 1.1439, time 44.28ms, mfu 6.96%\n",
            "iter 2510: loss 1.1217, time 45.09ms, mfu 6.95%\n",
            "iter 2520: loss 1.1356, time 44.67ms, mfu 6.95%\n",
            "iter 2530: loss 1.1073, time 43.26ms, mfu 6.97%\n",
            "iter 2540: loss 1.1402, time 45.40ms, mfu 6.96%\n",
            "iter 2550: loss 1.1420, time 45.36ms, mfu 6.95%\n",
            "iter 2560: loss 1.1440, time 43.68ms, mfu 6.97%\n",
            "iter 2570: loss 1.1532, time 44.49ms, mfu 6.97%\n",
            "iter 2580: loss 1.1514, time 45.45ms, mfu 6.95%\n",
            "iter 2590: loss 1.1365, time 43.39ms, mfu 6.97%\n",
            "iter 2600: loss 1.1296, time 45.33ms, mfu 6.96%\n",
            "iter 2610: loss 1.1473, time 45.11ms, mfu 6.96%\n",
            "iter 2620: loss 1.1530, time 44.15ms, mfu 6.96%\n",
            "iter 2630: loss 1.1117, time 45.35ms, mfu 6.95%\n",
            "iter 2640: loss 1.1491, time 45.35ms, mfu 6.94%\n",
            "iter 2650: loss 1.1583, time 43.91ms, mfu 6.96%\n",
            "iter 2660: loss 1.1054, time 44.74ms, mfu 6.95%\n",
            "iter 2670: loss 1.1238, time 45.85ms, mfu 6.94%\n",
            "iter 2680: loss 1.1264, time 44.20ms, mfu 6.95%\n",
            "iter 2690: loss 1.1383, time 45.65ms, mfu 6.93%\n",
            "iter 2700: loss 1.1072, time 44.63ms, mfu 6.93%\n",
            "iter 2710: loss 1.1147, time 44.98ms, mfu 6.93%\n",
            "iter 2720: loss 1.0910, time 45.05ms, mfu 6.93%\n",
            "iter 2730: loss 1.0991, time 43.23ms, mfu 6.95%\n",
            "iter 2740: loss 1.1185, time 45.74ms, mfu 6.94%\n",
            "iter 2750: loss 1.1088, time 45.09ms, mfu 6.93%\n",
            "iter 2760: loss 1.1270, time 44.13ms, mfu 6.94%\n",
            "iter 2770: loss 1.0975, time 44.60ms, mfu 6.95%\n",
            "iter 2780: loss 1.1081, time 45.14ms, mfu 6.94%\n",
            "iter 2790: loss 1.1415, time 43.74ms, mfu 6.96%\n",
            "iter 2800: loss 1.1063, time 44.16ms, mfu 6.96%\n",
            "iter 2810: loss 1.0893, time 45.84ms, mfu 6.94%\n",
            "iter 2820: loss 1.1184, time 43.43ms, mfu 6.97%\n",
            "iter 2830: loss 1.0908, time 43.37ms, mfu 6.99%\n",
            "iter 2840: loss 1.1269, time 45.22ms, mfu 6.97%\n",
            "iter 2850: loss 1.1337, time 44.21ms, mfu 6.98%\n",
            "iter 2860: loss 1.1009, time 43.80ms, mfu 6.99%\n",
            "iter 2870: loss 1.1208, time 44.72ms, mfu 6.99%\n",
            "iter 2880: loss 1.1125, time 45.00ms, mfu 6.98%\n",
            "iter 2890: loss 1.0972, time 43.55ms, mfu 6.99%\n",
            "iter 2900: loss 1.0923, time 44.70ms, mfu 6.99%\n",
            "iter 2910: loss 1.1071, time 44.72ms, mfu 6.98%\n",
            "iter 2920: loss 1.0929, time 43.73ms, mfu 7.00%\n",
            "iter 2930: loss 1.1148, time 43.18ms, mfu 7.02%\n",
            "iter 2940: loss 1.0918, time 44.92ms, mfu 7.01%\n",
            "iter 2950: loss 1.0722, time 45.20ms, mfu 6.99%\n",
            "iter 2960: loss 1.1048, time 44.11ms, mfu 7.00%\n",
            "iter 2970: loss 1.1144, time 44.72ms, mfu 6.99%\n",
            "iter 2980: loss 1.1115, time 45.34ms, mfu 6.98%\n",
            "iter 2990: loss 1.1092, time 45.03ms, mfu 6.97%\n",
            "step 3000: train loss 0.9730, val loss 1.4765\n",
            "iter 3000: loss 1.0798, time 5958.65ms, mfu 6.28%\n",
            "iter 3010: loss 1.1023, time 45.90ms, mfu 6.33%\n",
            "iter 3020: loss 1.0784, time 44.27ms, mfu 6.40%\n",
            "iter 3030: loss 1.0689, time 45.10ms, mfu 6.45%\n",
            "iter 3040: loss 1.0816, time 44.84ms, mfu 6.49%\n",
            "iter 3050: loss 1.0872, time 44.59ms, mfu 6.54%\n",
            "iter 3060: loss 1.0582, time 44.13ms, mfu 6.59%\n",
            "iter 3070: loss 1.0742, time 42.25ms, mfu 6.67%\n",
            "iter 3080: loss 1.0783, time 44.61ms, mfu 6.70%\n",
            "iter 3090: loss 1.0728, time 44.41ms, mfu 6.73%\n",
            "iter 3100: loss 1.0910, time 45.80ms, mfu 6.73%\n",
            "iter 3110: loss 1.0659, time 43.53ms, mfu 6.77%\n",
            "iter 3120: loss 1.0861, time 44.44ms, mfu 6.79%\n",
            "iter 3130: loss 1.1035, time 44.34ms, mfu 6.82%\n",
            "iter 3140: loss 1.0747, time 44.44ms, mfu 6.83%\n",
            "iter 3150: loss 1.0728, time 45.53ms, mfu 6.83%\n",
            "iter 3160: loss 1.0814, time 43.57ms, mfu 6.86%\n",
            "iter 3170: loss 1.0395, time 43.81ms, mfu 6.88%\n",
            "iter 3180: loss 1.0151, time 44.12ms, mfu 6.90%\n",
            "iter 3190: loss 1.0840, time 44.81ms, mfu 6.90%\n",
            "iter 3200: loss 1.0790, time 44.98ms, mfu 6.90%\n",
            "iter 3210: loss 1.0668, time 44.70ms, mfu 6.91%\n",
            "iter 3220: loss 1.0694, time 43.81ms, mfu 6.93%\n",
            "iter 3230: loss 1.0806, time 44.06ms, mfu 6.94%\n",
            "iter 3240: loss 1.0366, time 45.29ms, mfu 6.93%\n",
            "iter 3250: loss 1.0831, time 44.75ms, mfu 6.93%\n",
            "iter 3260: loss 1.0751, time 44.07ms, mfu 6.94%\n",
            "iter 3270: loss 1.0588, time 43.26ms, mfu 6.97%\n",
            "iter 3280: loss 1.0524, time 45.22ms, mfu 6.96%\n",
            "iter 3290: loss 1.0581, time 43.62ms, mfu 6.97%\n",
            "iter 3300: loss 1.0539, time 44.92ms, mfu 6.97%\n",
            "iter 3310: loss 1.0345, time 44.92ms, mfu 6.96%\n",
            "iter 3320: loss 1.0618, time 44.12ms, mfu 6.97%\n",
            "iter 3330: loss 1.0404, time 43.73ms, mfu 6.98%\n",
            "iter 3340: loss 1.0525, time 43.37ms, mfu 7.00%\n",
            "iter 3350: loss 1.0425, time 44.40ms, mfu 7.00%\n",
            "iter 3360: loss 1.0480, time 44.84ms, mfu 6.99%\n",
            "iter 3370: loss 1.0608, time 44.94ms, mfu 6.99%\n",
            "iter 3380: loss 1.0792, time 44.68ms, mfu 6.98%\n",
            "iter 3390: loss 1.0538, time 42.83ms, mfu 7.01%\n",
            "iter 3400: loss 1.0616, time 43.46ms, mfu 7.02%\n",
            "iter 3410: loss 1.0348, time 44.51ms, mfu 7.02%\n",
            "iter 3420: loss 1.0238, time 43.61ms, mfu 7.03%\n",
            "iter 3430: loss 1.0612, time 45.02ms, mfu 7.02%\n",
            "iter 3440: loss 1.0075, time 44.55ms, mfu 7.01%\n",
            "iter 3450: loss 1.0375, time 44.93ms, mfu 7.00%\n",
            "iter 3460: loss 1.0653, time 44.21ms, mfu 7.00%\n",
            "iter 3470: loss 1.0626, time 45.23ms, mfu 6.99%\n",
            "iter 3480: loss 1.0209, time 44.09ms, mfu 7.00%\n",
            "iter 3490: loss 1.0257, time 43.52ms, mfu 7.01%\n",
            "iter 3500: loss 1.0073, time 43.23ms, mfu 7.03%\n",
            "iter 3510: loss 1.0430, time 43.29ms, mfu 7.04%\n",
            "iter 3520: loss 1.0282, time 44.25ms, mfu 7.04%\n",
            "iter 3530: loss 1.0165, time 44.11ms, mfu 7.04%\n",
            "iter 3540: loss 1.0320, time 44.57ms, mfu 7.03%\n",
            "iter 3550: loss 1.0361, time 44.75ms, mfu 7.02%\n",
            "iter 3560: loss 1.0392, time 44.77ms, mfu 7.02%\n",
            "iter 3570: loss 1.0374, time 44.64ms, mfu 7.01%\n",
            "iter 3580: loss 1.0149, time 44.12ms, mfu 7.01%\n",
            "iter 3590: loss 1.0438, time 43.78ms, mfu 7.02%\n",
            "iter 3600: loss 1.0482, time 44.42ms, mfu 7.02%\n",
            "iter 3610: loss 1.0390, time 44.22ms, mfu 7.02%\n",
            "iter 3620: loss 1.0158, time 42.93ms, mfu 7.04%\n",
            "iter 3630: loss 1.0209, time 43.84ms, mfu 7.05%\n",
            "iter 3640: loss 1.0057, time 43.95ms, mfu 7.05%\n",
            "iter 3650: loss 1.0417, time 44.39ms, mfu 7.04%\n",
            "iter 3660: loss 1.0390, time 44.56ms, mfu 7.04%\n",
            "iter 3670: loss 1.0347, time 44.83ms, mfu 7.03%\n",
            "iter 3680: loss 1.0386, time 45.29ms, mfu 7.01%\n",
            "iter 3690: loss 1.0379, time 44.16ms, mfu 7.01%\n",
            "iter 3700: loss 1.0340, time 44.37ms, mfu 7.01%\n",
            "iter 3710: loss 1.0237, time 43.40ms, mfu 7.03%\n",
            "iter 3720: loss 0.9954, time 43.43ms, mfu 7.04%\n",
            "iter 3730: loss 1.0234, time 44.38ms, mfu 7.03%\n",
            "iter 3740: loss 1.0349, time 43.78ms, mfu 7.04%\n",
            "iter 3750: loss 1.0050, time 45.28ms, mfu 7.02%\n",
            "iter 3760: loss 1.0148, time 44.93ms, mfu 7.01%\n",
            "iter 3770: loss 1.0247, time 44.32ms, mfu 7.01%\n",
            "iter 3780: loss 1.0213, time 44.44ms, mfu 7.01%\n",
            "iter 3790: loss 1.0224, time 45.02ms, mfu 7.00%\n",
            "iter 3800: loss 1.0180, time 43.56ms, mfu 7.01%\n",
            "iter 3810: loss 1.0159, time 43.87ms, mfu 7.02%\n",
            "iter 3820: loss 1.0210, time 44.01ms, mfu 7.02%\n",
            "iter 3830: loss 0.9933, time 44.94ms, mfu 7.01%\n",
            "iter 3840: loss 0.9978, time 45.01ms, mfu 7.00%\n",
            "iter 3850: loss 1.0293, time 45.05ms, mfu 6.99%\n",
            "iter 3860: loss 0.9803, time 44.68ms, mfu 6.99%\n",
            "iter 3870: loss 1.0227, time 45.06ms, mfu 6.98%\n",
            "iter 3880: loss 1.0220, time 44.25ms, mfu 6.98%\n",
            "iter 3890: loss 1.0237, time 43.44ms, mfu 7.00%\n",
            "iter 3900: loss 1.0280, time 44.02ms, mfu 7.00%\n",
            "iter 3910: loss 1.0098, time 43.05ms, mfu 7.03%\n",
            "iter 3920: loss 1.0315, time 44.97ms, mfu 7.01%\n",
            "iter 3930: loss 1.0035, time 44.78ms, mfu 7.01%\n",
            "iter 3940: loss 0.9838, time 44.72ms, mfu 7.00%\n",
            "iter 3950: loss 0.9915, time 43.86ms, mfu 7.01%\n",
            "iter 3960: loss 0.9826, time 43.85ms, mfu 7.02%\n",
            "iter 3970: loss 1.0037, time 43.57ms, mfu 7.03%\n",
            "iter 3980: loss 1.0153, time 43.56ms, mfu 7.04%\n",
            "iter 3990: loss 1.0061, time 45.04ms, mfu 7.02%\n",
            "step 4000: train loss 0.8528, val loss 1.5227\n",
            "iter 4000: loss 1.0022, time 5936.36ms, mfu 6.33%\n",
            "iter 4010: loss 1.0280, time 45.63ms, mfu 6.37%\n",
            "iter 4020: loss 0.9933, time 44.80ms, mfu 6.43%\n",
            "iter 4030: loss 1.0051, time 44.30ms, mfu 6.49%\n",
            "iter 4040: loss 0.9983, time 45.46ms, mfu 6.52%\n",
            "iter 4050: loss 0.9879, time 44.70ms, mfu 6.57%\n",
            "iter 4060: loss 1.0115, time 43.91ms, mfu 6.62%\n",
            "iter 4070: loss 1.0139, time 43.37ms, mfu 6.67%\n",
            "iter 4080: loss 0.9877, time 45.99ms, mfu 6.68%\n",
            "iter 4090: loss 1.0084, time 44.03ms, mfu 6.72%\n",
            "iter 4100: loss 0.9911, time 44.98ms, mfu 6.74%\n",
            "iter 4110: loss 1.0167, time 44.35ms, mfu 6.76%\n",
            "iter 4120: loss 1.0057, time 43.66ms, mfu 6.80%\n",
            "iter 4130: loss 1.0033, time 45.13ms, mfu 6.81%\n",
            "iter 4140: loss 1.0020, time 45.19ms, mfu 6.81%\n",
            "iter 4150: loss 0.9925, time 43.98ms, mfu 6.84%\n",
            "iter 4160: loss 1.0139, time 43.63ms, mfu 6.87%\n",
            "iter 4170: loss 0.9790, time 44.21ms, mfu 6.88%\n",
            "iter 4180: loss 0.9747, time 43.97ms, mfu 6.90%\n",
            "iter 4190: loss 0.9915, time 44.69ms, mfu 6.91%\n",
            "iter 4200: loss 0.9977, time 44.01ms, mfu 6.92%\n",
            "iter 4210: loss 0.9768, time 44.44ms, mfu 6.93%\n",
            "iter 4220: loss 1.0087, time 43.37ms, mfu 6.95%\n",
            "iter 4230: loss 0.9914, time 43.87ms, mfu 6.96%\n",
            "iter 4240: loss 0.9765, time 45.74ms, mfu 6.95%\n",
            "iter 4250: loss 0.9708, time 44.61ms, mfu 6.95%\n",
            "iter 4260: loss 0.9863, time 44.81ms, mfu 6.95%\n",
            "iter 4270: loss 0.9916, time 44.11ms, mfu 6.96%\n",
            "iter 4280: loss 0.9852, time 45.66ms, mfu 6.94%\n",
            "iter 4290: loss 0.9646, time 44.03ms, mfu 6.95%\n",
            "iter 4300: loss 0.9904, time 44.18ms, mfu 6.96%\n",
            "iter 4310: loss 0.9767, time 43.56ms, mfu 6.98%\n",
            "iter 4320: loss 0.9932, time 44.32ms, mfu 6.98%\n",
            "iter 4330: loss 0.9742, time 45.40ms, mfu 6.97%\n",
            "iter 4340: loss 0.9735, time 45.26ms, mfu 6.96%\n",
            "iter 4350: loss 0.9669, time 44.51ms, mfu 6.96%\n",
            "iter 4360: loss 0.9734, time 43.75ms, mfu 6.97%\n",
            "iter 4370: loss 0.9606, time 43.79ms, mfu 6.99%\n",
            "iter 4380: loss 0.9762, time 44.06ms, mfu 6.99%\n",
            "iter 4390: loss 0.9563, time 45.26ms, mfu 6.98%\n",
            "iter 4400: loss 0.9683, time 44.07ms, mfu 6.99%\n",
            "iter 4410: loss 0.9774, time 43.99ms, mfu 6.99%\n",
            "iter 4420: loss 0.9667, time 43.30ms, mfu 7.01%\n",
            "iter 4430: loss 0.9468, time 43.36ms, mfu 7.03%\n",
            "iter 4440: loss 0.9914, time 45.56ms, mfu 7.01%\n",
            "iter 4450: loss 0.9715, time 44.65ms, mfu 7.00%\n",
            "iter 4460: loss 0.9863, time 44.46ms, mfu 7.00%\n",
            "iter 4470: loss 0.9946, time 43.20ms, mfu 7.02%\n",
            "iter 4480: loss 0.9634, time 44.12ms, mfu 7.02%\n",
            "iter 4490: loss 0.9615, time 45.12ms, mfu 7.01%\n",
            "iter 4500: loss 0.9606, time 44.92ms, mfu 7.00%\n",
            "iter 4510: loss 0.9691, time 44.02ms, mfu 7.00%\n",
            "iter 4520: loss 0.9539, time 44.00ms, mfu 7.01%\n",
            "iter 4530: loss 0.9576, time 44.51ms, mfu 7.01%\n",
            "iter 4540: loss 0.9567, time 45.01ms, mfu 7.00%\n",
            "iter 4550: loss 0.9522, time 45.17ms, mfu 6.98%\n",
            "iter 4560: loss 0.9702, time 44.75ms, mfu 6.98%\n",
            "iter 4570: loss 0.9647, time 44.61ms, mfu 6.98%\n",
            "iter 4580: loss 0.9378, time 42.90ms, mfu 7.00%\n",
            "iter 4590: loss 0.9492, time 45.37ms, mfu 6.99%\n",
            "iter 4600: loss 0.9530, time 44.96ms, mfu 6.98%\n",
            "iter 4610: loss 0.9631, time 44.54ms, mfu 6.98%\n",
            "iter 4620: loss 0.9468, time 43.29ms, mfu 7.00%\n",
            "iter 4630: loss 0.9442, time 44.30ms, mfu 7.00%\n",
            "iter 4640: loss 0.9531, time 44.96ms, mfu 6.99%\n",
            "iter 4650: loss 0.9667, time 45.21ms, mfu 6.98%\n",
            "iter 4660: loss 0.9602, time 43.60ms, mfu 6.99%\n",
            "iter 4670: loss 0.9620, time 44.36ms, mfu 7.00%\n",
            "iter 4680: loss 0.9386, time 45.71ms, mfu 6.98%\n",
            "iter 4690: loss 0.9819, time 44.07ms, mfu 6.98%\n",
            "iter 4700: loss 0.9610, time 43.17ms, mfu 7.00%\n",
            "iter 4710: loss 0.9796, time 44.31ms, mfu 7.00%\n",
            "iter 4720: loss 0.9663, time 45.23ms, mfu 6.99%\n",
            "iter 4730: loss 0.9738, time 44.79ms, mfu 6.99%\n",
            "iter 4740: loss 0.9470, time 44.82ms, mfu 6.98%\n",
            "iter 4750: loss 0.9662, time 43.42ms, mfu 7.00%\n",
            "iter 4760: loss 0.9609, time 45.49ms, mfu 6.98%\n",
            "iter 4770: loss 0.9576, time 45.20ms, mfu 6.97%\n",
            "iter 4780: loss 0.9766, time 44.22ms, mfu 6.98%\n",
            "iter 4790: loss 0.9414, time 43.03ms, mfu 7.00%\n",
            "iter 4800: loss 0.9661, time 43.40ms, mfu 7.02%\n",
            "iter 4810: loss 0.9683, time 44.87ms, mfu 7.01%\n",
            "iter 4820: loss 0.9575, time 44.56ms, mfu 7.00%\n",
            "iter 4830: loss 0.9492, time 44.63ms, mfu 7.00%\n",
            "iter 4840: loss 0.9558, time 44.43ms, mfu 7.00%\n",
            "iter 4850: loss 0.9502, time 44.39ms, mfu 7.00%\n",
            "iter 4860: loss 0.9472, time 43.90ms, mfu 7.01%\n",
            "iter 4870: loss 0.9755, time 45.43ms, mfu 6.99%\n",
            "iter 4880: loss 0.9565, time 43.67ms, mfu 7.00%\n",
            "iter 4890: loss 0.9391, time 43.22ms, mfu 7.02%\n",
            "iter 4900: loss 0.9553, time 45.02ms, mfu 7.01%\n",
            "iter 4910: loss 0.9480, time 45.21ms, mfu 6.99%\n",
            "iter 4920: loss 0.9500, time 44.51ms, mfu 6.99%\n",
            "iter 4930: loss 0.9399, time 43.21ms, mfu 7.01%\n",
            "iter 4940: loss 0.9572, time 44.44ms, mfu 7.01%\n",
            "iter 4950: loss 0.9595, time 44.53ms, mfu 7.01%\n",
            "iter 4960: loss 0.9320, time 44.97ms, mfu 7.00%\n",
            "iter 4970: loss 0.9635, time 44.52ms, mfu 7.00%\n",
            "iter 4980: loss 0.9420, time 43.41ms, mfu 7.01%\n",
            "iter 4990: loss 0.9608, time 44.37ms, mfu 7.01%\n",
            "step 5000: train loss 0.7863, val loss 1.5592\n",
            "iter 5000: loss 0.9588, time 5936.09ms, mfu 6.31%\n",
            "iter 5010: loss 0.9584, time 45.52ms, mfu 6.37%\n",
            "iter 5020: loss 0.9419, time 44.70ms, mfu 6.42%\n",
            "iter 5030: loss 0.9590, time 44.97ms, mfu 6.47%\n",
            "iter 5040: loss 0.9277, time 43.96ms, mfu 6.53%\n",
            "iter 5050: loss 0.9415, time 44.34ms, mfu 6.58%\n",
            "iter 5060: loss 0.9583, time 45.59ms, mfu 6.60%\n",
            "iter 5070: loss 0.9194, time 43.77ms, mfu 6.65%\n",
            "iter 5080: loss 0.9581, time 44.20ms, mfu 6.69%\n",
            "iter 5090: loss 0.9613, time 44.42ms, mfu 6.72%\n",
            "iter 5100: loss 0.9563, time 44.78ms, mfu 6.74%\n",
            "iter 5110: loss 0.9607, time 44.19ms, mfu 6.77%\n",
            "iter 5120: loss 0.9420, time 42.83ms, mfu 6.82%\n",
            "iter 5130: loss 0.9469, time 45.75ms, mfu 6.82%\n",
            "iter 5140: loss 0.9589, time 45.32ms, mfu 6.82%\n",
            "iter 5150: loss 0.9305, time 44.89ms, mfu 6.83%\n",
            "iter 5160: loss 0.9637, time 43.99ms, mfu 6.85%\n",
            "iter 5170: loss 0.9447, time 43.87ms, mfu 6.88%\n",
            "iter 5180: loss 0.9407, time 44.01ms, mfu 6.89%\n",
            "iter 5190: loss 0.9565, time 44.29ms, mfu 6.91%\n",
            "iter 5200: loss 0.9608, time 44.77ms, mfu 6.91%\n",
            "iter 5210: loss 0.9277, time 44.48ms, mfu 6.92%\n",
            "iter 5220: loss 0.9518, time 43.58ms, mfu 6.94%\n",
            "iter 5230: loss 0.9609, time 44.36ms, mfu 6.94%\n",
            "iter 5240: loss 0.9451, time 45.07ms, mfu 6.94%\n",
            "iter 5250: loss 0.9510, time 44.77ms, mfu 6.94%\n",
            "iter 5260: loss 0.9342, time 44.17ms, mfu 6.95%\n",
            "iter 5270: loss 0.9578, time 44.37ms, mfu 6.95%\n",
            "iter 5280: loss 0.9503, time 44.72ms, mfu 6.95%\n",
            "iter 5290: loss 0.9645, time 44.41ms, mfu 6.96%\n",
            "iter 5300: loss 0.9539, time 43.61ms, mfu 6.97%\n",
            "iter 5310: loss 0.9225, time 42.86ms, mfu 7.00%\n",
            "iter 5320: loss 0.9422, time 43.57ms, mfu 7.01%\n",
            "iter 5330: loss 0.9524, time 43.96ms, mfu 7.02%\n",
            "iter 5340: loss 0.9459, time 45.57ms, mfu 7.00%\n",
            "iter 5350: loss 0.9424, time 45.04ms, mfu 6.99%\n",
            "iter 5360: loss 0.9257, time 44.70ms, mfu 6.98%\n",
            "iter 5370: loss 0.9288, time 44.01ms, mfu 6.99%\n",
            "iter 5380: loss 0.9578, time 44.31ms, mfu 6.99%\n",
            "iter 5390: loss 0.9470, time 45.14ms, mfu 6.98%\n",
            "iter 5400: loss 0.9234, time 44.30ms, mfu 6.99%\n",
            "iter 5410: loss 0.9173, time 44.53ms, mfu 6.98%\n",
            "iter 5420: loss 0.9501, time 42.86ms, mfu 7.01%\n",
            "iter 5430: loss 0.9621, time 43.95ms, mfu 7.02%\n",
            "iter 5440: loss 0.9470, time 44.46ms, mfu 7.01%\n",
            "iter 5450: loss 0.9324, time 44.39ms, mfu 7.01%\n",
            "iter 5460: loss 0.9733, time 44.70ms, mfu 7.01%\n",
            "iter 5470: loss 0.9443, time 44.35ms, mfu 7.01%\n",
            "iter 5480: loss 0.9416, time 43.63ms, mfu 7.02%\n",
            "iter 5490: loss 0.9294, time 43.59ms, mfu 7.03%\n",
            "iter 5500: loss 0.9520, time 44.82ms, mfu 7.02%\n",
            "iter 5510: loss 0.9327, time 44.91ms, mfu 7.01%\n",
            "iter 5520: loss 0.9279, time 44.65ms, mfu 7.00%\n",
            "iter 5530: loss 0.9359, time 43.30ms, mfu 7.02%\n",
            "iter 5540: loss 0.9560, time 43.01ms, mfu 7.04%\n",
            "iter 5550: loss 0.9347, time 44.25ms, mfu 7.04%\n",
            "iter 5560: loss 0.9248, time 45.29ms, mfu 7.02%\n",
            "iter 5570: loss 0.9144, time 44.54ms, mfu 7.02%\n",
            "iter 5580: loss 0.9332, time 43.16ms, mfu 7.03%\n",
            "iter 5590: loss 0.9488, time 43.77ms, mfu 7.04%\n",
            "iter 5600: loss 0.9421, time 44.63ms, mfu 7.03%\n",
            "iter 5610: loss 0.9565, time 44.75ms, mfu 7.02%\n",
            "iter 5620: loss 0.9383, time 45.53ms, mfu 7.00%\n",
            "iter 5630: loss 0.9322, time 43.89ms, mfu 7.01%\n",
            "iter 5640: loss 0.9490, time 43.71ms, mfu 7.02%\n",
            "iter 5650: loss 0.9375, time 43.88ms, mfu 7.03%\n",
            "iter 5660: loss 0.9261, time 43.82ms, mfu 7.03%\n",
            "iter 5670: loss 0.9467, time 45.23ms, mfu 7.02%\n",
            "iter 5680: loss 0.9131, time 44.23ms, mfu 7.02%\n",
            "iter 5690: loss 0.9333, time 43.86ms, mfu 7.02%\n",
            "iter 5700: loss 0.9202, time 43.68ms, mfu 7.03%\n",
            "iter 5710: loss 0.9385, time 43.97ms, mfu 7.04%\n",
            "iter 5720: loss 0.9310, time 43.97ms, mfu 7.04%\n",
            "iter 5730: loss 0.9198, time 45.67ms, mfu 7.01%\n",
            "iter 5740: loss 0.9256, time 44.13ms, mfu 7.02%\n",
            "iter 5750: loss 0.9487, time 43.87ms, mfu 7.02%\n",
            "iter 5760: loss 0.9395, time 44.10ms, mfu 7.03%\n",
            "iter 5770: loss 0.9359, time 44.14ms, mfu 7.03%\n",
            "iter 5780: loss 0.9385, time 44.55ms, mfu 7.02%\n",
            "iter 5790: loss 0.9088, time 45.41ms, mfu 7.00%\n",
            "iter 5800: loss 0.9311, time 44.34ms, mfu 7.00%\n",
            "iter 5810: loss 0.9120, time 43.32ms, mfu 7.02%\n",
            "iter 5820: loss 0.9147, time 43.79ms, mfu 7.03%\n",
            "iter 5830: loss 0.9369, time 44.12ms, mfu 7.03%\n",
            "iter 5840: loss 0.9329, time 44.96ms, mfu 7.02%\n",
            "iter 5850: loss 0.9234, time 44.80ms, mfu 7.01%\n",
            "iter 5860: loss 0.9261, time 44.20ms, mfu 7.01%\n",
            "iter 5870: loss 0.9220, time 44.07ms, mfu 7.01%\n",
            "iter 5880: loss 0.9488, time 44.49ms, mfu 7.01%\n",
            "iter 5890: loss 0.9216, time 44.48ms, mfu 7.01%\n",
            "iter 5900: loss 0.9383, time 44.74ms, mfu 7.00%\n",
            "iter 5910: loss 0.9435, time 45.29ms, mfu 6.99%\n",
            "iter 5920: loss 0.9210, time 44.05ms, mfu 6.99%\n",
            "iter 5930: loss 0.9214, time 44.22ms, mfu 7.00%\n",
            "iter 5940: loss 0.9163, time 44.08ms, mfu 7.00%\n",
            "iter 5950: loss 0.9295, time 43.39ms, mfu 7.02%\n",
            "iter 5960: loss 0.8894, time 44.24ms, mfu 7.02%\n",
            "iter 5970: loss 0.9124, time 44.93ms, mfu 7.01%\n",
            "iter 5980: loss 0.9324, time 44.94ms, mfu 7.00%\n",
            "iter 5990: loss 0.9348, time 44.49ms, mfu 7.00%\n",
            "step 6000: train loss 0.7475, val loss 1.5930\n",
            "iter 6000: loss 0.9409, time 5941.61ms, mfu 6.30%\n",
            "Extracted losses: train=0.7863, val=1.5592\n",
            "\n",
            "Training model with 7 layers and 4 heads\n",
            "Running command: python train.py config/train_shakespeare_char_7layer_4head.py\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "W0503 17:12:27.403000 13893 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "Overriding config with config/train_shakespeare_char_7layer_4head.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 1000 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 7\n",
            "n_head = 4\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 6000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 12.42M\n",
            "num decayed parameter tensors: 30, with 12,509,568 parameters\n",
            "num non-decayed parameter tensors: 15, with 5,760 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 4.1967, val loss 4.1906\n",
            "iter 0: loss 4.2097, time 24746.11ms, mfu -100.00%\n",
            "iter 10: loss 3.0986, time 57.89ms, mfu 7.51%\n",
            "iter 20: loss 2.7448, time 64.44ms, mfu 7.43%\n",
            "iter 30: loss 2.6242, time 59.27ms, mfu 7.42%\n",
            "iter 40: loss 2.5578, time 63.05ms, mfu 7.37%\n",
            "iter 50: loss 2.5317, time 62.73ms, mfu 7.32%\n",
            "iter 60: loss 2.4921, time 59.42ms, mfu 7.32%\n",
            "iter 70: loss 2.5175, time 63.46ms, mfu 7.28%\n",
            "iter 80: loss 2.4920, time 60.13ms, mfu 7.27%\n",
            "iter 90: loss 2.4686, time 63.06ms, mfu 7.23%\n",
            "iter 100: loss 2.4757, time 60.90ms, mfu 7.22%\n",
            "iter 110: loss 2.4810, time 59.17ms, mfu 7.24%\n",
            "iter 120: loss 2.4509, time 63.85ms, mfu 7.19%\n",
            "iter 130: loss 2.4202, time 60.24ms, mfu 7.19%\n",
            "iter 140: loss 2.3737, time 62.31ms, mfu 7.17%\n",
            "iter 150: loss 2.3639, time 60.51ms, mfu 7.17%\n",
            "iter 160: loss 2.3306, time 59.92ms, mfu 7.18%\n",
            "iter 170: loss 2.3421, time 61.93ms, mfu 7.17%\n",
            "iter 180: loss 2.2682, time 60.23ms, mfu 7.17%\n",
            "iter 190: loss 2.2187, time 61.29ms, mfu 7.16%\n",
            "iter 200: loss 2.1824, time 60.47ms, mfu 7.16%\n",
            "iter 210: loss 2.1505, time 61.40ms, mfu 7.16%\n",
            "iter 220: loss 2.0983, time 64.07ms, mfu 7.12%\n",
            "iter 230: loss 2.0580, time 63.71ms, mfu 7.09%\n",
            "iter 240: loss 2.0490, time 59.91ms, mfu 7.11%\n",
            "iter 250: loss 2.0040, time 60.56ms, mfu 7.11%\n",
            "iter 260: loss 1.9646, time 63.13ms, mfu 7.09%\n",
            "iter 270: loss 1.9477, time 63.04ms, mfu 7.07%\n",
            "iter 280: loss 1.9138, time 63.60ms, mfu 7.05%\n",
            "iter 290: loss 1.9011, time 62.46ms, mfu 7.04%\n",
            "iter 300: loss 1.8977, time 63.34ms, mfu 7.02%\n",
            "iter 310: loss 1.8747, time 62.71ms, mfu 7.01%\n",
            "iter 320: loss 1.8667, time 64.17ms, mfu 6.99%\n",
            "iter 330: loss 1.8308, time 62.61ms, mfu 6.98%\n",
            "iter 340: loss 1.8335, time 62.74ms, mfu 6.98%\n",
            "iter 350: loss 1.8011, time 64.23ms, mfu 6.96%\n",
            "iter 360: loss 1.7849, time 62.20ms, mfu 6.96%\n",
            "iter 370: loss 1.7605, time 62.96ms, mfu 6.95%\n",
            "iter 380: loss 1.7371, time 64.47ms, mfu 6.93%\n",
            "iter 390: loss 1.7247, time 63.05ms, mfu 6.93%\n",
            "iter 400: loss 1.7399, time 62.61ms, mfu 6.93%\n",
            "iter 410: loss 1.6960, time 63.92ms, mfu 6.92%\n",
            "iter 420: loss 1.6907, time 64.18ms, mfu 6.90%\n",
            "iter 430: loss 1.6775, time 63.09ms, mfu 6.90%\n",
            "iter 440: loss 1.6711, time 62.40ms, mfu 6.91%\n",
            "iter 450: loss 1.6856, time 64.00ms, mfu 6.90%\n",
            "iter 460: loss 1.6181, time 64.30ms, mfu 6.88%\n",
            "iter 470: loss 1.6492, time 62.32ms, mfu 6.89%\n",
            "iter 480: loss 1.6195, time 63.70ms, mfu 6.88%\n",
            "iter 490: loss 1.6048, time 62.57ms, mfu 6.89%\n",
            "iter 500: loss 1.6359, time 62.89ms, mfu 6.89%\n",
            "iter 510: loss 1.5959, time 62.23ms, mfu 6.90%\n",
            "iter 520: loss 1.5744, time 62.55ms, mfu 6.91%\n",
            "iter 530: loss 1.5785, time 62.17ms, mfu 6.91%\n",
            "iter 540: loss 1.5880, time 63.00ms, mfu 6.91%\n",
            "iter 550: loss 1.5678, time 61.86ms, mfu 6.92%\n",
            "iter 560: loss 1.5761, time 62.78ms, mfu 6.92%\n",
            "iter 570: loss 1.5352, time 63.59ms, mfu 6.92%\n",
            "iter 580: loss 1.5772, time 62.26ms, mfu 6.92%\n",
            "iter 590: loss 1.5708, time 60.92ms, mfu 6.94%\n",
            "iter 600: loss 1.5512, time 61.61ms, mfu 6.95%\n",
            "iter 610: loss 1.4898, time 63.87ms, mfu 6.94%\n",
            "iter 620: loss 1.5229, time 63.63ms, mfu 6.93%\n",
            "iter 630: loss 1.5074, time 63.66ms, mfu 6.92%\n",
            "iter 640: loss 1.5347, time 61.94ms, mfu 6.93%\n",
            "iter 650: loss 1.4928, time 61.23ms, mfu 6.94%\n",
            "iter 660: loss 1.4853, time 60.70ms, mfu 6.97%\n",
            "iter 670: loss 1.5169, time 61.44ms, mfu 6.98%\n",
            "iter 680: loss 1.4682, time 60.56ms, mfu 7.00%\n",
            "iter 690: loss 1.4843, time 61.02ms, mfu 7.01%\n",
            "iter 700: loss 1.4922, time 60.18ms, mfu 7.03%\n",
            "iter 710: loss 1.4501, time 62.77ms, mfu 7.02%\n",
            "iter 720: loss 1.4669, time 61.11ms, mfu 7.03%\n",
            "iter 730: loss 1.4658, time 61.35ms, mfu 7.03%\n",
            "iter 740: loss 1.4336, time 61.20ms, mfu 7.04%\n",
            "iter 750: loss 1.4756, time 61.23ms, mfu 7.05%\n",
            "iter 760: loss 1.4409, time 61.41ms, mfu 7.05%\n",
            "iter 770: loss 1.4267, time 59.84ms, mfu 7.07%\n",
            "iter 780: loss 1.4167, time 62.36ms, mfu 7.06%\n",
            "iter 790: loss 1.4264, time 60.42ms, mfu 7.07%\n",
            "iter 800: loss 1.4198, time 63.07ms, mfu 7.06%\n",
            "iter 810: loss 1.4289, time 61.32ms, mfu 7.06%\n",
            "iter 820: loss 1.4326, time 59.99ms, mfu 7.08%\n",
            "iter 830: loss 1.4058, time 60.76ms, mfu 7.09%\n",
            "iter 840: loss 1.4194, time 61.75ms, mfu 7.08%\n",
            "iter 850: loss 1.3908, time 59.80ms, mfu 7.10%\n",
            "iter 860: loss 1.3619, time 62.17ms, mfu 7.09%\n",
            "iter 870: loss 1.3862, time 60.23ms, mfu 7.10%\n",
            "iter 880: loss 1.4181, time 62.55ms, mfu 7.09%\n",
            "iter 890: loss 1.3610, time 60.39ms, mfu 7.10%\n",
            "iter 900: loss 1.3453, time 63.04ms, mfu 7.08%\n",
            "iter 910: loss 1.3600, time 59.80ms, mfu 7.10%\n",
            "iter 920: loss 1.3714, time 61.08ms, mfu 7.10%\n",
            "iter 930: loss 1.3644, time 59.88ms, mfu 7.11%\n",
            "iter 940: loss 1.3573, time 59.97ms, mfu 7.13%\n",
            "iter 950: loss 1.3777, time 61.61ms, mfu 7.12%\n",
            "iter 960: loss 1.3599, time 59.65ms, mfu 7.14%\n",
            "iter 970: loss 1.3747, time 62.12ms, mfu 7.12%\n",
            "iter 980: loss 1.3595, time 60.40ms, mfu 7.13%\n",
            "iter 990: loss 1.3163, time 60.68ms, mfu 7.13%\n",
            "step 1000: train loss 1.2689, val loss 1.5170\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.3423, time 8309.53ms, mfu 6.42%\n",
            "iter 1010: loss 1.3342, time 58.45ms, mfu 6.53%\n",
            "iter 1020: loss 1.3175, time 64.39ms, mfu 6.55%\n",
            "iter 1030: loss 1.3354, time 59.51ms, mfu 6.62%\n",
            "iter 1040: loss 1.3343, time 62.70ms, mfu 6.65%\n",
            "iter 1050: loss 1.3315, time 62.71ms, mfu 6.68%\n",
            "iter 1060: loss 1.3256, time 58.85ms, mfu 6.75%\n",
            "iter 1070: loss 1.3367, time 62.50ms, mfu 6.77%\n",
            "iter 1080: loss 1.3136, time 60.60ms, mfu 6.81%\n",
            "iter 1090: loss 1.3260, time 60.32ms, mfu 6.85%\n",
            "iter 1100: loss 1.3261, time 61.15ms, mfu 6.88%\n",
            "iter 1110: loss 1.3356, time 59.53ms, mfu 6.92%\n",
            "iter 1120: loss 1.3233, time 61.09ms, mfu 6.94%\n",
            "iter 1130: loss 1.3410, time 62.13ms, mfu 6.94%\n",
            "iter 1140: loss 1.2929, time 59.13ms, mfu 6.98%\n",
            "iter 1150: loss 1.2794, time 62.07ms, mfu 6.99%\n",
            "iter 1160: loss 1.3092, time 60.24ms, mfu 7.01%\n",
            "iter 1170: loss 1.2948, time 59.94ms, mfu 7.03%\n",
            "iter 1180: loss 1.2896, time 61.34ms, mfu 7.04%\n",
            "iter 1190: loss 1.2941, time 61.82ms, mfu 7.04%\n",
            "iter 1200: loss 1.2679, time 59.71ms, mfu 7.06%\n",
            "iter 1210: loss 1.3159, time 61.06ms, mfu 7.07%\n",
            "iter 1220: loss 1.2700, time 61.47ms, mfu 7.07%\n",
            "iter 1230: loss 1.3022, time 60.11ms, mfu 7.08%\n",
            "iter 1240: loss 1.2866, time 59.74ms, mfu 7.10%\n",
            "iter 1250: loss 1.2884, time 62.81ms, mfu 7.08%\n",
            "iter 1260: loss 1.2600, time 61.91ms, mfu 7.08%\n",
            "iter 1270: loss 1.2644, time 60.69ms, mfu 7.09%\n",
            "iter 1280: loss 1.2632, time 59.26ms, mfu 7.11%\n",
            "iter 1290: loss 1.2777, time 60.23ms, mfu 7.12%\n",
            "iter 1300: loss 1.2797, time 61.23ms, mfu 7.12%\n",
            "iter 1310: loss 1.2510, time 59.81ms, mfu 7.13%\n",
            "iter 1320: loss 1.2481, time 61.88ms, mfu 7.12%\n",
            "iter 1330: loss 1.2829, time 59.54ms, mfu 7.14%\n",
            "iter 1340: loss 1.2691, time 60.68ms, mfu 7.14%\n",
            "iter 1350: loss 1.2421, time 60.55ms, mfu 7.15%\n",
            "iter 1360: loss 1.2452, time 59.84ms, mfu 7.16%\n",
            "iter 1370: loss 1.2317, time 59.63ms, mfu 7.17%\n",
            "iter 1380: loss 1.2782, time 59.70ms, mfu 7.18%\n",
            "iter 1390: loss 1.2536, time 61.97ms, mfu 7.16%\n",
            "iter 1400: loss 1.2455, time 61.68ms, mfu 7.15%\n",
            "iter 1410: loss 1.2220, time 60.52ms, mfu 7.16%\n",
            "iter 1420: loss 1.2616, time 62.20ms, mfu 7.14%\n",
            "iter 1430: loss 1.2613, time 59.52ms, mfu 7.16%\n",
            "iter 1440: loss 1.2259, time 63.11ms, mfu 7.13%\n",
            "iter 1450: loss 1.2334, time 61.51ms, mfu 7.12%\n",
            "iter 1460: loss 1.2265, time 62.05ms, mfu 7.11%\n",
            "iter 1470: loss 1.2239, time 60.65ms, mfu 7.12%\n",
            "iter 1480: loss 1.2266, time 61.47ms, mfu 7.11%\n",
            "iter 1490: loss 1.2502, time 62.12ms, mfu 7.10%\n",
            "iter 1500: loss 1.1983, time 62.36ms, mfu 7.09%\n",
            "iter 1510: loss 1.1982, time 60.45ms, mfu 7.10%\n",
            "iter 1520: loss 1.2459, time 61.12ms, mfu 7.10%\n",
            "iter 1530: loss 1.2247, time 61.14ms, mfu 7.10%\n",
            "iter 1540: loss 1.2350, time 62.70ms, mfu 7.08%\n",
            "iter 1550: loss 1.2172, time 61.15ms, mfu 7.08%\n",
            "iter 1560: loss 1.2253, time 61.01ms, mfu 7.09%\n",
            "iter 1570: loss 1.2070, time 61.14ms, mfu 7.09%\n",
            "iter 1580: loss 1.2263, time 61.11ms, mfu 7.09%\n",
            "iter 1590: loss 1.2179, time 62.36ms, mfu 7.08%\n",
            "iter 1600: loss 1.2101, time 59.88ms, mfu 7.10%\n",
            "iter 1610: loss 1.1944, time 60.49ms, mfu 7.11%\n",
            "iter 1620: loss 1.1744, time 59.97ms, mfu 7.12%\n",
            "iter 1630: loss 1.1874, time 62.51ms, mfu 7.10%\n",
            "iter 1640: loss 1.1943, time 61.05ms, mfu 7.11%\n",
            "iter 1650: loss 1.2041, time 62.15ms, mfu 7.09%\n",
            "iter 1660: loss 1.1617, time 61.31ms, mfu 7.09%\n",
            "iter 1670: loss 1.1767, time 60.35ms, mfu 7.10%\n",
            "iter 1680: loss 1.1994, time 61.19ms, mfu 7.10%\n",
            "iter 1690: loss 1.1788, time 61.73ms, mfu 7.10%\n",
            "iter 1700: loss 1.1993, time 61.88ms, mfu 7.09%\n",
            "iter 1710: loss 1.1795, time 61.77ms, mfu 7.08%\n",
            "iter 1720: loss 1.2243, time 63.03ms, mfu 7.07%\n",
            "iter 1730: loss 1.2006, time 62.29ms, mfu 7.06%\n",
            "iter 1740: loss 1.1804, time 62.47ms, mfu 7.05%\n",
            "iter 1750: loss 1.1921, time 61.08ms, mfu 7.05%\n",
            "iter 1760: loss 1.1807, time 61.35ms, mfu 7.06%\n",
            "iter 1770: loss 1.1637, time 61.14ms, mfu 7.06%\n",
            "iter 1780: loss 1.1985, time 61.18ms, mfu 7.07%\n",
            "iter 1790: loss 1.1349, time 61.26ms, mfu 7.07%\n",
            "iter 1800: loss 1.1769, time 60.99ms, mfu 7.07%\n",
            "iter 1810: loss 1.1889, time 61.32ms, mfu 7.08%\n",
            "iter 1820: loss 1.1451, time 61.54ms, mfu 7.07%\n",
            "iter 1830: loss 1.2129, time 61.16ms, mfu 7.08%\n",
            "iter 1840: loss 1.1554, time 61.42ms, mfu 7.08%\n",
            "iter 1850: loss 1.1248, time 60.86ms, mfu 7.08%\n",
            "iter 1860: loss 1.1612, time 60.27ms, mfu 7.10%\n",
            "iter 1870: loss 1.1758, time 61.05ms, mfu 7.10%\n",
            "iter 1880: loss 1.1516, time 61.16ms, mfu 7.10%\n",
            "iter 1890: loss 1.1613, time 61.91ms, mfu 7.09%\n",
            "iter 1900: loss 1.1397, time 61.31ms, mfu 7.09%\n",
            "iter 1910: loss 1.1721, time 61.16ms, mfu 7.09%\n",
            "iter 1920: loss 1.1322, time 60.53ms, mfu 7.10%\n",
            "iter 1930: loss 1.1564, time 60.90ms, mfu 7.10%\n",
            "iter 1940: loss 1.1424, time 61.49ms, mfu 7.10%\n",
            "iter 1950: loss 1.1511, time 61.33ms, mfu 7.10%\n",
            "iter 1960: loss 1.1127, time 60.59ms, mfu 7.11%\n",
            "iter 1970: loss 1.1400, time 62.05ms, mfu 7.10%\n",
            "iter 1980: loss 1.1514, time 60.21ms, mfu 7.11%\n",
            "iter 1990: loss 1.1367, time 61.14ms, mfu 7.11%\n",
            "step 2000: train loss 1.0434, val loss 1.4664\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2000: loss 1.1365, time 8393.08ms, mfu 6.40%\n",
            "iter 2010: loss 1.1395, time 59.18ms, mfu 6.50%\n",
            "iter 2020: loss 1.1004, time 66.55ms, mfu 6.50%\n",
            "iter 2030: loss 1.1408, time 59.76ms, mfu 6.58%\n",
            "iter 2040: loss 1.1473, time 61.45ms, mfu 6.63%\n",
            "iter 2050: loss 1.1507, time 62.49ms, mfu 6.66%\n",
            "iter 2060: loss 1.1365, time 59.30ms, mfu 6.73%\n",
            "iter 2070: loss 1.1136, time 64.48ms, mfu 6.73%\n",
            "iter 2080: loss 1.1150, time 61.44ms, mfu 6.76%\n",
            "iter 2090: loss 1.1363, time 60.86ms, mfu 6.80%\n",
            "iter 2100: loss 1.1321, time 62.00ms, mfu 6.82%\n",
            "iter 2110: loss 1.1006, time 59.89ms, mfu 6.86%\n",
            "iter 2120: loss 1.1384, time 62.37ms, mfu 6.88%\n",
            "iter 2130: loss 1.0754, time 61.45ms, mfu 6.89%\n",
            "iter 2140: loss 1.0809, time 61.97ms, mfu 6.91%\n",
            "iter 2150: loss 1.1241, time 62.57ms, mfu 6.91%\n",
            "iter 2160: loss 1.1340, time 60.26ms, mfu 6.94%\n",
            "iter 2170: loss 1.1149, time 62.87ms, mfu 6.94%\n",
            "iter 2180: loss 1.1082, time 60.93ms, mfu 6.96%\n",
            "iter 2190: loss 1.0962, time 60.88ms, mfu 6.98%\n",
            "iter 2200: loss 1.1142, time 61.48ms, mfu 6.98%\n",
            "iter 2210: loss 1.0905, time 60.89ms, mfu 7.00%\n",
            "iter 2220: loss 1.0890, time 61.53ms, mfu 7.01%\n",
            "iter 2230: loss 1.0890, time 60.12ms, mfu 7.03%\n",
            "iter 2240: loss 1.1200, time 61.62ms, mfu 7.03%\n",
            "iter 2250: loss 1.0663, time 61.67ms, mfu 7.03%\n",
            "iter 2260: loss 1.0691, time 61.34ms, mfu 7.04%\n",
            "iter 2270: loss 1.0988, time 60.85ms, mfu 7.05%\n",
            "iter 2280: loss 1.0670, time 61.32ms, mfu 7.05%\n",
            "iter 2290: loss 1.0893, time 61.17ms, mfu 7.06%\n",
            "iter 2300: loss 1.1230, time 61.01ms, mfu 7.06%\n",
            "iter 2310: loss 1.0854, time 60.50ms, mfu 7.08%\n",
            "iter 2320: loss 1.0776, time 61.18ms, mfu 7.08%\n",
            "iter 2330: loss 1.0675, time 60.08ms, mfu 7.09%\n",
            "iter 2340: loss 1.0918, time 62.47ms, mfu 7.08%\n",
            "iter 2350: loss 1.0753, time 59.73ms, mfu 7.10%\n",
            "iter 2360: loss 1.0886, time 62.92ms, mfu 7.08%\n",
            "iter 2370: loss 1.0762, time 60.31ms, mfu 7.09%\n",
            "iter 2380: loss 1.0891, time 61.81ms, mfu 7.09%\n",
            "iter 2390: loss 1.0743, time 59.81ms, mfu 7.10%\n",
            "iter 2400: loss 1.0739, time 62.22ms, mfu 7.09%\n",
            "iter 2410: loss 1.0229, time 59.76ms, mfu 7.11%\n",
            "iter 2420: loss 1.0680, time 61.79ms, mfu 7.10%\n",
            "iter 2430: loss 1.0654, time 60.45ms, mfu 7.11%\n",
            "iter 2440: loss 1.0825, time 61.15ms, mfu 7.11%\n",
            "iter 2450: loss 1.0721, time 60.15ms, mfu 7.12%\n",
            "iter 2460: loss 1.0384, time 60.55ms, mfu 7.13%\n",
            "iter 2470: loss 1.0533, time 59.80ms, mfu 7.14%\n",
            "iter 2480: loss 1.0307, time 61.08ms, mfu 7.14%\n",
            "iter 2490: loss 1.0536, time 61.93ms, mfu 7.13%\n",
            "iter 2500: loss 1.0599, time 60.26ms, mfu 7.14%\n",
            "iter 2510: loss 1.0390, time 62.59ms, mfu 7.12%\n",
            "iter 2520: loss 1.0558, time 60.74ms, mfu 7.12%\n",
            "iter 2530: loss 1.0372, time 61.01ms, mfu 7.12%\n",
            "iter 2540: loss 0.9880, time 59.58ms, mfu 7.14%\n",
            "iter 2550: loss 1.0285, time 61.37ms, mfu 7.13%\n",
            "iter 2560: loss 1.0546, time 60.23ms, mfu 7.14%\n",
            "iter 2570: loss 1.0380, time 61.10ms, mfu 7.14%\n",
            "iter 2580: loss 1.0364, time 59.73ms, mfu 7.15%\n",
            "iter 2590: loss 1.0136, time 61.62ms, mfu 7.14%\n",
            "iter 2600: loss 1.0522, time 62.55ms, mfu 7.12%\n",
            "iter 2610: loss 1.0417, time 59.36ms, mfu 7.14%\n",
            "iter 2620: loss 1.0338, time 61.95ms, mfu 7.13%\n",
            "iter 2630: loss 1.0256, time 60.42ms, mfu 7.14%\n",
            "iter 2640: loss 1.0539, time 61.43ms, mfu 7.13%\n",
            "iter 2650: loss 1.0292, time 59.38ms, mfu 7.15%\n",
            "iter 2660: loss 1.0506, time 61.62ms, mfu 7.14%\n",
            "iter 2670: loss 1.0202, time 59.88ms, mfu 7.15%\n",
            "iter 2680: loss 1.0412, time 60.97ms, mfu 7.15%\n",
            "iter 2690: loss 1.0298, time 59.69ms, mfu 7.16%\n",
            "iter 2700: loss 1.0283, time 61.00ms, mfu 7.16%\n",
            "iter 2710: loss 1.0166, time 63.26ms, mfu 7.13%\n",
            "iter 2720: loss 1.0043, time 59.85ms, mfu 7.14%\n",
            "iter 2730: loss 1.0025, time 62.06ms, mfu 7.13%\n",
            "iter 2740: loss 1.0067, time 59.43ms, mfu 7.15%\n",
            "iter 2750: loss 1.0134, time 61.84ms, mfu 7.14%\n",
            "iter 2760: loss 1.0319, time 60.89ms, mfu 7.14%\n",
            "iter 2770: loss 1.0147, time 62.80ms, mfu 7.11%\n",
            "iter 2780: loss 1.0226, time 60.96ms, mfu 7.12%\n",
            "iter 2790: loss 0.9951, time 60.37ms, mfu 7.12%\n",
            "iter 2800: loss 0.9747, time 61.90ms, mfu 7.11%\n",
            "iter 2810: loss 1.0065, time 61.20ms, mfu 7.11%\n",
            "iter 2820: loss 0.9910, time 60.29ms, mfu 7.12%\n",
            "iter 2830: loss 1.0124, time 63.03ms, mfu 7.10%\n",
            "iter 2840: loss 0.9851, time 61.30ms, mfu 7.10%\n",
            "iter 2850: loss 0.9825, time 60.52ms, mfu 7.11%\n",
            "iter 2860: loss 0.9858, time 60.84ms, mfu 7.11%\n",
            "iter 2870: loss 0.9905, time 60.29ms, mfu 7.12%\n",
            "iter 2880: loss 0.9991, time 61.37ms, mfu 7.12%\n",
            "iter 2890: loss 1.0027, time 60.42ms, mfu 7.12%\n",
            "iter 2900: loss 1.0126, time 62.17ms, mfu 7.11%\n",
            "iter 2910: loss 0.9758, time 59.52ms, mfu 7.13%\n",
            "iter 2920: loss 0.9951, time 62.32ms, mfu 7.11%\n",
            "iter 2930: loss 0.9612, time 59.43ms, mfu 7.13%\n",
            "iter 2940: loss 0.9944, time 61.36ms, mfu 7.13%\n",
            "iter 2950: loss 0.9522, time 60.27ms, mfu 7.14%\n",
            "iter 2960: loss 0.9765, time 61.76ms, mfu 7.13%\n",
            "iter 2970: loss 0.9851, time 61.75ms, mfu 7.12%\n",
            "iter 2980: loss 1.0107, time 59.67ms, mfu 7.13%\n",
            "iter 2990: loss 0.9601, time 60.65ms, mfu 7.14%\n",
            "step 3000: train loss 0.8377, val loss 1.5508\n",
            "iter 3000: loss 0.9741, time 8060.68ms, mfu 6.43%\n",
            "iter 3010: loss 0.9614, time 64.64ms, mfu 6.46%\n",
            "iter 3020: loss 0.9686, time 60.93ms, mfu 6.53%\n",
            "iter 3030: loss 0.9420, time 60.65ms, mfu 6.59%\n",
            "iter 3040: loss 0.9807, time 59.67ms, mfu 6.66%\n",
            "iter 3050: loss 0.9694, time 61.39ms, mfu 6.70%\n",
            "iter 3060: loss 0.9612, time 61.06ms, mfu 6.74%\n",
            "iter 3070: loss 0.9632, time 60.49ms, mfu 6.79%\n",
            "iter 3080: loss 0.9644, time 60.56ms, mfu 6.83%\n",
            "iter 3090: loss 0.9550, time 61.00ms, mfu 6.86%\n",
            "iter 3100: loss 0.9599, time 62.83ms, mfu 6.86%\n",
            "iter 3110: loss 0.9707, time 60.00ms, mfu 6.90%\n",
            "iter 3120: loss 0.9599, time 60.81ms, mfu 6.92%\n",
            "iter 3130: loss 0.9565, time 60.91ms, mfu 6.95%\n",
            "iter 3140: loss 0.9521, time 61.91ms, mfu 6.95%\n",
            "iter 3150: loss 0.9333, time 62.16ms, mfu 6.96%\n",
            "iter 3160: loss 0.9468, time 61.56ms, mfu 6.97%\n",
            "iter 3170: loss 0.9413, time 61.67ms, mfu 6.98%\n",
            "iter 3180: loss 0.9632, time 61.89ms, mfu 6.98%\n",
            "iter 3190: loss 0.9610, time 60.03ms, mfu 7.01%\n",
            "iter 3200: loss 0.9353, time 62.37ms, mfu 7.00%\n",
            "iter 3210: loss 0.9341, time 60.84ms, mfu 7.02%\n",
            "iter 3220: loss 0.9453, time 62.31ms, mfu 7.01%\n",
            "iter 3230: loss 0.9423, time 60.73ms, mfu 7.03%\n",
            "iter 3240: loss 0.9437, time 60.37ms, mfu 7.04%\n",
            "iter 3250: loss 0.9373, time 60.91ms, mfu 7.05%\n",
            "iter 3260: loss 0.9250, time 61.08ms, mfu 7.06%\n",
            "iter 3270: loss 0.9296, time 59.87ms, mfu 7.08%\n",
            "iter 3280: loss 0.9323, time 61.39ms, mfu 7.08%\n",
            "iter 3290: loss 0.9528, time 59.50ms, mfu 7.10%\n",
            "iter 3300: loss 0.9220, time 63.03ms, mfu 7.08%\n",
            "iter 3310: loss 0.9400, time 60.99ms, mfu 7.09%\n",
            "iter 3320: loss 0.9029, time 60.82ms, mfu 7.09%\n",
            "iter 3330: loss 0.9261, time 60.33ms, mfu 7.10%\n",
            "iter 3340: loss 0.9395, time 60.88ms, mfu 7.11%\n",
            "iter 3350: loss 0.9287, time 62.87ms, mfu 7.09%\n",
            "iter 3360: loss 0.9452, time 60.62ms, mfu 7.10%\n",
            "iter 3370: loss 0.9378, time 60.64ms, mfu 7.10%\n",
            "iter 3380: loss 0.9033, time 62.31ms, mfu 7.09%\n",
            "iter 3390: loss 0.8978, time 62.49ms, mfu 7.08%\n",
            "iter 3400: loss 0.9256, time 61.44ms, mfu 7.08%\n",
            "iter 3410: loss 0.9068, time 60.33ms, mfu 7.09%\n",
            "iter 3420: loss 0.9026, time 61.80ms, mfu 7.08%\n",
            "iter 3430: loss 0.9276, time 60.51ms, mfu 7.09%\n",
            "iter 3440: loss 0.9167, time 61.10ms, mfu 7.09%\n",
            "iter 3450: loss 0.8907, time 60.88ms, mfu 7.10%\n",
            "iter 3460: loss 0.9068, time 61.26ms, mfu 7.10%\n",
            "iter 3470: loss 0.9151, time 61.57ms, mfu 7.09%\n",
            "iter 3480: loss 0.9150, time 62.37ms, mfu 7.08%\n",
            "iter 3490: loss 0.9189, time 62.16ms, mfu 7.07%\n",
            "iter 3500: loss 0.9479, time 62.67ms, mfu 7.06%\n",
            "iter 3510: loss 0.8867, time 59.76ms, mfu 7.08%\n",
            "iter 3520: loss 0.9197, time 61.64ms, mfu 7.08%\n",
            "iter 3530: loss 0.8902, time 60.88ms, mfu 7.08%\n",
            "iter 3540: loss 0.8921, time 62.20ms, mfu 7.07%\n",
            "iter 3550: loss 0.8891, time 61.04ms, mfu 7.08%\n",
            "iter 3560: loss 0.8980, time 60.98ms, mfu 7.08%\n",
            "iter 3570: loss 0.8762, time 61.41ms, mfu 7.08%\n",
            "iter 3580: loss 0.8731, time 61.85ms, mfu 7.08%\n",
            "iter 3590: loss 0.8803, time 61.57ms, mfu 7.08%\n",
            "iter 3600: loss 0.8996, time 60.51ms, mfu 7.09%\n",
            "iter 3610: loss 0.8923, time 60.38ms, mfu 7.10%\n",
            "iter 3620: loss 0.8476, time 60.68ms, mfu 7.10%\n",
            "iter 3630: loss 0.8940, time 60.82ms, mfu 7.11%\n",
            "iter 3640: loss 0.8796, time 61.24ms, mfu 7.11%\n",
            "iter 3650: loss 0.9211, time 60.57ms, mfu 7.11%\n",
            "iter 3660: loss 0.8586, time 60.90ms, mfu 7.12%\n",
            "iter 3670: loss 0.9030, time 62.53ms, mfu 7.10%\n",
            "iter 3680: loss 0.8728, time 62.16ms, mfu 7.09%\n",
            "iter 3690: loss 0.8898, time 62.68ms, mfu 7.07%\n",
            "iter 3700: loss 0.8837, time 59.93ms, mfu 7.09%\n",
            "iter 3710: loss 0.8775, time 63.55ms, mfu 7.07%\n",
            "iter 3720: loss 0.8694, time 61.78ms, mfu 7.06%\n",
            "iter 3730: loss 0.8896, time 62.21ms, mfu 7.05%\n",
            "iter 3740: loss 0.8747, time 60.57ms, mfu 7.07%\n",
            "iter 3750: loss 0.8725, time 59.98ms, mfu 7.08%\n",
            "iter 3760: loss 0.8522, time 61.02ms, mfu 7.09%\n",
            "iter 3770: loss 0.8484, time 59.79ms, mfu 7.11%\n",
            "iter 3780: loss 0.8890, time 62.78ms, mfu 7.09%\n",
            "iter 3790: loss 0.8688, time 60.69ms, mfu 7.10%\n",
            "iter 3800: loss 0.8707, time 63.15ms, mfu 7.07%\n",
            "iter 3810: loss 0.8587, time 60.64ms, mfu 7.08%\n",
            "iter 3820: loss 0.8514, time 60.86ms, mfu 7.09%\n",
            "iter 3830: loss 0.8492, time 60.97ms, mfu 7.09%\n",
            "iter 3840: loss 0.8666, time 60.40ms, mfu 7.10%\n",
            "iter 3850: loss 0.8337, time 61.69ms, mfu 7.10%\n",
            "iter 3860: loss 0.8575, time 61.92ms, mfu 7.09%\n",
            "iter 3870: loss 0.8489, time 61.73ms, mfu 7.08%\n",
            "iter 3880: loss 0.8579, time 62.32ms, mfu 7.07%\n",
            "iter 3890: loss 0.8817, time 60.51ms, mfu 7.08%\n",
            "iter 3900: loss 0.8519, time 61.34ms, mfu 7.08%\n",
            "iter 3910: loss 0.8454, time 60.52ms, mfu 7.09%\n",
            "iter 3920: loss 0.8443, time 60.99ms, mfu 7.10%\n",
            "iter 3930: loss 0.8686, time 60.10ms, mfu 7.11%\n",
            "iter 3940: loss 0.8795, time 60.50ms, mfu 7.12%\n",
            "iter 3950: loss 0.8547, time 60.79ms, mfu 7.12%\n",
            "iter 3960: loss 0.8542, time 60.86ms, mfu 7.12%\n",
            "iter 3970: loss 0.8294, time 62.45ms, mfu 7.11%\n",
            "iter 3980: loss 0.8306, time 61.18ms, mfu 7.11%\n",
            "iter 3990: loss 0.8592, time 60.43ms, mfu 7.11%\n",
            "step 4000: train loss 0.6621, val loss 1.6725\n",
            "iter 4000: loss 0.8416, time 8104.79ms, mfu 6.41%\n",
            "iter 4010: loss 0.8471, time 60.44ms, mfu 6.49%\n",
            "iter 4020: loss 0.8432, time 61.59ms, mfu 6.54%\n",
            "iter 4030: loss 0.8243, time 61.74ms, mfu 6.59%\n",
            "iter 4040: loss 0.8249, time 61.07ms, mfu 6.65%\n",
            "iter 4050: loss 0.8460, time 62.39ms, mfu 6.68%\n",
            "iter 4060: loss 0.8257, time 60.12ms, mfu 6.73%\n",
            "iter 4070: loss 0.8188, time 61.30ms, mfu 6.77%\n",
            "iter 4080: loss 0.8319, time 59.98ms, mfu 6.82%\n",
            "iter 4090: loss 0.8294, time 61.82ms, mfu 6.84%\n",
            "iter 4100: loss 0.8285, time 62.29ms, mfu 6.85%\n",
            "iter 4110: loss 0.8490, time 60.34ms, mfu 6.89%\n",
            "iter 4120: loss 0.8430, time 62.55ms, mfu 6.89%\n",
            "iter 4130: loss 0.8273, time 61.36ms, mfu 6.91%\n",
            "iter 4140: loss 0.8259, time 62.88ms, mfu 6.91%\n",
            "iter 4150: loss 0.8334, time 60.04ms, mfu 6.94%\n",
            "iter 4160: loss 0.8194, time 62.02ms, mfu 6.95%\n",
            "iter 4170: loss 0.7894, time 60.95ms, mfu 6.97%\n",
            "iter 4180: loss 0.8057, time 62.06ms, mfu 6.97%\n",
            "iter 4190: loss 0.8301, time 61.23ms, mfu 6.98%\n",
            "iter 4200: loss 0.8113, time 60.60ms, mfu 7.00%\n",
            "iter 4210: loss 0.8170, time 62.30ms, mfu 7.00%\n",
            "iter 4220: loss 0.8200, time 61.31ms, mfu 7.01%\n",
            "iter 4230: loss 0.8085, time 62.12ms, mfu 7.01%\n",
            "iter 4240: loss 0.8251, time 62.97ms, mfu 7.00%\n",
            "iter 4250: loss 0.8016, time 60.23ms, mfu 7.02%\n",
            "iter 4260: loss 0.8052, time 60.90ms, mfu 7.03%\n",
            "iter 4270: loss 0.8022, time 61.28ms, mfu 7.04%\n",
            "iter 4280: loss 0.8349, time 60.80ms, mfu 7.05%\n",
            "iter 4290: loss 0.8054, time 62.27ms, mfu 7.04%\n",
            "iter 4300: loss 0.7994, time 60.77ms, mfu 7.05%\n",
            "iter 4310: loss 0.8481, time 61.11ms, mfu 7.06%\n",
            "iter 4320: loss 0.8111, time 62.55ms, mfu 7.05%\n",
            "iter 4330: loss 0.8225, time 62.68ms, mfu 7.04%\n",
            "iter 4340: loss 0.8130, time 62.78ms, mfu 7.02%\n",
            "iter 4350: loss 0.8203, time 59.96ms, mfu 7.05%\n",
            "iter 4360: loss 0.8180, time 61.96ms, mfu 7.04%\n",
            "iter 4370: loss 0.8298, time 61.02ms, mfu 7.05%\n",
            "iter 4380: loss 0.7802, time 59.93ms, mfu 7.07%\n",
            "iter 4390: loss 0.8065, time 62.70ms, mfu 7.06%\n",
            "iter 4400: loss 0.8061, time 60.27ms, mfu 7.07%\n",
            "iter 4410: loss 0.8077, time 62.03ms, mfu 7.07%\n",
            "iter 4420: loss 0.7888, time 60.44ms, mfu 7.08%\n",
            "iter 4430: loss 0.7928, time 60.75ms, mfu 7.09%\n",
            "iter 4440: loss 0.8115, time 61.47ms, mfu 7.08%\n",
            "iter 4450: loss 0.7800, time 62.20ms, mfu 7.07%\n",
            "iter 4460: loss 0.8076, time 62.77ms, mfu 7.06%\n",
            "iter 4470: loss 0.8012, time 60.50ms, mfu 7.07%\n",
            "iter 4480: loss 0.7840, time 59.79ms, mfu 7.09%\n",
            "iter 4490: loss 0.7889, time 62.35ms, mfu 7.08%\n",
            "iter 4500: loss 0.7815, time 60.52ms, mfu 7.09%\n",
            "iter 4510: loss 0.7793, time 62.17ms, mfu 7.08%\n",
            "iter 4520: loss 0.7951, time 59.88ms, mfu 7.10%\n",
            "iter 4530: loss 0.8082, time 60.64ms, mfu 7.10%\n",
            "iter 4540: loss 0.7957, time 62.18ms, mfu 7.09%\n",
            "iter 4550: loss 0.7733, time 61.49ms, mfu 7.09%\n",
            "iter 4560: loss 0.8027, time 62.77ms, mfu 7.07%\n",
            "iter 4570: loss 0.7662, time 61.45ms, mfu 7.07%\n",
            "iter 4580: loss 0.7823, time 59.72ms, mfu 7.09%\n",
            "iter 4590: loss 0.8060, time 61.95ms, mfu 7.09%\n",
            "iter 4600: loss 0.7942, time 61.76ms, mfu 7.08%\n",
            "iter 4610: loss 0.7879, time 63.02ms, mfu 7.06%\n",
            "iter 4620: loss 0.7905, time 61.32ms, mfu 7.07%\n",
            "iter 4630: loss 0.7921, time 60.12ms, mfu 7.08%\n",
            "iter 4640: loss 0.8019, time 60.63ms, mfu 7.09%\n",
            "iter 4650: loss 0.8095, time 61.58ms, mfu 7.09%\n",
            "iter 4660: loss 0.7896, time 61.90ms, mfu 7.08%\n",
            "iter 4670: loss 0.7642, time 60.97ms, mfu 7.09%\n",
            "iter 4680: loss 0.7940, time 60.78ms, mfu 7.09%\n",
            "iter 4690: loss 0.7872, time 60.57ms, mfu 7.10%\n",
            "iter 4700: loss 0.7771, time 61.90ms, mfu 7.09%\n",
            "iter 4710: loss 0.7757, time 62.48ms, mfu 7.08%\n",
            "iter 4720: loss 0.7563, time 62.20ms, mfu 7.07%\n",
            "iter 4730: loss 0.7762, time 62.42ms, mfu 7.06%\n",
            "iter 4740: loss 0.7684, time 60.32ms, mfu 7.07%\n",
            "iter 4750: loss 0.8025, time 61.01ms, mfu 7.08%\n",
            "iter 4760: loss 0.7889, time 61.37ms, mfu 7.08%\n",
            "iter 4770: loss 0.7871, time 63.14ms, mfu 7.06%\n",
            "iter 4780: loss 0.8046, time 62.18ms, mfu 7.05%\n",
            "iter 4790: loss 0.7665, time 59.71ms, mfu 7.07%\n",
            "iter 4800: loss 0.7846, time 61.49ms, mfu 7.07%\n",
            "iter 4810: loss 0.7559, time 60.17ms, mfu 7.09%\n",
            "iter 4820: loss 0.7631, time 62.16ms, mfu 7.08%\n",
            "iter 4830: loss 0.7753, time 60.52ms, mfu 7.09%\n",
            "iter 4840: loss 0.8039, time 60.66ms, mfu 7.10%\n",
            "iter 4850: loss 0.7836, time 61.57ms, mfu 7.09%\n",
            "iter 4860: loss 0.7869, time 60.39ms, mfu 7.10%\n",
            "iter 4870: loss 0.7533, time 62.43ms, mfu 7.09%\n",
            "iter 4880: loss 0.7707, time 61.71ms, mfu 7.08%\n",
            "iter 4890: loss 0.7578, time 62.24ms, mfu 7.07%\n",
            "iter 4900: loss 0.7884, time 61.14ms, mfu 7.08%\n",
            "iter 4910: loss 0.7624, time 60.24ms, mfu 7.09%\n",
            "iter 4920: loss 0.7786, time 62.16ms, mfu 7.08%\n",
            "iter 4930: loss 0.7643, time 60.53ms, mfu 7.09%\n",
            "iter 4940: loss 0.7787, time 61.79ms, mfu 7.09%\n",
            "iter 4950: loss 0.7690, time 61.03ms, mfu 7.09%\n",
            "iter 4960: loss 0.7894, time 62.82ms, mfu 7.07%\n",
            "iter 4970: loss 0.7518, time 60.36ms, mfu 7.08%\n",
            "iter 4980: loss 0.7429, time 60.40ms, mfu 7.10%\n",
            "iter 4990: loss 0.7825, time 60.83ms, mfu 7.10%\n",
            "step 5000: train loss 0.5667, val loss 1.7624\n",
            "iter 5000: loss 0.7608, time 8090.30ms, mfu 6.40%\n",
            "iter 5010: loss 0.7567, time 62.47ms, mfu 6.45%\n",
            "iter 5020: loss 0.7703, time 61.30ms, mfu 6.52%\n",
            "iter 5030: loss 0.7672, time 60.65ms, mfu 6.58%\n",
            "iter 5040: loss 0.7576, time 62.65ms, mfu 6.62%\n",
            "iter 5050: loss 0.7637, time 59.75ms, mfu 6.68%\n",
            "iter 5060: loss 0.7607, time 59.69ms, mfu 6.74%\n",
            "iter 5070: loss 0.7858, time 60.28ms, mfu 6.79%\n",
            "iter 5080: loss 0.7655, time 60.92ms, mfu 6.82%\n",
            "iter 5090: loss 0.7606, time 61.65ms, mfu 6.85%\n",
            "iter 5100: loss 0.7676, time 60.07ms, mfu 6.88%\n",
            "iter 5110: loss 0.7600, time 62.82ms, mfu 6.89%\n",
            "iter 5120: loss 0.7397, time 59.71ms, mfu 6.93%\n",
            "iter 5130: loss 0.7623, time 60.98ms, mfu 6.95%\n",
            "iter 5140: loss 0.7646, time 61.88ms, mfu 6.95%\n",
            "iter 5150: loss 0.7711, time 60.88ms, mfu 6.97%\n",
            "iter 5160: loss 0.7642, time 62.35ms, mfu 6.97%\n",
            "iter 5170: loss 0.7780, time 60.41ms, mfu 6.99%\n",
            "iter 5180: loss 0.7533, time 61.11ms, mfu 7.01%\n",
            "iter 5190: loss 0.7763, time 61.18ms, mfu 7.02%\n",
            "iter 5200: loss 0.7566, time 61.19ms, mfu 7.02%\n",
            "iter 5210: loss 0.7818, time 61.14ms, mfu 7.03%\n",
            "iter 5220: loss 0.7822, time 60.49ms, mfu 7.05%\n",
            "iter 5230: loss 0.7402, time 60.53ms, mfu 7.06%\n",
            "iter 5240: loss 0.7741, time 61.46ms, mfu 7.06%\n",
            "iter 5250: loss 0.7567, time 61.27ms, mfu 7.07%\n",
            "iter 5260: loss 0.7767, time 62.38ms, mfu 7.06%\n",
            "iter 5270: loss 0.7417, time 62.41ms, mfu 7.05%\n",
            "iter 5280: loss 0.7744, time 61.28ms, mfu 7.05%\n",
            "iter 5290: loss 0.7520, time 60.21ms, mfu 7.07%\n",
            "iter 5300: loss 0.7671, time 61.17ms, mfu 7.07%\n",
            "iter 5310: loss 0.7554, time 61.89ms, mfu 7.07%\n",
            "iter 5320: loss 0.7661, time 62.22ms, mfu 7.06%\n",
            "iter 5330: loss 0.7723, time 61.62ms, mfu 7.06%\n",
            "iter 5340: loss 0.7726, time 61.07ms, mfu 7.06%\n",
            "iter 5350: loss 0.7808, time 61.78ms, mfu 7.06%\n",
            "iter 5360: loss 0.7565, time 62.25ms, mfu 7.05%\n",
            "iter 5370: loss 0.7542, time 62.04ms, mfu 7.05%\n",
            "iter 5380: loss 0.7513, time 61.34ms, mfu 7.05%\n",
            "iter 5390: loss 0.7667, time 60.08ms, mfu 7.07%\n",
            "iter 5400: loss 0.7448, time 61.43ms, mfu 7.07%\n",
            "iter 5410: loss 0.7473, time 61.03ms, mfu 7.08%\n",
            "iter 5420: loss 0.7725, time 60.50ms, mfu 7.09%\n",
            "iter 5430: loss 0.7594, time 61.29ms, mfu 7.09%\n",
            "iter 5440: loss 0.7530, time 59.64ms, mfu 7.11%\n",
            "iter 5450: loss 0.7573, time 62.98ms, mfu 7.09%\n",
            "iter 5460: loss 0.7531, time 61.63ms, mfu 7.08%\n",
            "iter 5470: loss 0.7559, time 62.45ms, mfu 7.07%\n",
            "iter 5480: loss 0.7435, time 60.35ms, mfu 7.08%\n",
            "iter 5490: loss 0.7477, time 60.40ms, mfu 7.09%\n",
            "iter 5500: loss 0.7424, time 60.57ms, mfu 7.10%\n",
            "iter 5510: loss 0.7495, time 61.29ms, mfu 7.10%\n",
            "iter 5520: loss 0.7404, time 60.05ms, mfu 7.12%\n",
            "iter 5530: loss 0.7368, time 60.54ms, mfu 7.12%\n",
            "iter 5540: loss 0.7396, time 61.81ms, mfu 7.11%\n",
            "iter 5550: loss 0.7500, time 61.65ms, mfu 7.11%\n",
            "iter 5560: loss 0.7650, time 62.07ms, mfu 7.10%\n",
            "iter 5570: loss 0.7493, time 60.56ms, mfu 7.10%\n",
            "iter 5580: loss 0.7678, time 60.95ms, mfu 7.11%\n",
            "iter 5590: loss 0.7482, time 61.01ms, mfu 7.11%\n",
            "iter 5600: loss 0.7502, time 60.98ms, mfu 7.11%\n",
            "iter 5610: loss 0.7375, time 59.62ms, mfu 7.13%\n",
            "iter 5620: loss 0.7618, time 61.43ms, mfu 7.12%\n",
            "iter 5630: loss 0.7368, time 61.40ms, mfu 7.12%\n",
            "iter 5640: loss 0.7265, time 61.92ms, mfu 7.11%\n",
            "iter 5650: loss 0.7518, time 62.78ms, mfu 7.09%\n",
            "iter 5660: loss 0.7531, time 61.74ms, mfu 7.08%\n",
            "iter 5670: loss 0.7211, time 60.88ms, mfu 7.09%\n",
            "iter 5680: loss 0.7529, time 60.64ms, mfu 7.10%\n",
            "iter 5690: loss 0.7217, time 61.13ms, mfu 7.10%\n",
            "iter 5700: loss 0.7383, time 62.08ms, mfu 7.09%\n",
            "iter 5710: loss 0.7510, time 59.87ms, mfu 7.11%\n",
            "iter 5720: loss 0.7467, time 61.71ms, mfu 7.10%\n",
            "iter 5730: loss 0.7210, time 59.80ms, mfu 7.12%\n",
            "iter 5740: loss 0.7555, time 62.34ms, mfu 7.10%\n",
            "iter 5750: loss 0.7699, time 61.00ms, mfu 7.10%\n",
            "iter 5760: loss 0.7454, time 62.42ms, mfu 7.09%\n",
            "iter 5770: loss 0.7237, time 61.12ms, mfu 7.09%\n",
            "iter 5780: loss 0.7354, time 59.97ms, mfu 7.11%\n",
            "iter 5790: loss 0.7092, time 62.97ms, mfu 7.09%\n",
            "iter 5800: loss 0.7301, time 60.37ms, mfu 7.10%\n",
            "iter 5810: loss 0.7335, time 60.52ms, mfu 7.11%\n",
            "iter 5820: loss 0.7290, time 59.94ms, mfu 7.12%\n",
            "iter 5830: loss 0.7039, time 60.03ms, mfu 7.13%\n",
            "iter 5840: loss 0.7520, time 60.88ms, mfu 7.13%\n",
            "iter 5850: loss 0.7467, time 61.23ms, mfu 7.13%\n",
            "iter 5860: loss 0.7493, time 63.01ms, mfu 7.11%\n",
            "iter 5870: loss 0.7336, time 61.54ms, mfu 7.10%\n",
            "iter 5880: loss 0.7456, time 61.13ms, mfu 7.10%\n",
            "iter 5890: loss 0.7548, time 60.96ms, mfu 7.11%\n",
            "iter 5900: loss 0.7386, time 61.75ms, mfu 7.10%\n",
            "iter 5910: loss 0.7428, time 60.37ms, mfu 7.11%\n",
            "iter 5920: loss 0.7185, time 61.32ms, mfu 7.11%\n",
            "iter 5930: loss 0.7278, time 60.25ms, mfu 7.12%\n",
            "iter 5940: loss 0.7489, time 60.17ms, mfu 7.13%\n",
            "iter 5950: loss 0.7190, time 61.78ms, mfu 7.12%\n",
            "iter 5960: loss 0.7491, time 59.80ms, mfu 7.13%\n",
            "iter 5970: loss 0.7379, time 60.78ms, mfu 7.14%\n",
            "iter 5980: loss 0.7308, time 61.29ms, mfu 7.13%\n",
            "iter 5990: loss 0.7251, time 62.31ms, mfu 7.12%\n",
            "step 6000: train loss 0.5096, val loss 1.8178\n",
            "iter 6000: loss 0.7309, time 8052.78ms, mfu 6.41%\n",
            "Extracted losses: train=0.5667, val=1.7624\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best configuration: 3 layers with 4 heads, validation loss: 1.4773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 4: Training BabyGPT for Code Generation\n",
        "\n",
        "!mkdir -p data/code_generation\n",
        "\n",
        "# copy the prepare.py file from the shakespeare dataset\n",
        "!cp data/shakespeare_char/prepare.py data/code_generation/\n",
        "\n",
        "# download Python code from GitHub repositories for the dataset\n",
        "import requests\n",
        "\n",
        "def download_github_file(repo, path, branch=\"main\"):\n",
        "    url = f\"https://raw.githubusercontent.com/{repo}/{branch}/{path}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        print(f\"Failed to download {url}, status code: {response.status_code}\")\n",
        "        return \"\"\n",
        "\n",
        "# repositories and files to download - including more to reach 100k tokens\n",
        "repos_and_files = [\n",
        "    # PyTorch examples\n",
        "    (\"pytorch/pytorch\", \"torch/nn/modules/activation.py\", \"master\"),\n",
        "    (\"pytorch/pytorch\", \"torch/nn/modules/linear.py\", \"master\"),\n",
        "    (\"pytorch/pytorch\", \"torch/nn/modules/loss.py\", \"master\"),\n",
        "]\n",
        "\n",
        "# combine into a dataset\n",
        "code_collection = \"\"\n",
        "\n",
        "for repo, path, branch in repos_and_files:\n",
        "    print(f\"Downloading {path} from {repo}...\")\n",
        "    code = download_github_file(repo, path, branch)\n",
        "    if code:\n",
        "        code_collection += f\"\\n# Source: {repo}/{path}\\n\\n{code}\\n\\n\"\n",
        "\n",
        "# write to input.txt\n",
        "with open('data/code_generation/input.txt', 'w') as f:\n",
        "    f.write(code_collection)\n",
        "\n",
        "print(f\"\\nTotal characters in code dataset: {len(code_collection)}\")\n",
        "\n",
        "# prepare the code dataset\n",
        "!python data/code_generation/prepare.py\n",
        "\n",
        "# create a new configuration file for code generation\n",
        "with open('config/train_shakespeare_char.py', 'r') as f:\n",
        "    shakespeare_config = f.read()\n",
        "\n",
        "# Modify the configuration for code generation\n",
        "code_gen_config = shakespeare_config.replace(\n",
        "    \"dataset = 'shakespeare_char'\",\n",
        "    \"dataset = 'code_generation'\"\n",
        ")\n",
        "code_gen_config = code_gen_config.replace(\n",
        "    \"out_dir = 'out-shakespeare-char'\",\n",
        "    \"out_dir = 'out-code-generation'\"\n",
        ")\n",
        "code_gen_config = code_gen_config.replace(\n",
        "    \"max_iters = \\d+\",\n",
        "    \"max_iters = 6000\"\n",
        ")\n",
        "code_gen_config = code_gen_config.replace(\n",
        "    \"eval_interval = \\d+\",\n",
        "    \"eval_interval = 1000\"\n",
        ")\n",
        "\n",
        "with open('config/train_code_generation.py', 'w') as f:\n",
        "    f.write(code_gen_config)\n",
        "\n",
        "# train the model on the code dataset\n",
        "!python train.py config/train_code_generation.py\n",
        "\n",
        "# generate samples from the trained model\n",
        "!python sample.py --out_dir=out-code-generation --max_new_tokens=500\n",
        "\n",
        "# get token count from meta.pkl\n",
        "import pickle\n",
        "token_count = \"Unknown\"\n",
        "try:\n",
        "    with open('data/code_generation/meta.pkl', 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "        token_count = len(np.memmap('data/code_generation/train.bin', dtype=np.uint16, mode='r'))\n",
        "        vocab_size = meta.get('vocab_size', 'Unknown')\n",
        "        print(f\"Token count: {token_count}, Vocabulary size: {vocab_size}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error getting token count: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxBRuSwlO1X3",
        "outputId": "7885e6e3-0575-4f22-ae83-33995987c944"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading torch/nn/modules/activation.py from pytorch/pytorch...\n",
            "Downloading torch/nn/modules/linear.py from pytorch/pytorch...\n",
            "Downloading torch/nn/modules/loss.py from pytorch/pytorch...\n",
            "\n",
            "Total characters in code dataset: 163121\n",
            "length of dataset in characters: 163,121\n",
            "all the unique characters: \n",
            " !\"#%&'()*+,-./0123456789:;<=>?@ABCDEFGHIKLMNOPQRSTUVWXY[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n",
            "vocab size: 93\n",
            "train has 146,808 tokens\n",
            "val has 16,313 tokens\n",
            "Overriding config with config/train_code_generation.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-code-generation'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'code_generation'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 93 (inside data/code_generation/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.66M\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 26, with 10,750,848 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "W0503 17:42:24.062000 21358 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "step 0: train loss 4.5545, val loss 4.5597\n",
            "iter 0: loss 4.5424, time 26738.95ms, mfu -100.00%\n",
            "iter 10: loss 3.2146, time 47.76ms, mfu 7.81%\n",
            "iter 20: loss 2.8465, time 47.80ms, mfu 7.81%\n",
            "iter 30: loss 2.6519, time 47.95ms, mfu 7.81%\n",
            "iter 40: loss 2.5697, time 47.91ms, mfu 7.80%\n",
            "iter 50: loss 2.4513, time 47.48ms, mfu 7.81%\n",
            "iter 60: loss 2.4376, time 47.85ms, mfu 7.81%\n",
            "iter 70: loss 2.3449, time 48.27ms, mfu 7.80%\n",
            "iter 80: loss 2.3127, time 48.34ms, mfu 7.79%\n",
            "iter 90: loss 2.1838, time 48.88ms, mfu 7.77%\n",
            "iter 100: loss 2.1839, time 49.18ms, mfu 7.76%\n",
            "iter 110: loss 2.0871, time 48.17ms, mfu 7.75%\n",
            "iter 120: loss 1.9666, time 49.09ms, mfu 7.74%\n",
            "iter 130: loss 1.8796, time 47.95ms, mfu 7.74%\n",
            "iter 140: loss 1.7094, time 48.72ms, mfu 7.73%\n",
            "iter 150: loss 1.5401, time 48.80ms, mfu 7.72%\n",
            "iter 160: loss 1.5014, time 48.86ms, mfu 7.72%\n",
            "iter 170: loss 1.5606, time 49.43ms, mfu 7.70%\n",
            "iter 180: loss 1.4272, time 49.06ms, mfu 7.69%\n",
            "iter 190: loss 1.2519, time 48.82ms, mfu 7.68%\n",
            "iter 200: loss 1.3124, time 49.03ms, mfu 7.68%\n",
            "iter 210: loss 1.2573, time 49.02ms, mfu 7.67%\n",
            "iter 220: loss 1.2663, time 49.26ms, mfu 7.66%\n",
            "iter 230: loss 1.1655, time 48.98ms, mfu 7.66%\n",
            "iter 240: loss 1.1153, time 49.31ms, mfu 7.65%\n",
            "step 250: train loss 0.8648, val loss 1.7228\n",
            "saving checkpoint to out-code-generation\n",
            "iter 250: loss 1.1007, time 6846.15ms, mfu 6.89%\n",
            "iter 260: loss 0.9529, time 48.30ms, mfu 6.97%\n",
            "iter 270: loss 0.8794, time 49.41ms, mfu 7.03%\n",
            "iter 280: loss 0.8663, time 48.56ms, mfu 7.09%\n",
            "iter 290: loss 0.9259, time 49.90ms, mfu 7.13%\n",
            "iter 300: loss 0.8383, time 49.18ms, mfu 7.18%\n",
            "iter 310: loss 0.7448, time 48.60ms, mfu 7.23%\n",
            "iter 320: loss 0.8098, time 48.88ms, mfu 7.27%\n",
            "iter 330: loss 0.7763, time 49.65ms, mfu 7.29%\n",
            "iter 340: loss 0.7207, time 48.92ms, mfu 7.32%\n",
            "iter 350: loss 0.7641, time 49.83ms, mfu 7.34%\n",
            "iter 360: loss 0.6211, time 48.88ms, mfu 7.37%\n",
            "iter 370: loss 0.6847, time 49.60ms, mfu 7.38%\n",
            "iter 380: loss 0.6042, time 49.42ms, mfu 7.40%\n",
            "iter 390: loss 0.6505, time 49.36ms, mfu 7.42%\n",
            "iter 400: loss 0.6770, time 49.68ms, mfu 7.43%\n",
            "iter 410: loss 0.5933, time 49.61ms, mfu 7.43%\n",
            "iter 420: loss 0.6779, time 49.88ms, mfu 7.44%\n",
            "iter 430: loss 0.5540, time 49.68ms, mfu 7.45%\n",
            "iter 440: loss 0.5440, time 49.50ms, mfu 7.45%\n",
            "iter 450: loss 0.5097, time 50.00ms, mfu 7.46%\n",
            "iter 460: loss 0.4275, time 49.68ms, mfu 7.46%\n",
            "iter 470: loss 0.4797, time 50.20ms, mfu 7.46%\n",
            "iter 480: loss 0.4590, time 50.11ms, mfu 7.46%\n",
            "iter 490: loss 0.4680, time 49.12ms, mfu 7.47%\n",
            "step 500: train loss 0.3331, val loss 1.5549\n",
            "saving checkpoint to out-code-generation\n",
            "iter 500: loss 0.4795, time 6861.99ms, mfu 6.73%\n",
            "iter 510: loss 0.3931, time 48.64ms, mfu 6.82%\n",
            "iter 520: loss 0.4420, time 49.15ms, mfu 6.90%\n",
            "iter 530: loss 0.4112, time 49.70ms, mfu 6.96%\n",
            "iter 540: loss 0.4041, time 49.96ms, mfu 7.01%\n",
            "iter 550: loss 0.3436, time 49.54ms, mfu 7.06%\n",
            "iter 560: loss 0.3894, time 51.18ms, mfu 7.08%\n",
            "iter 570: loss 0.4045, time 49.26ms, mfu 7.13%\n",
            "iter 580: loss 0.3618, time 51.02ms, mfu 7.15%\n",
            "iter 590: loss 0.3515, time 49.65ms, mfu 7.19%\n",
            "iter 600: loss 0.3523, time 50.71ms, mfu 7.20%\n",
            "iter 610: loss 0.3161, time 49.51ms, mfu 7.24%\n",
            "iter 620: loss 0.3365, time 51.36ms, mfu 7.24%\n",
            "iter 630: loss 0.3412, time 50.05ms, mfu 7.26%\n",
            "iter 640: loss 0.3226, time 51.53ms, mfu 7.26%\n",
            "iter 650: loss 0.2802, time 49.77ms, mfu 7.28%\n",
            "iter 660: loss 0.3139, time 51.39ms, mfu 7.28%\n",
            "iter 670: loss 0.2930, time 50.39ms, mfu 7.29%\n",
            "iter 680: loss 0.3028, time 50.77ms, mfu 7.30%\n",
            "iter 690: loss 0.3038, time 50.56ms, mfu 7.30%\n",
            "iter 700: loss 0.2909, time 51.10ms, mfu 7.30%\n",
            "iter 710: loss 0.2562, time 50.30ms, mfu 7.32%\n",
            "iter 720: loss 0.2830, time 50.56ms, mfu 7.32%\n",
            "iter 730: loss 0.2711, time 51.00ms, mfu 7.32%\n",
            "iter 740: loss 0.2804, time 51.14ms, mfu 7.32%\n",
            "step 750: train loss 0.1681, val loss 1.7699\n",
            "iter 750: loss 0.2516, time 6686.41ms, mfu 6.59%\n",
            "iter 760: loss 0.2431, time 52.23ms, mfu 6.65%\n",
            "iter 770: loss 0.2494, time 50.77ms, mfu 6.72%\n",
            "iter 780: loss 0.2798, time 51.83ms, mfu 6.76%\n",
            "iter 790: loss 0.2610, time 50.73ms, mfu 6.82%\n",
            "iter 800: loss 0.2439, time 52.49ms, mfu 6.85%\n",
            "iter 810: loss 0.2578, time 51.46ms, mfu 6.89%\n",
            "iter 820: loss 0.2317, time 52.30ms, mfu 6.91%\n",
            "iter 830: loss 0.2414, time 50.92ms, mfu 6.96%\n",
            "iter 840: loss 0.2124, time 51.86ms, mfu 6.98%\n",
            "iter 850: loss 0.2179, time 51.99ms, mfu 7.00%\n",
            "iter 860: loss 0.2176, time 52.23ms, mfu 7.01%\n",
            "iter 870: loss 0.1957, time 52.05ms, mfu 7.03%\n",
            "iter 880: loss 0.2090, time 52.24ms, mfu 7.04%\n",
            "iter 890: loss 0.2092, time 52.50ms, mfu 7.05%\n",
            "iter 900: loss 0.2203, time 52.59ms, mfu 7.05%\n",
            "iter 910: loss 0.2146, time 51.89ms, mfu 7.06%\n",
            "iter 920: loss 0.2166, time 52.69ms, mfu 7.07%\n",
            "iter 930: loss 0.2040, time 51.99ms, mfu 7.08%\n",
            "iter 940: loss 0.2045, time 52.64ms, mfu 7.08%\n",
            "iter 950: loss 0.2034, time 52.79ms, mfu 7.08%\n",
            "iter 960: loss 0.2082, time 52.63ms, mfu 7.08%\n",
            "iter 970: loss 0.1854, time 51.98ms, mfu 7.09%\n",
            "iter 980: loss 0.2014, time 52.04ms, mfu 7.09%\n",
            "iter 990: loss 0.2022, time 51.80ms, mfu 7.11%\n",
            "step 1000: train loss 0.1227, val loss 1.8971\n",
            "iter 1000: loss 0.1746, time 6753.62ms, mfu 6.40%\n",
            "iter 1010: loss 0.2116, time 53.69ms, mfu 6.46%\n",
            "iter 1020: loss 0.1925, time 50.97ms, mfu 6.54%\n",
            "iter 1030: loss 0.1965, time 52.98ms, mfu 6.59%\n",
            "iter 1040: loss 0.1724, time 52.45ms, mfu 6.64%\n",
            "iter 1050: loss 0.1932, time 52.27ms, mfu 6.69%\n",
            "iter 1060: loss 0.1722, time 52.93ms, mfu 6.73%\n",
            "iter 1070: loss 0.1740, time 52.96ms, mfu 6.76%\n",
            "iter 1080: loss 0.1792, time 53.55ms, mfu 6.78%\n",
            "iter 1090: loss 0.1875, time 52.99ms, mfu 6.81%\n",
            "iter 1100: loss 0.1722, time 53.80ms, mfu 6.82%\n",
            "iter 1110: loss 0.1701, time 52.96ms, mfu 6.84%\n",
            "iter 1120: loss 0.1759, time 53.32ms, mfu 6.86%\n",
            "iter 1130: loss 0.1647, time 53.91ms, mfu 6.86%\n",
            "iter 1140: loss 0.1625, time 53.30ms, mfu 6.88%\n",
            "iter 1150: loss 0.1718, time 53.01ms, mfu 6.89%\n",
            "iter 1160: loss 0.1696, time 53.66ms, mfu 6.90%\n",
            "iter 1170: loss 0.1773, time 52.70ms, mfu 6.92%\n",
            "iter 1180: loss 0.1681, time 53.58ms, mfu 6.92%\n",
            "iter 1190: loss 0.1720, time 53.51ms, mfu 6.93%\n",
            "iter 1200: loss 0.1593, time 53.59ms, mfu 6.93%\n",
            "iter 1210: loss 0.1574, time 53.60ms, mfu 6.93%\n",
            "iter 1220: loss 0.1613, time 54.28ms, mfu 6.93%\n",
            "iter 1230: loss 0.1714, time 53.87ms, mfu 6.93%\n",
            "iter 1240: loss 0.1486, time 53.68ms, mfu 6.93%\n",
            "step 1250: train loss 0.1027, val loss 2.0042\n",
            "iter 1250: loss 0.1591, time 6818.45ms, mfu 6.24%\n",
            "iter 1260: loss 0.1506, time 54.69ms, mfu 6.30%\n",
            "iter 1270: loss 0.1730, time 53.42ms, mfu 6.37%\n",
            "iter 1280: loss 0.1520, time 54.88ms, mfu 6.41%\n",
            "iter 1290: loss 0.1593, time 53.50ms, mfu 6.47%\n",
            "iter 1300: loss 0.1423, time 54.79ms, mfu 6.50%\n",
            "iter 1310: loss 0.1517, time 54.21ms, mfu 6.54%\n",
            "iter 1320: loss 0.1548, time 54.25ms, mfu 6.57%\n",
            "iter 1330: loss 0.1471, time 53.68ms, mfu 6.61%\n",
            "iter 1340: loss 0.1549, time 54.70ms, mfu 6.63%\n",
            "iter 1350: loss 0.1365, time 54.38ms, mfu 6.65%\n",
            "iter 1360: loss 0.1532, time 54.57ms, mfu 6.67%\n",
            "iter 1370: loss 0.1402, time 54.22ms, mfu 6.69%\n",
            "iter 1380: loss 0.1413, time 54.05ms, mfu 6.71%\n",
            "iter 1390: loss 0.1547, time 54.30ms, mfu 6.73%\n",
            "iter 1400: loss 0.1389, time 54.54ms, mfu 6.74%\n",
            "iter 1410: loss 0.1428, time 54.86ms, mfu 6.75%\n",
            "iter 1420: loss 0.1329, time 54.12ms, mfu 6.76%\n",
            "iter 1430: loss 0.1367, time 53.66ms, mfu 6.78%\n",
            "iter 1440: loss 0.1320, time 53.49ms, mfu 6.80%\n",
            "iter 1450: loss 0.1316, time 54.00ms, mfu 6.81%\n",
            "iter 1460: loss 0.1290, time 54.03ms, mfu 6.82%\n",
            "iter 1470: loss 0.1401, time 53.42ms, mfu 6.83%\n",
            "iter 1480: loss 0.1345, time 53.87ms, mfu 6.84%\n",
            "iter 1490: loss 0.1327, time 54.81ms, mfu 6.84%\n",
            "step 1500: train loss 0.0796, val loss 2.0565\n",
            "iter 1500: loss 0.1283, time 6805.41ms, mfu 6.16%\n",
            "iter 1510: loss 0.1361, time 54.30ms, mfu 6.23%\n",
            "iter 1520: loss 0.1203, time 52.99ms, mfu 6.31%\n",
            "iter 1530: loss 0.1215, time 53.94ms, mfu 6.37%\n",
            "iter 1540: loss 0.1371, time 52.29ms, mfu 6.45%\n",
            "iter 1550: loss 0.1243, time 54.27ms, mfu 6.49%\n",
            "iter 1560: loss 0.1268, time 53.15ms, mfu 6.54%\n",
            "iter 1570: loss 0.1316, time 53.61ms, mfu 6.59%\n",
            "iter 1580: loss 0.1307, time 54.11ms, mfu 6.62%\n",
            "iter 1590: loss 0.1310, time 52.53ms, mfu 6.66%\n",
            "iter 1600: loss 0.1163, time 54.01ms, mfu 6.69%\n",
            "iter 1610: loss 0.1266, time 53.17ms, mfu 6.72%\n",
            "iter 1620: loss 0.1201, time 53.10ms, mfu 6.75%\n",
            "iter 1630: loss 0.1237, time 53.81ms, mfu 6.77%\n",
            "iter 1640: loss 0.1178, time 53.72ms, mfu 6.79%\n",
            "iter 1650: loss 0.1084, time 52.97ms, mfu 6.81%\n",
            "iter 1660: loss 0.1205, time 53.53ms, mfu 6.83%\n",
            "iter 1670: loss 0.1250, time 52.53ms, mfu 6.85%\n",
            "iter 1680: loss 0.1147, time 53.13ms, mfu 6.87%\n",
            "iter 1690: loss 0.1230, time 53.35ms, mfu 6.88%\n",
            "iter 1700: loss 0.1189, time 53.17ms, mfu 6.90%\n",
            "iter 1710: loss 0.1136, time 52.95ms, mfu 6.91%\n",
            "iter 1720: loss 0.1203, time 53.03ms, mfu 6.92%\n",
            "iter 1730: loss 0.1166, time 53.18ms, mfu 6.93%\n",
            "iter 1740: loss 0.1145, time 53.34ms, mfu 6.94%\n",
            "step 1750: train loss 0.0718, val loss 2.1619\n",
            "iter 1750: loss 0.1129, time 6779.56ms, mfu 6.25%\n",
            "iter 1760: loss 0.1090, time 53.20ms, mfu 6.33%\n",
            "iter 1770: loss 0.1088, time 52.26ms, mfu 6.41%\n",
            "iter 1780: loss 0.1171, time 52.15ms, mfu 6.48%\n",
            "iter 1790: loss 0.1057, time 52.98ms, mfu 6.54%\n",
            "iter 1800: loss 0.1120, time 53.17ms, mfu 6.59%\n",
            "iter 1810: loss 0.1204, time 52.45ms, mfu 6.64%\n",
            "iter 1820: loss 0.1067, time 52.88ms, mfu 6.68%\n",
            "iter 1830: loss 0.0978, time 52.35ms, mfu 6.72%\n",
            "iter 1840: loss 0.1050, time 52.67ms, mfu 6.76%\n",
            "iter 1850: loss 0.1002, time 52.43ms, mfu 6.79%\n",
            "iter 1860: loss 0.1047, time 52.78ms, mfu 6.82%\n",
            "iter 1870: loss 0.1169, time 52.95ms, mfu 6.84%\n",
            "iter 1880: loss 0.1094, time 52.21ms, mfu 6.87%\n",
            "iter 1890: loss 0.1144, time 53.22ms, mfu 6.89%\n",
            "iter 1900: loss 0.1041, time 52.57ms, mfu 6.91%\n",
            "iter 1910: loss 0.0998, time 52.36ms, mfu 6.93%\n",
            "iter 1920: loss 0.1080, time 52.61ms, mfu 6.95%\n",
            "iter 1930: loss 0.1046, time 53.02ms, mfu 6.95%\n",
            "iter 1940: loss 0.1118, time 52.48ms, mfu 6.97%\n",
            "iter 1950: loss 0.1008, time 52.16ms, mfu 6.99%\n",
            "iter 1960: loss 0.1082, time 53.54ms, mfu 6.99%\n",
            "iter 1970: loss 0.1056, time 52.78ms, mfu 6.99%\n",
            "iter 1980: loss 0.1005, time 52.90ms, mfu 7.00%\n",
            "iter 1990: loss 0.1005, time 52.26ms, mfu 7.01%\n",
            "step 2000: train loss 0.0646, val loss 2.1702\n",
            "iter 2000: loss 0.1005, time 6756.07ms, mfu 6.32%\n",
            "iter 2010: loss 0.0973, time 53.82ms, mfu 6.38%\n",
            "iter 2020: loss 0.0887, time 51.60ms, mfu 6.46%\n",
            "iter 2030: loss 0.1010, time 53.02ms, mfu 6.52%\n",
            "iter 2040: loss 0.0913, time 51.11ms, mfu 6.60%\n",
            "iter 2050: loss 0.1027, time 53.45ms, mfu 6.64%\n",
            "iter 2060: loss 0.0932, time 51.93ms, mfu 6.69%\n",
            "iter 2070: loss 0.1033, time 53.16ms, mfu 6.72%\n",
            "iter 2080: loss 0.0926, time 52.45ms, mfu 6.76%\n",
            "iter 2090: loss 0.0967, time 52.75ms, mfu 6.79%\n",
            "iter 2100: loss 0.0972, time 52.87ms, mfu 6.82%\n",
            "iter 2110: loss 0.0963, time 53.13ms, mfu 6.84%\n",
            "iter 2120: loss 0.0934, time 52.63ms, mfu 6.86%\n",
            "iter 2130: loss 0.0907, time 52.51ms, mfu 6.89%\n",
            "iter 2140: loss 0.0897, time 52.99ms, mfu 6.90%\n",
            "iter 2150: loss 0.0913, time 51.98ms, mfu 6.93%\n",
            "iter 2160: loss 0.0919, time 52.67ms, mfu 6.95%\n",
            "iter 2170: loss 0.0963, time 52.36ms, mfu 6.96%\n",
            "iter 2180: loss 0.0878, time 53.30ms, mfu 6.97%\n",
            "iter 2190: loss 0.0909, time 51.39ms, mfu 7.00%\n",
            "iter 2200: loss 0.0929, time 53.72ms, mfu 6.99%\n",
            "iter 2210: loss 0.0888, time 52.10ms, mfu 7.01%\n",
            "iter 2220: loss 0.0964, time 52.68ms, mfu 7.01%\n",
            "iter 2230: loss 0.0904, time 53.58ms, mfu 7.01%\n",
            "iter 2240: loss 0.0920, time 52.76ms, mfu 7.01%\n",
            "step 2250: train loss 0.0599, val loss 2.2347\n",
            "iter 2250: loss 0.0944, time 6760.51ms, mfu 6.32%\n",
            "iter 2260: loss 0.0939, time 53.93ms, mfu 6.38%\n",
            "iter 2270: loss 0.0880, time 51.78ms, mfu 6.46%\n",
            "iter 2280: loss 0.0904, time 53.69ms, mfu 6.51%\n",
            "iter 2290: loss 0.0914, time 51.92ms, mfu 6.58%\n",
            "iter 2300: loss 0.0868, time 54.11ms, mfu 6.61%\n",
            "iter 2310: loss 0.0817, time 52.52ms, mfu 6.66%\n",
            "iter 2320: loss 0.0871, time 54.03ms, mfu 6.68%\n",
            "iter 2330: loss 0.0882, time 52.53ms, mfu 6.72%\n",
            "iter 2340: loss 0.0831, time 53.74ms, mfu 6.75%\n",
            "iter 2350: loss 0.0818, time 52.39ms, mfu 6.78%\n",
            "iter 2360: loss 0.0791, time 53.42ms, mfu 6.80%\n",
            "iter 2370: loss 0.0847, time 52.56ms, mfu 6.83%\n",
            "iter 2380: loss 0.0824, time 53.29ms, mfu 6.85%\n",
            "iter 2390: loss 0.0842, time 52.95ms, mfu 6.87%\n",
            "iter 2400: loss 0.0869, time 52.71ms, mfu 6.89%\n",
            "iter 2410: loss 0.0754, time 52.76ms, mfu 6.91%\n",
            "iter 2420: loss 0.0796, time 52.71ms, mfu 6.92%\n",
            "iter 2430: loss 0.0780, time 53.32ms, mfu 6.93%\n",
            "iter 2440: loss 0.0798, time 52.18ms, mfu 6.95%\n",
            "iter 2450: loss 0.0828, time 53.17ms, mfu 6.96%\n",
            "iter 2460: loss 0.0822, time 52.67ms, mfu 6.97%\n",
            "iter 2470: loss 0.0765, time 53.20ms, mfu 6.98%\n",
            "iter 2480: loss 0.0804, time 53.25ms, mfu 6.98%\n",
            "iter 2490: loss 0.0760, time 52.68ms, mfu 6.99%\n",
            "step 2500: train loss 0.0558, val loss 2.3223\n",
            "iter 2500: loss 0.0855, time 6778.15ms, mfu 6.29%\n",
            "iter 2510: loss 0.0810, time 54.64ms, mfu 6.35%\n",
            "iter 2520: loss 0.0785, time 53.56ms, mfu 6.41%\n",
            "iter 2530: loss 0.0834, time 53.47ms, mfu 6.47%\n",
            "iter 2540: loss 0.0750, time 52.46ms, mfu 6.53%\n",
            "iter 2550: loss 0.0729, time 53.33ms, mfu 6.58%\n",
            "iter 2560: loss 0.0755, time 52.56ms, mfu 6.63%\n",
            "iter 2570: loss 0.0793, time 53.15ms, mfu 6.67%\n",
            "iter 2580: loss 0.0752, time 53.14ms, mfu 6.70%\n",
            "iter 2590: loss 0.0779, time 53.89ms, mfu 6.72%\n",
            "iter 2600: loss 0.0767, time 52.57ms, mfu 6.76%\n",
            "iter 2610: loss 0.0767, time 52.91ms, mfu 6.79%\n",
            "iter 2620: loss 0.0767, time 54.59ms, mfu 6.79%\n",
            "iter 2630: loss 0.0751, time 53.15ms, mfu 6.82%\n",
            "iter 2640: loss 0.0793, time 53.60ms, mfu 6.83%\n",
            "iter 2650: loss 0.0824, time 52.75ms, mfu 6.85%\n",
            "iter 2660: loss 0.0749, time 53.43ms, mfu 6.87%\n",
            "iter 2670: loss 0.0718, time 53.52ms, mfu 6.88%\n",
            "iter 2680: loss 0.0705, time 52.63ms, mfu 6.90%\n",
            "iter 2690: loss 0.0763, time 53.09ms, mfu 6.91%\n",
            "iter 2700: loss 0.0692, time 52.57ms, mfu 6.93%\n",
            "iter 2710: loss 0.0748, time 53.85ms, mfu 6.93%\n",
            "iter 2720: loss 0.0753, time 53.42ms, mfu 6.93%\n",
            "iter 2730: loss 0.0737, time 53.42ms, mfu 6.94%\n",
            "iter 2740: loss 0.0682, time 52.93ms, mfu 6.95%\n",
            "step 2750: train loss 0.0525, val loss 2.3210\n",
            "iter 2750: loss 0.0716, time 6792.81ms, mfu 6.26%\n",
            "iter 2760: loss 0.0708, time 54.44ms, mfu 6.32%\n",
            "iter 2770: loss 0.0751, time 51.43ms, mfu 6.41%\n",
            "iter 2780: loss 0.0700, time 54.27ms, mfu 6.46%\n",
            "iter 2790: loss 0.0720, time 51.26ms, mfu 6.54%\n",
            "iter 2800: loss 0.0702, time 54.03ms, mfu 6.58%\n",
            "iter 2810: loss 0.0738, time 53.82ms, mfu 6.61%\n",
            "iter 2820: loss 0.0685, time 53.44ms, mfu 6.65%\n",
            "iter 2830: loss 0.0700, time 53.27ms, mfu 6.68%\n",
            "iter 2840: loss 0.0649, time 53.50ms, mfu 6.71%\n",
            "iter 2850: loss 0.0695, time 53.16ms, mfu 6.74%\n",
            "iter 2860: loss 0.0775, time 53.81ms, mfu 6.76%\n",
            "iter 2870: loss 0.0764, time 53.32ms, mfu 6.78%\n",
            "iter 2880: loss 0.0723, time 53.00ms, mfu 6.81%\n",
            "iter 2890: loss 0.0663, time 53.33ms, mfu 6.83%\n",
            "iter 2900: loss 0.0726, time 52.34ms, mfu 6.86%\n",
            "iter 2910: loss 0.0758, time 54.26ms, mfu 6.86%\n",
            "iter 2920: loss 0.0640, time 52.70ms, mfu 6.88%\n",
            "iter 2930: loss 0.0597, time 53.87ms, mfu 6.89%\n",
            "iter 2940: loss 0.0695, time 52.47ms, mfu 6.91%\n",
            "iter 2950: loss 0.0633, time 53.63ms, mfu 6.91%\n",
            "iter 2960: loss 0.0659, time 52.88ms, mfu 6.93%\n",
            "iter 2970: loss 0.0683, time 53.31ms, mfu 6.93%\n",
            "iter 2980: loss 0.0756, time 54.19ms, mfu 6.93%\n",
            "iter 2990: loss 0.0635, time 53.17ms, mfu 6.94%\n",
            "step 3000: train loss 0.0503, val loss 2.3727\n",
            "iter 3000: loss 0.0716, time 6783.85ms, mfu 6.25%\n",
            "iter 3010: loss 0.0633, time 53.33ms, mfu 6.32%\n",
            "iter 3020: loss 0.0686, time 52.79ms, mfu 6.40%\n",
            "iter 3030: loss 0.0702, time 53.68ms, mfu 6.45%\n",
            "iter 3040: loss 0.0641, time 52.16ms, mfu 6.52%\n",
            "iter 3050: loss 0.0672, time 53.23ms, mfu 6.57%\n",
            "iter 3060: loss 0.0658, time 52.87ms, mfu 6.62%\n",
            "iter 3070: loss 0.0627, time 52.90ms, mfu 6.66%\n",
            "iter 3080: loss 0.0702, time 53.78ms, mfu 6.69%\n",
            "iter 3090: loss 0.0608, time 54.19ms, mfu 6.71%\n",
            "iter 3100: loss 0.0640, time 52.55ms, mfu 6.75%\n",
            "iter 3110: loss 0.0622, time 53.18ms, mfu 6.77%\n",
            "iter 3120: loss 0.0623, time 52.66ms, mfu 6.81%\n",
            "iter 3130: loss 0.0684, time 53.30ms, mfu 6.82%\n",
            "iter 3140: loss 0.0641, time 52.66ms, mfu 6.85%\n",
            "iter 3150: loss 0.0629, time 52.88ms, mfu 6.87%\n",
            "iter 3160: loss 0.0657, time 52.63ms, mfu 6.89%\n",
            "iter 3170: loss 0.0625, time 53.02ms, mfu 6.91%\n",
            "iter 3180: loss 0.0643, time 53.74ms, mfu 6.91%\n",
            "iter 3190: loss 0.0650, time 53.14ms, mfu 6.92%\n",
            "iter 3200: loss 0.0611, time 53.59ms, mfu 6.92%\n",
            "iter 3210: loss 0.0600, time 52.82ms, mfu 6.94%\n",
            "iter 3220: loss 0.0644, time 52.70ms, mfu 6.95%\n",
            "iter 3230: loss 0.0655, time 52.75ms, mfu 6.96%\n",
            "iter 3240: loss 0.0566, time 53.22ms, mfu 6.97%\n",
            "step 3250: train loss 0.0467, val loss 2.4631\n",
            "iter 3250: loss 0.0615, time 6774.31ms, mfu 6.28%\n",
            "iter 3260: loss 0.0659, time 54.21ms, mfu 6.34%\n",
            "iter 3270: loss 0.0577, time 51.58ms, mfu 6.43%\n",
            "iter 3280: loss 0.0588, time 53.90ms, mfu 6.48%\n",
            "iter 3290: loss 0.0578, time 51.64ms, mfu 6.55%\n",
            "iter 3300: loss 0.0622, time 53.96ms, mfu 6.59%\n",
            "iter 3310: loss 0.0610, time 52.57ms, mfu 6.64%\n",
            "iter 3320: loss 0.0669, time 54.38ms, mfu 6.66%\n",
            "iter 3330: loss 0.0632, time 53.04ms, mfu 6.70%\n",
            "iter 3340: loss 0.0573, time 53.34ms, mfu 6.73%\n",
            "iter 3350: loss 0.0575, time 52.64ms, mfu 6.76%\n",
            "iter 3360: loss 0.0607, time 53.38ms, mfu 6.78%\n",
            "iter 3370: loss 0.0620, time 52.61ms, mfu 6.82%\n",
            "iter 3380: loss 0.0615, time 52.10ms, mfu 6.85%\n",
            "iter 3390: loss 0.0606, time 53.14ms, mfu 6.87%\n",
            "iter 3400: loss 0.0613, time 53.61ms, mfu 6.88%\n",
            "iter 3410: loss 0.0606, time 52.81ms, mfu 6.89%\n",
            "iter 3420: loss 0.0569, time 52.71ms, mfu 6.91%\n",
            "iter 3430: loss 0.0611, time 53.29ms, mfu 6.92%\n",
            "iter 3440: loss 0.0575, time 52.59ms, mfu 6.94%\n",
            "iter 3450: loss 0.0565, time 53.15ms, mfu 6.95%\n",
            "iter 3460: loss 0.0580, time 52.76ms, mfu 6.96%\n",
            "iter 3470: loss 0.0596, time 53.06ms, mfu 6.97%\n",
            "iter 3480: loss 0.0581, time 53.08ms, mfu 6.97%\n",
            "iter 3490: loss 0.0541, time 52.91ms, mfu 6.98%\n",
            "step 3500: train loss 0.0451, val loss 2.4833\n",
            "iter 3500: loss 0.0542, time 6773.24ms, mfu 6.29%\n",
            "iter 3510: loss 0.0591, time 54.02ms, mfu 6.35%\n",
            "iter 3520: loss 0.0619, time 52.29ms, mfu 6.43%\n",
            "iter 3530: loss 0.0612, time 52.83ms, mfu 6.49%\n",
            "iter 3540: loss 0.0578, time 51.82ms, mfu 6.56%\n",
            "iter 3550: loss 0.0620, time 52.75ms, mfu 6.61%\n",
            "iter 3560: loss 0.0547, time 52.21ms, mfu 6.66%\n",
            "iter 3570: loss 0.0595, time 53.12ms, mfu 6.70%\n",
            "iter 3580: loss 0.0603, time 53.10ms, mfu 6.73%\n",
            "iter 3590: loss 0.0623, time 52.89ms, mfu 6.76%\n",
            "iter 3600: loss 0.0601, time 53.02ms, mfu 6.79%\n",
            "iter 3610: loss 0.0564, time 52.34ms, mfu 6.83%\n",
            "iter 3620: loss 0.0559, time 52.43ms, mfu 6.85%\n",
            "iter 3630: loss 0.0558, time 52.60ms, mfu 6.88%\n",
            "iter 3640: loss 0.0566, time 52.67ms, mfu 6.90%\n",
            "iter 3650: loss 0.0558, time 52.43ms, mfu 6.92%\n",
            "iter 3660: loss 0.0606, time 52.72ms, mfu 6.93%\n",
            "iter 3670: loss 0.0533, time 52.93ms, mfu 6.95%\n",
            "iter 3680: loss 0.0520, time 52.90ms, mfu 6.96%\n",
            "iter 3690: loss 0.0516, time 52.70ms, mfu 6.97%\n",
            "iter 3700: loss 0.0572, time 52.29ms, mfu 6.98%\n",
            "iter 3710: loss 0.0571, time 54.02ms, mfu 6.98%\n",
            "iter 3720: loss 0.0549, time 52.94ms, mfu 6.98%\n",
            "iter 3730: loss 0.0539, time 54.03ms, mfu 6.98%\n",
            "iter 3740: loss 0.0558, time 52.64ms, mfu 6.99%\n",
            "step 3750: train loss 0.0439, val loss 2.4674\n",
            "iter 3750: loss 0.0518, time 6772.36ms, mfu 6.29%\n",
            "iter 3760: loss 0.0553, time 54.57ms, mfu 6.35%\n",
            "iter 3770: loss 0.0572, time 52.08ms, mfu 6.43%\n",
            "iter 3780: loss 0.0550, time 54.29ms, mfu 6.47%\n",
            "iter 3790: loss 0.0539, time 51.65ms, mfu 6.55%\n",
            "iter 3800: loss 0.0559, time 53.08ms, mfu 6.60%\n",
            "iter 3810: loss 0.0560, time 52.11ms, mfu 6.65%\n",
            "iter 3820: loss 0.0529, time 52.95ms, mfu 6.69%\n",
            "iter 3830: loss 0.0578, time 52.12ms, mfu 6.74%\n",
            "iter 3840: loss 0.0559, time 53.94ms, mfu 6.76%\n",
            "iter 3850: loss 0.0557, time 53.05ms, mfu 6.78%\n",
            "iter 3860: loss 0.0518, time 53.15ms, mfu 6.81%\n",
            "iter 3870: loss 0.0571, time 52.66ms, mfu 6.83%\n",
            "iter 3880: loss 0.0589, time 53.55ms, mfu 6.85%\n",
            "iter 3890: loss 0.0543, time 53.19ms, mfu 6.86%\n",
            "iter 3900: loss 0.0541, time 53.58ms, mfu 6.87%\n",
            "iter 3910: loss 0.0515, time 53.77ms, mfu 6.88%\n",
            "iter 3920: loss 0.0551, time 52.85ms, mfu 6.90%\n",
            "iter 3930: loss 0.0542, time 52.37ms, mfu 6.92%\n",
            "iter 3940: loss 0.0522, time 53.11ms, mfu 6.93%\n",
            "iter 3950: loss 0.0517, time 52.44ms, mfu 6.95%\n",
            "iter 3960: loss 0.0528, time 53.07ms, mfu 6.96%\n",
            "iter 3970: loss 0.0531, time 52.94ms, mfu 6.97%\n",
            "iter 3980: loss 0.0505, time 53.06ms, mfu 6.97%\n",
            "iter 3990: loss 0.0532, time 53.45ms, mfu 6.97%\n",
            "step 4000: train loss 0.0434, val loss 2.4884\n",
            "iter 4000: loss 0.0530, time 6765.10ms, mfu 6.28%\n",
            "iter 4010: loss 0.0498, time 53.83ms, mfu 6.35%\n",
            "iter 4020: loss 0.0539, time 51.84ms, mfu 6.43%\n",
            "iter 4030: loss 0.0517, time 53.62ms, mfu 6.48%\n",
            "iter 4040: loss 0.0551, time 51.77ms, mfu 6.55%\n",
            "iter 4050: loss 0.0535, time 53.43ms, mfu 6.60%\n",
            "iter 4060: loss 0.0535, time 52.35ms, mfu 6.65%\n",
            "iter 4070: loss 0.0516, time 54.02ms, mfu 6.68%\n",
            "iter 4080: loss 0.0570, time 52.41ms, mfu 6.72%\n",
            "iter 4090: loss 0.0532, time 53.79ms, mfu 6.74%\n",
            "iter 4100: loss 0.0519, time 53.36ms, mfu 6.77%\n",
            "iter 4110: loss 0.0504, time 53.51ms, mfu 6.79%\n",
            "iter 4120: loss 0.0531, time 52.42ms, mfu 6.82%\n",
            "iter 4130: loss 0.0535, time 52.76ms, mfu 6.84%\n",
            "iter 4140: loss 0.0512, time 52.90ms, mfu 6.86%\n",
            "iter 4150: loss 0.0496, time 52.26ms, mfu 6.89%\n",
            "iter 4160: loss 0.0506, time 53.11ms, mfu 6.91%\n",
            "iter 4170: loss 0.0508, time 52.84ms, mfu 6.92%\n",
            "iter 4180: loss 0.0557, time 53.03ms, mfu 6.93%\n",
            "iter 4190: loss 0.0538, time 52.59ms, mfu 6.95%\n",
            "iter 4200: loss 0.0527, time 54.05ms, mfu 6.94%\n",
            "iter 4210: loss 0.0517, time 52.56ms, mfu 6.96%\n",
            "iter 4220: loss 0.0575, time 53.73ms, mfu 6.96%\n",
            "iter 4230: loss 0.0556, time 52.85ms, mfu 6.97%\n",
            "iter 4240: loss 0.0524, time 53.93ms, mfu 6.96%\n",
            "step 4250: train loss 0.0415, val loss 2.5535\n",
            "iter 4250: loss 0.0490, time 6772.52ms, mfu 6.27%\n",
            "iter 4260: loss 0.0510, time 53.57ms, mfu 6.34%\n",
            "iter 4270: loss 0.0519, time 52.46ms, mfu 6.42%\n",
            "iter 4280: loss 0.0519, time 53.69ms, mfu 6.47%\n",
            "iter 4290: loss 0.0479, time 52.82ms, mfu 6.53%\n",
            "iter 4300: loss 0.0518, time 53.21ms, mfu 6.58%\n",
            "iter 4310: loss 0.0451, time 53.24ms, mfu 6.62%\n",
            "iter 4320: loss 0.0467, time 53.87ms, mfu 6.65%\n",
            "iter 4330: loss 0.0489, time 52.19ms, mfu 6.70%\n",
            "iter 4340: loss 0.0498, time 53.66ms, mfu 6.72%\n",
            "iter 4350: loss 0.0485, time 52.20ms, mfu 6.77%\n",
            "iter 4360: loss 0.0514, time 53.61ms, mfu 6.79%\n",
            "iter 4370: loss 0.0502, time 52.37ms, mfu 6.82%\n",
            "iter 4380: loss 0.0506, time 53.38ms, mfu 6.84%\n",
            "iter 4390: loss 0.0511, time 52.66ms, mfu 6.86%\n",
            "iter 4400: loss 0.0509, time 53.87ms, mfu 6.87%\n",
            "iter 4410: loss 0.0491, time 53.31ms, mfu 6.88%\n",
            "iter 4420: loss 0.0496, time 52.21ms, mfu 6.91%\n",
            "iter 4430: loss 0.0516, time 52.90ms, mfu 6.92%\n",
            "iter 4440: loss 0.0512, time 53.16ms, mfu 6.93%\n",
            "iter 4450: loss 0.0492, time 54.01ms, mfu 6.93%\n",
            "iter 4460: loss 0.0499, time 52.91ms, mfu 6.94%\n",
            "iter 4470: loss 0.0511, time 53.31ms, mfu 6.95%\n",
            "iter 4480: loss 0.0516, time 52.50ms, mfu 6.96%\n",
            "iter 4490: loss 0.0491, time 53.11ms, mfu 6.97%\n",
            "step 4500: train loss 0.0415, val loss 2.5394\n",
            "iter 4500: loss 0.0491, time 6769.49ms, mfu 6.28%\n",
            "iter 4510: loss 0.0523, time 52.99ms, mfu 6.35%\n",
            "iter 4520: loss 0.0491, time 51.75ms, mfu 6.44%\n",
            "iter 4530: loss 0.0483, time 53.23ms, mfu 6.49%\n",
            "iter 4540: loss 0.0495, time 51.92ms, mfu 6.56%\n",
            "iter 4550: loss 0.0502, time 53.38ms, mfu 6.61%\n",
            "iter 4560: loss 0.0495, time 52.81ms, mfu 6.65%\n",
            "iter 4570: loss 0.0505, time 53.01ms, mfu 6.69%\n",
            "iter 4580: loss 0.0497, time 53.03ms, mfu 6.72%\n",
            "iter 4590: loss 0.0519, time 53.43ms, mfu 6.75%\n",
            "iter 4600: loss 0.0489, time 52.74ms, mfu 6.78%\n",
            "iter 4610: loss 0.0501, time 53.25ms, mfu 6.80%\n",
            "iter 4620: loss 0.0496, time 53.11ms, mfu 6.83%\n",
            "iter 4630: loss 0.0478, time 53.17ms, mfu 6.84%\n",
            "iter 4640: loss 0.0486, time 53.21ms, mfu 6.86%\n",
            "iter 4650: loss 0.0447, time 53.32ms, mfu 6.87%\n",
            "iter 4660: loss 0.0482, time 54.57ms, mfu 6.87%\n",
            "iter 4670: loss 0.0431, time 53.06ms, mfu 6.89%\n",
            "iter 4680: loss 0.0498, time 54.38ms, mfu 6.88%\n",
            "iter 4690: loss 0.0493, time 53.35ms, mfu 6.89%\n",
            "iter 4700: loss 0.0501, time 54.15ms, mfu 6.89%\n",
            "iter 4710: loss 0.0469, time 53.36ms, mfu 6.90%\n",
            "iter 4720: loss 0.0465, time 53.39ms, mfu 6.91%\n",
            "iter 4730: loss 0.0497, time 53.20ms, mfu 6.92%\n",
            "iter 4740: loss 0.0469, time 52.69ms, mfu 6.94%\n",
            "step 4750: train loss 0.0404, val loss 2.5425\n",
            "iter 4750: loss 0.0509, time 6770.95ms, mfu 6.25%\n",
            "iter 4760: loss 0.0472, time 53.52ms, mfu 6.32%\n",
            "iter 4770: loss 0.0459, time 52.18ms, mfu 6.40%\n",
            "iter 4780: loss 0.0466, time 53.77ms, mfu 6.46%\n",
            "iter 4790: loss 0.0468, time 51.16ms, mfu 6.54%\n",
            "iter 4800: loss 0.0470, time 53.83ms, mfu 6.58%\n",
            "iter 4810: loss 0.0498, time 52.38ms, mfu 6.63%\n",
            "iter 4820: loss 0.0493, time 53.29ms, mfu 6.67%\n",
            "iter 4830: loss 0.0505, time 52.65ms, mfu 6.71%\n",
            "iter 4840: loss 0.0458, time 52.95ms, mfu 6.74%\n",
            "iter 4850: loss 0.0464, time 53.14ms, mfu 6.77%\n",
            "iter 4860: loss 0.0498, time 53.16ms, mfu 6.80%\n",
            "iter 4870: loss 0.0489, time 53.36ms, mfu 6.82%\n",
            "iter 4880: loss 0.0488, time 52.20ms, mfu 6.85%\n",
            "iter 4890: loss 0.0459, time 53.55ms, mfu 6.86%\n",
            "iter 4900: loss 0.0471, time 51.86ms, mfu 6.89%\n",
            "iter 4910: loss 0.0468, time 53.80ms, mfu 6.90%\n",
            "iter 4920: loss 0.0456, time 53.07ms, mfu 6.91%\n",
            "iter 4930: loss 0.0483, time 53.99ms, mfu 6.91%\n",
            "iter 4940: loss 0.0472, time 52.39ms, mfu 6.93%\n",
            "iter 4950: loss 0.0488, time 53.17ms, mfu 6.94%\n",
            "iter 4960: loss 0.0475, time 52.25ms, mfu 6.96%\n",
            "iter 4970: loss 0.0465, time 53.42ms, mfu 6.96%\n",
            "iter 4980: loss 0.0467, time 52.25ms, mfu 6.98%\n",
            "iter 4990: loss 0.0488, time 52.86ms, mfu 6.99%\n",
            "step 5000: train loss 0.0408, val loss 2.5711\n",
            "iter 5000: loss 0.0466, time 6770.74ms, mfu 6.29%\n",
            "Overriding: out_dir = out-code-generation\n",
            "Overriding: max_new_tokens = 500\n",
            "number of parameters: 10.66M\n",
            "Loading meta from data/code_generation/meta.pkl...\n",
            "\n",
            "             If given, the case a do be a Tensor, optional in the range [0, 1]].\n",
            "\n",
            "     def __init__(\n",
            "        self, out_features: int, bias: bool = True, device=None, dtype=None\n",
            "        self.device, dtype\n",
            "\n",
            "    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n",
            "       return F.multi_margin_margin_margin_margin_label_margin\n",
            "\n",
            "def ix_mak, inplace\n",
            "\n",
            "class Linear UnitializedParameter Unitial Linear Units function elements and sets and :math:`KL(S)`, where :math:`N` is the batch size. If :attr:`\n",
            "---------------\n",
            "\n",
            "               ask_type = 1 self.in_proj_weight add none:\n",
            "              constant_mask.view(\n",
            "              attn_mask,\n",
            "                        self.add_zero_attn,\n",
            "                     self.ad_zero_attn,\n",
            "                  self.regist,\n",
            "               self.reduction=self.reduction,\n",
            "            )\n",
            "\n",
            "class antMasstignLoss(_Loss):\n",
            "    r\"\"\"Creates a criterion that measures the loss given an a targets to a class.\n",
            "    This will be avalue as tensors :math:`[0, C)` or :math:`(minibatch ddim is cal+ Tensor, :ma\n",
            "---------------\n",
            "\n",
            "              size_average (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
            "          losses are averaged or sumed over observations for each minibatch depending\n",
            "           on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
            "            batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
            "        reduction (str, optional): Specifies the reduction to apply to the output:\n",
            "             ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'`\n",
            "---------------\n",
            "\n",
            "   .. math::\n",
            "        \\text{ReLU}(x) + \\text{ inplace} * \\exp(x) - 1)\n",
            "   Shape:\n",
            "        - Input: :math:`(N)` or :math:`(D)`, where :math:`*` means any number of additional\n",
            "    .. math::\n",
            "        \\text{loss}(x, y) = \\begin{cases}\n",
            "             \\operatorname{mean}(L), & \\text{if reduction} = \\text{`mean';}\\\\\n",
            "             \\operatorname{sum}(L), & \\text{if reduction} = \\text{`sum'.}\n",
            "        \\enl_n,   &\n",
            "          \\text{num'.}\n",
            "           \\operatorname{sum}(L), & \\text{num}), & \\text{if reduction} = \\text\n",
            "---------------\n",
            "\n",
            "                      - Output: :math:`(N)` or :math:`()`, same shape as the input.\n",
            "\n",
            "    .. image:: ../scripts/activation_images/Hardsigmoid.png\n",
            "\n",
            "    Examples::\n",
            "\n",
            "        >> m = nn.LeakyReLU()\n",
            "        >> input = torch.randn(3, 2)\n",
            "          >> output = m(input.size())\n",
            "         >> output = m(input)\n",
            "    .. _`Convidim information\n",
            "    Examples::\n",
            "      >> y = torch.ran Tensor < x value < ttensorsign.\n",
            "         >> loss(m(), = torch.empty(dim=1)\n",
            "         >> loss = nn.NLLLLoss()\n",
            "       >> loss = torch.rand\n",
            "---------------\n",
            "\n",
            "\n",
            "            - Target: :math:`(*)`, same shape as the input.\n",
            "\n",
            "    .. image:: ../scripts/activation_images/Hardswish.png\n",
            "\n",
            "    Examples::\n",
            "\n",
            "         >> m = nn.ReLU6()\n",
            "         >>> input = torch.randn(2)\n",
            "        >>> output = m(input)\n",
            "       >> output = m(input)\n",
            "    \"\"\"\n",
            "    __constants__ = [\"approximation\"]\n",
            "   def forward(self, input: Tensor) -> Tensor:\n",
            "        return F.celu(input, self.dim, _stacklevel=5)\n",
            "\n",
            "   def extra_repr(self) -> str:\n",
            "      return f\"dim={self.dim}\"\n",
            "\n",
            "class ELU(Module):\n",
            "    r\"\"\"App\n",
            "---------------\n",
            "\n",
            "                   mask,\n",
            "                                 self.reduction = _attn_mask,\n",
            "                 self.dim,\n",
            "             self.dropout,\n",
            "                  self.dropout,\n",
            "                         self.dropout,\n",
            "                 self.dropout,\n",
            "                        self.out_proj.weight,\n",
            "               self.weight,\n",
            "            self.reduction=self.reduction,\n",
            "             )\n",
            "\n",
            "\n",
            "class Loss(_Loss,\n",
            "    r\"\"\"Creates a criterion that mean aures a squared error a class in the lear recated in the elements :\n",
            "---------------\n",
            "\n",
            "             self.reset_parameters()\n",
            "    def reset_parameters(self) -> None:\n",
            "        return F.nll_loss(\n",
            "   def input1, target, target, reduction=self.reduction\n",
            "\n",
            "class KLDivLoss(_Loss):\n",
            "    r\"\"\"Creates a criterion that measures the loss. It In ditial\n",
            "    This f constance between input tensors of the ran a scan be perationals in the class.\n",
            "    This module of the finitial one ass for each sample. If reduction elements o addd its described in the paper `ReLU function` lay can be described as:\n",
            "\n",
            "    .\n",
            "---------------\n",
            "\n",
            "     When to is learngument :attr:`weight` is is equery.dim()``\n",
            "   \"\"\n",
            "    __constants__ = [\"add\"]\n",
            "   def __init__(\n",
            "        self,\n",
            "        in1_features: int,\n",
            "         self,\n",
            "         turn atn_weights=self.num_heads,\n",
            "             self.out_features,\n",
            "          f\"weight,\n",
            "              self.weight,\n",
            "             self.weight,\n",
            "              self.in_proj_weight,\n",
            "          self.reduction=self.reduction\n",
            "\n",
            "class True,\n",
            "         reduction=self.reduction\n",
            "\n",
            "class EmbeddingLoss(_Loss):\n",
            "    r\"\"\"Creates a crites a crit\n",
            "---------------\n",
            "\n",
            "              if self.in_proj_weight is not None:\n",
            "                  weight (x.input, self.bias, -bias, bool, optional)\n",
            "    def extra_repr(self) -> str:\n",
            "        return F.glu(input, self.labd, self.beta, self.dim,)\n",
            "    def extra_repr(self) -> str:\n",
            "        return f\"dim={self.dim}\n",
            "\n",
            "class RELU(Module):\n",
            "    r\"\"\"Applies the Softmax value function element-wise.\n",
            "    .. math::\n",
            "       \\text{LogSoftmax}(x) = \\frac{ * \\exp(-x) + \\exp(-x) \\exp(-x)}{x))\n",
            "\n",
            "    (here :math:`\\alpha f in1 < \\text{input} - \\exp(-x))\n",
            "---------------\n",
            "Token count: 146808, Vocabulary size: 93\n"
          ]
        }
      ]
    }
  ]
}